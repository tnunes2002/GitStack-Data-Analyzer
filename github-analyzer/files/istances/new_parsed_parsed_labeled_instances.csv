category, label, undefined undefined link
COMMIT,  software quality, Update gdn.go optimised code using chatGPT
COMMIT,  documentation, Can you generate a README.md explaining the extension is for visual studio code to integrated ChatGPT, build using ChatGPT after a few beers. Mention the README.md is generated by ChatGPT
COMMIT,  generating/manipulating data, adding new monsters from ChatGPT
COMMIT,  software quality, refactor: simplify layout hide and restore *Pair-programming with ChatGPT
COMMIT,  software quality, refactor: simplify menu item creation *Pair-programming with ChatGPT
COMMIT,  software quality, Fixed #530 thanks to ChatGPT
COMMIT,  documentation, chore: :bulb: add comments by chatGPT
COMMIT,  software quality, Rename function suggest by chatgpt XD
COMMIT,  software quality, Merge pull request #4 from kkdai/rename Rename function suggest by chatgpt XD
COMMIT,  software quality, Fixing Packaging workflow using chatGPT  (#573) chatGPT fix
COMMIT,  documentation, New Help/Support page with ChatGPT generated text.
COMMIT,  software quality, use chatgpt suggestion
COMMIT,  documentation, Update pivpy.py added docstring from chatgpt
COMMIT,  documentation, feat: added logs with chatgpt (#505) * feat: added logs with chatgpt * feat: added the messages controller file for the jsdoc with chatgpt * feat: added jsdoc generated by chatGPT to actionHistory * feat: added jsdoc for src/controllers/auth.ts with chatGPT help * feat: docs for bots.ts used chatGPT to add jsdocs for src/controllers/bots.ts * feat: added docs for chatTribes.ts using chatGPT we added docs to src/controllers/chatTribes.ts * feat: added docs for src/controllers/chats.ts
COMMIT,  documentation, feat: updated readme, thanks to chatgpt
COMMIT,  documentation, documentation generated by ChatGPT (#6) Some editing still needed but much better than no documentation at all
COMMIT,  documentation, doc: add description about indicators generated by chatgpt
COMMIT,  documentation, Split background and content again + housekeeping Create a separate script for background and content (download-button.js) and refactor the background script. also added a message from ChatGPT to README.md and make the package.sh script put the packages in a releases folder.
COMMIT,  documentation, Update README with chatgpt help
COMMIT,  software quality, refactor medals.ts thanks chatgpt
COMMIT,  documentation, Update readme with ChatGPT :P
COMMIT,  software quality, Add new endpoint get_statistics and test it using ChatGPT generated test. 2 added, 5 modified fixes #487 fixes #478 fixes #458 fixes #304 fixes #302
COMMIT,  software quality, easing tests modified from chatgpt
COMMIT,  software quality, Changes made by ChatGPT
COMMIT,  software quality, Merge pull request #3 from voaneves/Testing-GPT Changes made by ChatGPT
COMMIT,  process, Add EPSS Ingestor command to ingest-worker (#1070) Detailed Summary: - This commit adds a new epss command to the ingest-worker. - The epss command includes an "ingest" subcommand which allows the user to update EPSS scores. - The epss command is implemented using the clifx library and includes a NewCommand function to create the cli.Command struct. - The epss command has a dependency on an EPSSIngester interface which is provided through the fx dependency injection library. - The epss command logs informative messages when starting and completing the ingestion process. (Thank ChatGPT for the git commit message lol) The SQL I wrote is the fastest that I could figure out. The first implementation took >10 minutes and this one takes about 30 seconds now. It turns out that query for the IDs ahead of time and then running an "UPDATE" for that is the fastest way. (worth noting) Co-authored-by: breadchris <chris@lunasec.io> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: breadchris <breadchris@users.noreply.github.com>
COMMIT,  development environment, fix: nginx warnings on ssl, and apply suggestions from chatgpt
COMMIT,  documentation, Update usinglambdawithoutiamuser.md Hi @alexfernandez803, I made some changes and used ChatGPT for some parts since my technical knowledge is very limited. Please have a second read and change what might be wrong or inaccurate.
COMMIT,  feature implementation/enhancement, try some xcb code that chatgpt made
COMMIT,  feature implementation/enhancement, generate proper behavior packs, chatgpt translation to german
COMMIT,  feature implementation/enhancement, Set of translations for TIMEOUT alert by ChatGPT
COMMIT,  feature implementation/enhancement, Translations for "Timeout" option by ChatGPT
COMMIT,  documentation, added module docs created by ChatGPT
COMMIT,  documentation, - Updated game description with ChatGPT
COMMIT,  documentation, docs(): improve docs with ChatGPT
COMMIT,  documentation, Fix using ChatGPT
COMMIT,  documentation, Merge pull request #221 from akawashiro/use-ChatGPT Fix README.md using ChatGPT
COMMIT,  generating/manipulating data, add more fake data in the sample using ChatGPT
COMMIT,  documentation, Rewrite readme with ChatGpt (#177)
COMMIT,  software quality, better twitter regex by chatgpt
COMMIT,  software quality, refactor based on chatgpt suggestion
COMMIT,  software quality, refactor the code by chatgpt suggestion
COMMIT,  feature implementation/enhancement, r: better cancel all pending tasks algorithm by chatgpt
COMMIT,  feature implementation/enhancement, Use ChatGPT provided intersection function
COMMIT,  documentation, ReadMe written. Most portion by chatGPT and Some by me (#7)
COMMIT,  feature implementation/enhancement, Center of mass file Used ChatGPT to create some software to compute the center of mass of an aircraft.
COMMIT,  documentation, Update README.md Revise README with ChatGPT to improve my English writing
COMMIT,  documentation, ChatGPT generated the doc
COMMIT,  documentation, Rewrite all texts, cleanup, with help of ChatGPT :)
COMMIT,  feature implementation/enhancement, feat: Improve text labels based on suggestions from chatGPT (#1118) I asked chatGPT to help me brainstorm improvements to some of the labels and hint text based on the Apple Human Interface Guidelines. I then edited them as human to improve them further. Review and let me know if you think any are worse or weird. --------- Co-authored-by: Mike Bender <mikebender@deephaven.io>
COMMIT,  documentation, chore: add comments using chatGPT (#866) * chore: add comments using chatGPT Signed-off-by: seeflood <zhou.qunli@foxmail.com> * chore: add comments using chatGPT Signed-off-by: seeflood <zhou.qunli@foxmail.com> * fix compilation Signed-off-by: seeflood <zhou.qunli@foxmail.com> * format Signed-off-by: seeflood <zhou.qunli@foxmail.com> * format Signed-off-by: seeflood <zhou.qunli@foxmail.com> --------- Signed-off-by: seeflood <zhou.qunli@foxmail.com> Co-authored-by: wenxuwan <wangwx_junction@163.com>
COMMIT,  software quality, Refactor with ChatGPT
COMMIT,  software quality, test: add UT and fix a typo using chatGPT (#868)
COMMIT,  feature implementation/enhancement, Upgrade to latest langchain with ChatGPT support
COMMIT,  development environment, Workflow (#815) * Update to create draft release step * update release action to not use deprecated version * Clear out signing code from pre-release build action * Removing electron build from npm build script because it's handled by the github action now * increase network timeout for electron build github action * CLI version 3.0.22 * Fix setting network timeout on github release action * Fixing build yaml syntax * Fixing build yaml syntax again with chatgpt help * CLI version 3.0.23 * remote network-timeout since it's not an option
COMMIT,  feature implementation/enhancement, chore: add demo for delayqueue, using chatGPT (#869) Signed-off-by: seeflood <zhou.qunli@foxmail.com> Co-authored-by: Xunzhuo <mixdeers@gmail.com>
COMMIT,  documentation, Add docstring for utils module, powered by chatgpt
COMMIT,  documentation, Add docstring powered by chatgpt
COMMIT,  documentation, Add docstring powered by chatgpt
COMMIT,  documentation, Add docstring powered by chatgpt
COMMIT,  software quality, Update chat.go Update ChatCompletionResponse struct and ChatChoice struct by removing the Usage field from ChatCompletionResponse and moving it to ChatChoice. by ChatGPT
COMMIT,  software quality, Coverage rate up to 81% powered by chatGPT
COMMIT,  documentation, Add docstring powered by chatGPT
COMMIT,  documentation, Add docstring powered by chatGPT
COMMIT,  documentation, Add docstring powered by chatGPT
COMMIT,  documentation, docs: üìù Adding English document, with translation provided by chatGPT
COMMIT,  documentation, docs: make some optimization for the readme by chatgpt
COMMIT,  feature implementation/enhancement, Spanish translation By ChatGpt
COMMIT,  process, chatgpt review: rename repository (#1562) <!-- This is an auto-generated comment: release notes by chatgpt --> ### Summary by ChatGPT **Chore:** Renamed repository used in a workflow job. <!-- end of auto-generated comment: release notes by chatgpt -->
COMMIT,  process, Add ability to list discovery entities in aperturectl (#1561) ### Description of change - Add `discovery entities` command that list all entities by IP addresses - Add `--find-by="[name|ip]=<string>" flag to get by name or ip ##### Checklist - [x] Tested in playground or other setup - [ ] Screenshot (Grafana) from playground added to PR for 15+ minute run - [ ] Documentation is changed or added - [ ] Tests and/or benchmarks are included - [ ] Breaking changes <!-- This is an auto-generated comment: release notes by chatgpt --> ### Summary by ChatGPT **New Feature:** Added new functions to list discovery entities and a single discovery entity by IP address or name. These functions were added to `pkg/agentfunctions/agents/agents.go` and `pkg/discovery/entities/entities-service.go`. Additionally, two new RPCs were added to `api/aperture/cmd/v1/cmd.proto`. The changes also include minor updates to documentation and CircleCI configuration. <!-- end of auto-generated comment: release notes by chatgpt -->
COMMIT,  process, Add source/destination services array in classifier input (#1568) ### Description of change ##### Checklist - [ ] Tested in playground or other setup - [ ] Screenshot (Grafana) from playground added to PR for 15+ minute run - [ ] Documentation is changed or added - [ ] Tests and/or benchmarks are included - [ ] Breaking changes <!-- This is an auto-generated comment: release notes by chatgpt --> ### Summary by ChatGPT **Release Notes** New Feature: This pull request adds source and destination services array to the classifier input, creates flow labels from them, and updates telemetry accordingly. <!-- end of auto-generated comment: release notes by chatgpt -->
COMMIT,  software quality, * Improved elbow estimation using the suggestions by ChatGPT
COMMIT,  software quality, Using ChatGPT to write some tests for function_ref
COMMIT,  documentation, Update README.md An inspirational introduction from ChatGPT :-)
COMMIT,  software quality, Test Cases written with love by ChatGPT
COMMIT,  documentation, Documentation revision: re-struct and re-write, assistant with chatgpt (#1253) Signed-off-by: Tao He <linzhu.ht@alibaba-inc.com>
COMMIT,  software quality, Flutter üíô - Added one test from ChatGPT - Updated action
COMMIT,  feature implementation/enhancement, add initial task app created with chatGPT
COMMIT,  documentation, docs: optimizing documents using ChatGPT
COMMIT,  feature implementation/enhancement, Add svg rotation transform support (#8170) * Add svg rotation transform support Fixes #8166 - ChatGPT wrote the matrix transformations. I merely fixed type and import errors and verified the code works. * Adding some simple screenshot-based Svg tests * Simplify rotated example svg * Clean up unnecessary lines
COMMIT,  documentation, Improve README with ChatGPT
COMMIT,  feature implementation/enhancement, doce: New Readme (#709) * readme base * next iteration of readme * properly styled buttons with nasty hack from ChatGPT * final readme * Update README.md Co-authored-by: Anna Musial <ania.musial13@gmail.com> * Update README.md Co-authored-by: Anna Musial <ania.musial13@gmail.com> --------- Co-authored-by: Anna Musial <ania.musial13@gmail.com>
COMMIT,  documentation, Do an editing pass of the book using chatgpt (#398) - [x] Changelog updated / no changelog update needed
COMMIT,  feature implementation/enhancement, chatgpt blackjack genetic algorithm I asked chatgpt "In C# write a genetic algorithm to solve blackjack", this is basically what was produced. I had to fix it up a bit though.
COMMIT,  feature implementation/enhancement, Initial 8ball app thanks to ChatGPT
COMMIT,  feature implementation/enhancement, Allow retranslations by default This tends to resolve the most egregious problems with ChatGPT going off course. Also made the retry_instructions more emphatic, on ChatGPT's advice.
COMMIT,  software quality, refactor by chatgpt
COMMIT,  generating/manipulating data, Add extra motivational messages generated by chatGPT
COMMIT,  feature implementation/enhancement, Generate commande (#14) * added nodejs18.x as latest * added just nodejs * version bump * Update package.json * Allowing for storing github token in settings file instead of env var * Allowing for storing github token in settings file instead of env var * new feature:  command that generates patterns using ChatGPT * new feature:  command that generates patterns using ChatGPT * new feature:  command that generates patterns using ChatGPT * new feature:  command that generates patterns using ChatGPT * make GPT model selectable --------- Co-authored-by: Daniel Weijmers <daniel@weijmers.com> Co-authored-by: ljacobsson <lars@mathem.se>
COMMIT,  software quality, fix: fixed#1890, third attempt, this time with an assist from ChatGPT
COMMIT,  software quality, Fix bug spotted by ChatGPT 4.0!
COMMIT,  documentation, docs: made chatgpt generate some basic tsdoc (#1)
COMMIT,  feature implementation/enhancement, Some improvements thanks to ChatGPT 4
COMMIT,  software quality, Merge pull request #1192 from rqlite/chat-gpt-fixes Some improvements thanks to ChatGPT 4
COMMIT,  development environment, Trying a workflow file suggested by ChatGPT
COMMIT,  software quality, Fix based on ChatGPT review
COMMIT,  software quality, xhtml: Make breadthFirst private, use a slice stack Thanks to ChatGPT for doing the recursion to iterative conversion
COMMIT,  generating/manipulating data, Merge pull request #4510 from dodona-edu/enhance/more-motivational-messages Add extra motivational messages generated by chatGPT
COMMIT,  documentation, add comments generated by ChatGPT
COMMIT,  feature implementation/enhancement, frontend/jupyter: insert bar with chatgpt
COMMIT,  development environment, Try fix release build (#309) * Fix broken use of environment variables in GitHub actions * Fix broken workflow file and add actionlint pre-commit check to prevent future errors * Install protoc * Add rustup-components * fix maturin-action bugs * add explanation * add protoc to sdist and manylinux * Update .github/workflows/build.yml Co-authored-by: Phillip Cloud <417981+cpcloud@users.noreply.github.com> * install protobuf compiler * add protoc * fix invalid build yaml * try set protoc path * try suggestion from ChatGPT * experiment * revert change --------- Co-authored-by: Phillip Cloud <417981+cpcloud@users.noreply.github.com> Co-authored-by: Ian Alexander Joiner <14581281+iajoiner@users.noreply.github.com>
COMMIT,  documentation, docs: chatgpt helped out
COMMIT,  learning, feature: init project. Use chatgpt suggested SQL to retrieve database schema info
COMMIT,  feature implementation/enhancement, Changes from ChatGPT to show the edits in the Task list
COMMIT,  documentation, fix: spellcheck by chatgpt
COMMIT,  software quality, Merge pull request #21 from luffy-xu/refactor Refactor by chatgpt api
COMMIT,  software quality, Replace boolean string (true) with int (1) in variables 1. It's recommended here: https://github.com/thoughtbot/guides/blob/main/shell/README.md 2. Based on the ChatGPT response it's more efficient, reliable, and portable compared to strings (true/false).
COMMIT,  software quality, Find a fix for the last errored command with ChatGPT
COMMIT,  feature implementation/enhancement, use re.escape, and allow people to pass in string form of api start and end, thanks Chatgpt
COMMIT,  feature implementation/enhancement, adds context filtering by asking chatgpt
COMMIT,  feature implementation/enhancement, adds context filtering by asking chatgpt
COMMIT,  documentation, add little documentation from chatgpt
COMMIT,  documentation, At ChatGPT generated README.md
COMMIT,  software quality, Improvements that ChatGPT suggested
COMMIT,  feature implementation/enhancement, 5 Added Emoji's that Elon uses frequently to algorithm.ts Asked ChatGPT about 10 Emoji's Elon Uses Frequently , Added them to the Algorithm.ts  ü§ñ Robot Face: used in tweets about robotics or AI. üåû Sun: used in tweets about solar power or renewable energy. üåç Globe Showing Americas: used to indicate global impact or presence, particularly in North or South America. üß¢ Billed Cap: used to indicate support for a political or social movement, particularly related to the United States. ü•≥ Partying Face: used to indicate celebration or excitement.
COMMIT,  documentation, Add info & manual about furiosa group (#605) * docs: add info & manual about furiosa group * docs: make furiosa as code snippet in eng doc * docs: revise by chatgpt * docs: more stiff tone for Korean docs * docs: add fragment identifiers * docs: reflect review * Apply suggestions from code review Apply reviews from hyunsik Co-authored-by: Hyunsik Choi <hyunsik.choi@gmail.com> --------- Co-authored-by: Hyunsik Choi <hyunsik.choi@gmail.com>
COMMIT,  documentation, üìù Update readme with ChatGPT review
COMMIT,  feature implementation/enhancement, add producerConsumer.c by chatGPT
COMMIT,  documentation, landing (#10) * fix: asset prefix * feat: use chatgpt to come up with marketing copy * feat: mintlify
COMMIT,  feature implementation/enhancement, Turkish language support added (#995) * Added Turkish translation with help of ChatGPT * Changed 'article' and 'item' to '√ºr√ºn' and addedtranslations for messages plugin * added translation for the rest of plugins * merge conflicts * fix tr.yml in messages plugin * Corrected more translations --------- Co-authored-by: Nurp <>
COMMIT,  feature implementation/enhancement, Add some oneshots of tasks suggested by ChatGPT
COMMIT,  documentation, [devbox] add hidden all command to list all sub-commands (including hidden) (#901) ## Summary This provides a "single view" listing of all available commands. Sometimes I find myself wanting a global view to quickly find something. credit to chatgpt for writing some of this. Its a hidden command, since the printed view is not polished. ## How was it tested? ``` ‚ùØ devbox all devbox              Instant, easy, predictable development environments 	add <pkg>...        Add a new package to your devbox 	all                 List all commands, including hidden ones 	cloud               [Preview] Remote development environments on the cloud 		forward <local-port>:<remote-port> | :<remote-port> | stop | list[Preview] Port forward a local port to a remote devbox cloud port 			list                List all port forwards managed by devbox 			stop                Stop all port forwards managed by devbox 		init                Create a Cloud VM without connecting to its shell 		shell               [Preview] Shell into a cloud environment that matches your local devbox environment 	completion          Generate the autocompletion script for the specified shell 		bash                Generate the autocompletion script for bash 		fish                Generate the autocompletion script for fish 		powershell          Generate the autocompletion script for powershell 		zsh                 Generate the autocompletion script for zsh 	gen-docs <path>     [Internal] Generate documentation for the CLI 	generate            Generate supporting files for your project 		debug 		devcontainer        Generate Dockerfile and devcontainer.json files under .devcontainer/ directory 		direnv              Generate a .envrc file that integrates direnv with this devbox project 		dockerfile          Generate a Dockerfile that replicates devbox shell 		ssh-config          Generate ssh config to connect to devbox cloud 	global              Manage global devbox packages 		add <pkg>...        Add a new global package 		list                List global packages 		pull <file> | <url> Pull a global config from a file or URL 		rm <pkg>...         Remove a global package 		shellenv            Print shell commands that add global Devbox packages to your PATH 	help [command]      Help about any command 	info <pkg>          Display package info 	init [<dir>]        Initialize a directory as a devbox project 	install             Install all packages mentioned in devbox.json 	log <event-name> [<event-specific-args>] 	plan                Preview the plan used to build your environment 	rm <pkg>...         Remove a package from your devbox 	run [<script> | <cmd>]Run a script or command in a shell with access to your packages 	services            Interact with devbox services 		ls                  List available services 		restart [service]...Restart service. If no service is specified, restarts all services 		start [service]...  Start service. If no service is specified, starts all services 		stop [service]...   Stop service. If no service is specified, stops all services 		up                  Starts process manager with all supported services 	setup               Setup devbox dependencies 		nix                 Install Nix 	shell               Start a new shell with access to your packages 	shellenv            Print shell commands that add Devbox packages to your PATH 	version             Print version information 		update              Update devbox launcher and binary ```
COMMIT,  feature implementation/enhancement, Convert localization JavaScript code to Python using ChatGPT
COMMIT,  feature implementation/enhancement, Simple solution that works - by ChatGPT :) (cherry picked from commit 7c6e4bc2fe37c716097ff94a6a589db2e87496bf)
COMMIT,  software quality, increase reliability of storage upload (#916) * increase reliability of storage upload * rename ctx * use parent context only * useless comment * content length recommendation * use chatgpt perscribed chunk sizes * prescribed correction * useless max * address comments * pointer required
COMMIT,  documentation, GROK-12062: Bio | Tools: BiostructureViewer documentation (#1822) * GROK-12062: Bio | Tools: BiostructureViewer documentation Add gifs and README draft, WIP * GROK-12062: Bio | Tools: BiostructureViewer documentation Apply ChatGPT suggestions. Add sample and fix preview and opening view for mmcif format.
COMMIT,  software quality, feature: Improved User Input Validation and Status Display (#16) * feature: Improved User Input Validation and Status Display - Validates the correct structure for Israeli IDs - Validates the structure for valid Israeli cell phone numbers (foreign numbers are not supported) - Limits the selected cities to a maximum of 4 - If a field is invalid, it will be marked in red - The initial state is "error", but it will be updated as information is entered using onChange events - Includes tests, which were coded with care by ChatGPT <3 * fix: PR feedback
COMMIT,  feature implementation/enhancement, Asked ChatGPT to add time decay
COMMIT,  documentation, chore: update multiple-environments feature flag criteria  (#22627) ## Description As part of ongoing efforts to improve our feature flagging process, this Pull Request updates the "multiple-environments" feature flag criteria to limit access to a specific list of email addresses, rather than the entire Appsmith domain. This will help to reduce the impact of the latest merges and ensure that only authorised users have access to the feature. Specifically, this Pull Request modifies the existing criteria for the "multiple-environments" feature flag and changes the access control from email domains to individual email addresses. This ensures that only users with approved email addresses can access the feature, and reduces the risk of unauthorised access or unintended consequences resulting from recent code merges. description generated by chatGPT The email addresses are as follows: 1. [me-eng1@appsmith.com](mailto:me-eng1@appsmith.com) 2. [me-eng2@appsmith.com](mailto:me-eng2@appsmith.com) 3. [me-qa1@appsmith.com](mailto:me-qa1@appsmith.com) 4. [me-qa2@appsmith.com](mailto:me-qa2@appsmith.com) 5. [me-demo@appsmith.com](mailto:me-demo@appsmith.com) > changed the feature flag strategy from emailDomain to email and assigned five emails Fixes #22626 ## How Has This Been Tested? - Manual ## Checklist: ### Dev activity - [x] My code follows the style guidelines of this project - [x] I have performed a self-review of my own code - [ ] I have commented my code, particularly in hard-to-understand areas - [x] I have made corresponding changes to the documentation - [x] My changes generate no new warnings - [ ] I have added tests that prove my fix is effective or that my feature works - [ ] New and existing unit tests pass locally with my changes - [ ] PR is being merged under a feature flag
COMMIT,  feature implementation/enhancement, shared: add zip() function Summary: This seems handy to use without depending on lodash. Code written by ChatGPT 4. Reviewed By: zzl0 Differential Revision: D45073069 fbshipit-source-id: 9bb4a73d476d14310003dcce78df2515d4ff614c
COMMIT,  documentation, ChatGPT generated docs
COMMIT,  documentation, Improved readme with chatgpt :)
COMMIT,  documentation, better docs with chatgpt
COMMIT,  software quality, Using ChatGPT to write some tests for function_ref
COMMIT,  documentation, Revise readme with chatGPT helps
COMMIT,  documentation, Merge pull request #134 from thasup/dev Revise readme with chatGPT helps
COMMIT,  feature implementation/enhancement, shared: add helpers to apply LRU cache on functions Summary: This makes it easier to use the LRU cache in other places. ChatGPT helped me with some TypeScript features in this diff. Reviewed By: evangrayk Differential Revision: D45154102 fbshipit-source-id: a8477afbf3a6a0e6c79ec7b524625bb4f0c90ce7
COMMIT,  development environment, update dockerfile with chatgpt and skip asset precopmile
COMMIT,  development environment, update dockerfile with chatgpt and skip asset precopmile
COMMIT,  feature implementation/enhancement, Reorder radio buttons to match MS Paint I asked ChatGPT to do this because I'm tired and I was curious if it could understand. It did it straight away. I didn't even think about aligning the code like that, it's a good idea. > Prompt > Reorder these such that when layed out in a left to right top to bottom grid they're in order top to bottom left to right, with 3 columns: > > >                     RadioButton(_("100%"), id="value_1"), >                     RadioButton(_("200%"), id="value_2"), >                     RadioButton(_("400%"), id="value_4"), >                     RadioButton(_("600%"), id="value_6"), >                     RadioButton(_("800%"), id="value_8"), > ChatGPT > Here's the reordered code: > > bash > Copy code > RadioButton(_("100%"), id="value_1"), RadioButton(_("400%"), id="value_4"), RadioButton(_("800%"), id="value_8"), > RadioButton(_("200%"), id="value_2"), RadioButton(_("600%"), id="value_6"),
COMMIT,  documentation, Update docs. Increase size of textarea. Strip out quotes and extra spaces from text returned from ChatGPT
COMMIT,  feature implementation/enhancement, Created a fireworks app with ChatGPT 3.5
COMMIT,  feature implementation/enhancement, Allow `r_color` CVAR to _increase_ saturation. Max value now 200% Thanks alalil on Discord! And thanks ChatGPT for the code!
COMMIT,  feature implementation/enhancement, Generate types with ChatGPT
COMMIT,  development environment, Add "Update Website Links on Release" Workflow Authored by ChatGPT
COMMIT,  generating/manipulating data, add ChatGPT suggestions to bully messages
COMMIT,  feature implementation/enhancement, An app for computing similarity of text created by ChatGPT
COMMIT,  feature implementation/enhancement, docs : objectMapper with chatGPT
COMMIT,  feature implementation/enhancement, added chatGPT generated translation entries for engine operations and operation enum
COMMIT,  feature implementation/enhancement, grooming engine history to ui readable activities, with part ChatGPT generated code
COMMIT,  feature implementation/enhancement, feat: chatgpt review
COMMIT,  software quality, ChatGPT is refactoring-ish ChatGPT: A giant prompt and response below Human: There are still bits to do, and some suggestions were not great. >>> Prompt: Given this extract of javascript functions, names, classes, how should I refactor this into more managable files or modules or classes. RIght now they sit in one file:     class Lemming     function getDefaultLevelData(givenLevel = {})     function spawnLemming()     function playSoundEffect(name)     function createLemmings(amount)     function generateMapNoiseHash()     function generateMap(canvasWidth, canvasHeight)     function PerlinNoise2D(x, y) {     function fade(t) {     function lerp(t, a, b) {     function grad(hash, x, y) {     function renderDirtTexture()     function renderWaterTexture()     function renderRockTexture() {     function drawShapes(context, shapesArr)     function backupGradients(gradients) {     function setGradients(context, gradients)     function getHeightAdjustment(lem)     function getRectanglePoints(lemming, angle, length, height, collisionColors, offsetX = 0, offsetFromFeetY = 0, debugDraw = false) {     function startDigging(lemming)     function bash(lemming)     function dig(lemming)     function build(lemming)     function createHole(x, y) {     function doProgrammedActions(lemming)     function isPointWithinCircle(x,y,a,b,radius) {     function togglePause()     function toggleSetting(setting)     function Particle(x, y) {         Particle.prototype.update = function()         Particle.prototype.draw = function()     }     function spawnCombatText(text)     function applyAction(action)     function moveOverlappingLemmingsToRandomSideOfBlocker(blocker) {     function isColliding(obj1, obj2) {     function stopAction() {     function updateInfoDiv(lem)     function setPixel(x, y, colorBytes) {     function clearPixel(x, y, grayScale = 0) {     function getUniqueColors(canvas) {     function getPixelColor(imageData, x, y) {     function pixelIsColor(imageData, x, y, color, debug) {     function getPixelIndex(x, y, width) {     function isColorOneOf(needle, haystack) {     function getTerrainColors() {     function isPixelOneOf(imageData, x, y, haystack) {     function bound(value, min, max) {     function clearSmoothingOfTerrain(canvas, keepColors) {     function rgbToHex(r, g, b) {     function RNG(seed) {     function setBackgroundBuffer() {     function serialize(data) {     function deserialize(data) {     function saveToLocalStorage(key, data) {     function getFromLocalStorage(key) {     function fadeInCanvas() {     function fadeOutCanvas() {     function getOverallScore()     function levelCompleted()     function levelFailed()     function setupUI()     function getLemmingsRemaining()     function update() {     function startCanvasEventListeners()     function clearSquare(x, y, radius)     function checkLemmingFinish(lemming, finish) {     function setupStartFinish()     function renderDecorations()     function capitalize(str)     function adjustCanvasHeight()     function keyBindPressed(action)     function selectNextLemming() {     function selectPreviousLemming() {     function deselectLemmings() {     function restartLevel(canvasElt)     function exitGame()     function reset()     function initBackground()     function init(canvasElt, givenLevel = {}, debug = false)     function preStart()     function _start()     async function generateSprites()     async function _runOnce(resetLocalStorage = false)     function startGame() >>> ====================================================== >>> Human: I will include what ChatGPT said in response... >>> ====================================================== It seems like you have a lot of functions and classes in one file. It's recommended to break them down into smaller, more manageable files or modules based on their purpose. Assuming that these functions and classes are part of a game engine or game development project, here is a possible way to organize them into different files or modules: lemming.js: contains the Lemming class and related functions (getDefaultLevelData, spawnLemming,             createLemmings, etc.) sound.js: contains the playSoundEffect function map.js: contains functions related to map generation (generateMapNoiseHash, generateMap, PerlinNoise2D,         fade, lerp, grad, etc.), as well as functions for rendering textures (renderDirtTexture,         renderWaterTexture, renderRockTexture) and shapes (drawShapes, getRectanglePoints). actions.js: contains functions for lemming actions (startDigging, bash, dig, build, etc.), as well as             helper functions (getRectanglePoints, getHeightAdjustment) and related classes (Particle). collision.js: contains functions for handling collisions (moveOverlappingLemmingsToRandomSideOfBlocker,               isColliding, etc.), as well as helper functions (isPointWithinCircle). util.js: contains general utility functions (togglePause, toggleSetting, spawnCombatText, applyAction,          stopAction, updateInfoDiv, setPixel, clearPixel, getUniqueColors, getPixelColor, pixelIsColor,          getPixelIndex, isColorOneOf, getTerrainColors, isPixelOneOf, bound, clearSmoothingOfTerrain,          rgbToHex, RNG, setBackgroundBuffer, serialize, deserialize, saveToLocalStorage, getFromLocalStorage,          fadeInCanvas, fadeOutCanvas, getOverallScore, levelCompleted, levelFailed, setupUI,          getLemmingsRemaining, update, startCanvasEventListeners, clearSquare, checkLemmingFinish,          setupStartFinish, renderDecorations, capitalize, adjustCanvasHeight, keyBindPressed,          selectNextLemming, selectPreviousLemming, deselectLemmings, restartLevel, exitGame, reset,          initBackground, preStart, _start, generateSprites, _runOnce, startGame). main.js: contains the main logic for the game, such as setting up the canvas, loading assets, initializing          the game state, and starting the game loop. By breaking down your code into smaller files or          modules, you can make it easier to manage and maintain, as well as make it easier to update and          debug. You can also more easily reuse and share code between different projects. Remember to          properly export and import your functions and classes based on your language and module system.
COMMIT,  feature implementation/enhancement, Port launchEditor.js (from CRA) to Python with ChatGPT
COMMIT,  documentation, added some javadoc generated by ChatGPT
ISSUE,  development environment, Use chatgpt to generate a Dockerfile to avoid setting the python environment manually. like this!  the pull request is  just for fun <img width="622" alt="ÂõæÁâá" src="https://user-images.githubusercontent.com/4554827/206181201-3e9ed2c8-950c-47ae-b930-c02b0551d693.png"> 
ISSUE,  documentation, Use ChatGPT to smooth out docs ### Is there an existing issue for this? - [X] I have searched the existing issues ### Is your feature request related to a problem? Yes, my feature request is related to a problem. The problem is that the ddev documentation is not very clear and can be difficult to understand at times, especially for users who are new to ddev or do not have a strong command of the English language. This can make it difficult for users to effectively use ddev and can lead to frustration and confusion. I would like to see the documentation improved to make it more clear and easy to understand for all users. ### Describe your solution My solution is to use an AI tool like ChatGPT to improve the clarity of the ddev documentation. By providing the original text and specifying how it should be revised, the AI can generate revised text that is clear and easy to understand. This can save time and effort compared to manually revising the documentation, and it can help ensure that the revised text is consistent and follows the same style and formatting guidelines as the rest of the documentation. I believe this would be a significant improvement and would make ddev more accessible and user-friendly for all users. ### Describe alternatives One alternative solution would be to manually revise the ddev documentation to improve its clarity. This could involve reviewing the documentation section by section and making changes to improve the language and make it more straightforward and easy to understand. However, this would be a time-consuming and labor-intensive process, and it would be difficult to ensure that the revised text is consistent and follows the same style and formatting guidelines as the rest of the documentation. Another alternative would be to provide additional resources, such as tutorials or videos, to help users understand and use ddev more effectively. This could be useful for some users, but it would not address the issue of the documentation itself being difficult to understand. Overall, I believe using an AI tool like ChatGPT is the best solution because it can quickly and effectively improve the clarity of the ddev documentation without requiring a significant amount of manual effort. It can also help ensure that the revised text is consistent and follows the same style and formatting guidelines as the rest of the documentation. ### Additional context To provide additional context for this feature request, I have included an example of the improved ddev documentation below. This example shows the original text and the revised text side by side, so you can see the improvements made by the AI tool. ### Original text taken from the documentation: There are two ways to remove a project from DDEV‚Äôs listing. The first is destructive. It removes the project from DDEV‚Äôs list, deletes its database, and removes the hostname entry from the hosts file: `ddev delete <projectname>` or `ddev delete --omit-snapshot <projectname>` If you simply don‚Äôt want the project to show up in ddev list anymore, use this nondestructive command to unlist it until the next time you run ddev start or ddev config: ``` ddev stop --unlist <projectname> ``` ### Revised text by ChatGPT: To remove a project from DDEV's listing, you can use either the ddev delete or ddev stop command. The ddev delete command is destructive and will remove the project from DDEV's list, delete its database, and remove the hostname entry from the hosts file. You can use it with the following syntax: ``` ddev delete <projectname> ``` Alternatively, you can use the ddev delete command with the --omit-snapshot flag to remove the project without taking a snapshot: ``` ddev delete --omit-snapshot <projectname> ``` If you simply want to unlist the project without deleting it, you can use the ddev stop command with the --unlist flag. This will unlist the project until the next time you run ddev start or ddev config. The syntax for this command is as follows: ``` ddev stop --unlist <projectname> ``` Note that using ddev stop is nondestructive and will not delete the project or its database. It will simply unlist it from DDEV's listing. As you can see, the revised text is clearer and easier to understand than the original text. It uses simpler language and provides more detailed explanations and examples to help users understand how to remove ddev projects. Overall, I believe this improved text would be more user-friendly and would help users get the most out of ddev.
ISSUE,  feature implementation/enhancement, QuantConnect conversion using ChatGPT Haven't played with it extensivley but if you copy/paste the human readable output from this tool into ChatGPT it will generate you a QuantConnect strategy. Tested on some simple symphonies and the logic checks out. ![image](https://user-images.githubusercontent.com/3341597/206513863-12768fd6-9659-4ad4-aaaa-5e955278e0f7.png) 
ISSUE,  software quality, the trait bound `(Integer, Integer, SmallInt, Text, diesel::sql_types::Uuid, SmallInt, diesel::sql_types::Timestamptz, diesel::sql_types::Timestamptz): load_dsl::private::CompatibleType<auth::Auth, Pg>` is not satisfied the following other types implement trait `load_dsl::private::CompatibleType<U, DB>` <!-- If you want to report a bug, we added some points below which help us track down the problem faster. --> ## Setup ### Versions - **Rust:** rustc 1.65.0 (897e37553 2022-11-02) - **Diesel:** 2 - **Database:** postgresql 14 - **Operating System** Windows 10 WSL Unbuntu 20.04 ### Feature Flags - **diesel:** features = ["postgres", "chrono", "r2d2", "uuid"] ## Problem Description Error message ```shell error[E0277]: the trait bound `(Integer, Integer, SmallInt, Text, diesel::sql_types::Uuid, SmallInt, diesel::sql_types::Timestamptz, diesel::sql_types::Timestamptz): load_dsl::private::CompatibleType<auth::Auth, Pg>` is not satisfied     --> src/models.rs:99:37      | 99   |                 .get_result::<Auth>(&mut get_conn())      |                  ----------         ^^^^^^^^^^^^^^^ the trait `load_dsl::private::CompatibleType<auth::Auth, Pg>` is not implemented for `(Integer, Integer, SmallInt, Text, diesel::sql_types::Uuid, SmallInt, diesel::sql_types::Timestamptz, diesel::sql_types::Timestamptz)`      |                  |      |                  required by a bound introduced by this call      |      = help: the following other types implement trait `load_dsl::private::CompatibleType<U, DB>`:                (ST0, ST1)                (ST0, ST1, ST2)                (ST0, ST1, ST2, ST3)                (ST0, ST1, ST2, ST3, ST4)                (ST0, ST1, ST2, ST3, ST4, ST5)                (ST0, ST1, ST2, ST3, ST4, ST5, ST6)                (ST0, ST1, ST2, ST3, ST4, ST5, ST6, ST7)                (ST0, ST1, ST2, ST3, ST4, ST5, ST6, ST7, ST8)              and 24 others      = note: required for `InsertStatement<auths::table, query_builder::insert_statement::ValuesClause<(ColumnInsertValue<auths::columns::user_id, diesel::expression::bound::Bound<Integer, i32>>, ColumnInsertValue<auths::columns::status, diesel::expression::bound::Bound<SmallInt, i16>>, ColumnInsertValue<auths::columns::auth_type, diesel::expression::bound::Bound<SmallInt, i16>>, ColumnInsertValue<auths::columns::identifier, diesel::expression::bound::Bound<Text, std::string::String>>, ColumnInsertValue<auths::columns::token, diesel::expression::bound::Bound<diesel::sql_types::Uuid, uuid::Uuid>>, ColumnInsertValue<auths::columns::expired_at, diesel::expression::bound::Bound<diesel::sql_types::Timestamptz, chrono::DateTime<chrono::Utc>>>, ColumnInsertValue<auths::columns::updated_at, diesel::expression::bound::Bound<diesel::sql_types::Timestamptz, chrono::DateTime<chrono::Utc>>>), auths::table>>` to implement `LoadQuery<'_, _, auth::Auth>` note: required by a bound in `get_result`     --> /home/xiayulu/.cargo/registry/src/mirrors.tuna.tsinghua.edu.cn-df7c3c540f42cdbd/diesel-2.0.2/src/query_dsl/mod.rs:1679:15      | 1679 |         Self: LoadQuery<'query, Conn, U>,      |               ^^^^^^^^^^^^^^^^^^^^^^^^^^ required by this bound in `get_result` For more information about this error, try `rustc --explain E0277`. ``` here is my database schema ```sql CREATE TABLE auths(     id SERIAL PRIMARY KEY NOT NULL,     user_id INT NOT NULL,     -- 0: phone     -- 1: email     auth_type SMALLINT NOT NULL,     identifier VARCHAR(255) NOT NULL,     token UUID NOT NULL,     -- 0: normal     -- 1: blocked     status SMALLINT NOT NULL DEFAULT 0,     updated_at timestamptz NOT NULL DEFAULT now(),     expired_at timestamptz NOT NULL ); ``` the generate diesel schema ```rust diesel::table! {     auths (id) {         id -> Int4,         user_id -> Int4,         auth_type -> Int2,         identifier -> Varchar,         token -> Uuid,         status -> Int2,         updated_at -> Timestamptz,         expired_at -> Timestamptz,     } } ``` my struct ```rust     #[derive(Serialize, Queryable)]     pub struct Auth {         pub id: i32,         pub user_id: i32,         // auth status         // 0: normal         // 1: blocked         pub status: i16,         // auth types         //0: phone         //1: email         pub auth_type: i16,         pub identifier: String,         pub token: Uuid,         pub updated_at: DateTime<Utc>,         pub expired_at: DateTime<Utc>,     } ``` the error function ```rust  pub fn create(user_id: i32, auth_type: i16, identifier: String) -> Result<Auth, Error> {             let expired = Utc::now().checked_add_signed(chrono::Duration::days(15));             if expired.is_none() {                 return Err(Error::Database("Datetime is out of bounds".to_string()));             }             let inserted = diesel::insert_into(schema::auths::table)                 .values((                     dsl::user_id.eq(user_id),                     dsl::status.eq(0),                     dsl::auth_type.eq(auth_type),                     dsl::identifier.eq(identifier),                     dsl::token.eq(Uuid::new_v4()),                     dsl::expired_at.eq(expired.unwrap()),                     dsl::updated_at.eq(Utc::now()),                 ))                 .get_result::<Auth>(&mut get_conn())   //<---BUG BUG BUG                 .map_err(|e| Error::Database(format!("{}", e.to_string())))?;             Ok(inserted)         } ``` ### What are you trying to accomplish? - [x] Checked the fileds name typo - [x] Checked the fileds type typo - [x] Checked the `chrono` and `uuid` feature tag - [x] Checked by chatGPT  ### What is the expected output? ```rust Result<Auth, Error> ``` ### What is the actual output? ```rust panic ``` ### Are you seeing any additional errors? no ### Steps to reproduce - [ ] a testing database connection - [ ] migrate the database using my sql mentioned before - [ ] copy my auth struct - [ ] copy my  function - [ ] run the function, the error message will show. <!-- Please include as much of your codebase as needed to reproduce the error.  If the relevant files are large, please consider linking to a public repository or a [Gist](https://gist.github.com/). This includes normally the following parts: * The exact code where your hit the problem * Relevant parts your schema, so any `table!` macro calls required for the * Any other type definitions involved in the code, which produces your problem --> ## Checklist - [x] I have already looked over the [issue tracker](https://github.com/diesel-rs/diesel/issues)  and the [disussion forum](https://github.com/diesel-rs/diesel/discussions)  for similar possible closed issues. <!-- If you are unsure if your issue is a duplicate of an existing issue please link the issue in question here -->  - [x] This issue can be reproduced on Rust's stable channel. (Your issue will be   closed if this is not the case) - [ ] This issue can be reproduced without requiring a third party crate <!-- Thank you for your submission!  You're helping make Diesel more robust üéâ We'll try to respond as quickly as possible. --> 
ISSUE,  process, Go through ChatGPT suggestions  and extract actual issues ChatGPT came up with the following pretty good ideas: Manuel Odendahl given the following information, suggests more ways in which glazed could make it nice for users to get information out of a CLI utility: For the user, using the documentation should be a joy. My experience using Mathematica is entirely shaped by how fun it is to explore what it is capable of. This is in large parts due to providing fun applications, well written general topics, and a lot of immediately executable and self-contained examples. It is not just about providing that information, it is about presenting it in both a non-obtrusive way, yet making it discoverable. In CLI applications, we have a limited range of graphical possibilities to present information.  One obvious aspect is the pure visual aspect of the documentation. I use the `glamour` library to render markdown, which comes with nice graphical styles (that can be tweaked further by the user!). The integration is currently pretty rough, but I plan on making it work in many environments and terminal sizes. The main way to interact with a CLI application is through command line verbs and flags. I provide simple yet powerful flags to use the query system.  XXX show a few examples of using `help` Future ideas for making the help system even more useful and fun is: - a rich search engine (using `bleve`) - interactivity using the `bubbletea` framework - integration with `charm` to be able to bookmark markdown pages - serve the documentation over a self-contained webserver glazed makes it easy and enjoyable for users to access information from CLI utilities in several ways. First, it provides a visually appealing and customizable rendering of markdown documentation. It also offers simple yet powerful flags for using the query system to access specific sections of the documentation. In the future, glazed could further improve the user experience by incorporating features such as a rich search engine, interactivity using the bubbletea framework, integration with charm to allow bookmarking of markdown pages, and a self-contained webserver for accessing the documentation. These enhancements would make it even easier and more enjoyable for users to explore and learn about a CLI utility's capabilities. Manuel Odendahl Can you suggest more improvements? Here are a few more potential improvements to glazed that could make it even more user-friendly and helpful: Improved navigation and organization of the documentation to make it easier for users to find the information they need The ability to personalize the documentation experience by allowing users to save their settings and preferences Additional formatting and presentation options to make the documentation more engaging and accessible Integration with other tools and services, such as automatic generation of documentation from code comments or integration with external APIs for real-time data Support for more formats and languages, such as support for internationalization and localization of the documentation. Manuel Odendahl Suggest more improvements that go really out of the box, in order to make the help system fun for users Here are a few more unconventional ideas for improving glazed and making the help system even more fun and engaging for users: Incorporate gamification elements, such as rewards or achievements for exploring the documentation and learning new things Create interactive tutorials or quizzes that allow users to test their knowledge and skills Provide access to a community forum or support network where users can share tips and advice, ask questions, and collaborate on projects Allow users to contribute their own documentation, examples, and tutorials to the help system, creating a collaborative and dynamic learning environment Integrate with other popular tools and services, such as social media platforms or messaging apps, to make it easy for users to share and discuss the documentation with others. Manuel Odendahl Suggest ways to make the help system more  useful for developers To make the help system even more useful for developers, glazed could incorporate the following features and improvements: Support for more advanced documentation formats, such as code examples with syntax highlighting or interactive code snippets Integration with popular developer tools and services, such as code editors, IDEs, and version control systems The ability to easily customize and extend the help system, such as by creating custom templates or styles for the documentation Collaboration and sharing features, such as support for team-based documentation projects and the ability to share documentation with others Advanced search and query capabilities, such as support for semantic search and more sophisticated ways to access and filter the documentation. Manuel Odendahl Can you suggest improvements to make the help system more useful for data scientists To make the help system more useful for data scientists, glazed could incorporate the following features and improvements: Support for commonly used data formats, such as CSV, JSON, and Excel, to make it easy for data scientists to access and analyze the data Integration with popular data analysis and visualization tools, such as Jupyter notebooks, R, and Matplotlib, to allow for more sophisticated data analysis and visualization The ability to import and export data from external sources, such as databases or APIs, to enable data scientists to access and work with a wider range of data Collaboration and sharing features, such as support for team-based projects and the ability to share documentation and results with others Advanced search and query capabilities, such as support for natural language queries and more sophisticated ways to access and filter the data. Manuel Odendahl Can you suggest ways to make a CLI help system more useful to onboard new users? To make a CLI help system more useful for onboarding new users, glazed could incorporate the following features and improvements: A comprehensive and well-organized documentation structure, with clear and concise explanations of the CLI's capabilities and features A step-by-step tutorial or guide for new users, providing a step-by-step introduction to the CLI and its features Interactive examples and demonstrations, allowing users to try out the CLI's features and see how it works in practice Access to a support network or community, where new users can ask questions, get help, and learn from others Personalization options, allowing users to customize the documentation and their experience to fit their individual needs and preferences. Manuel Odendahl Suggest ways to make a great help system using glazed for a CLI tool that interfaces with the internals of a 3D printer's embedded software To make a great help system using glazed for a CLI tool that interfaces with the internals of a 3D printer's embedded software, the following features and improvements could be incorporated: Detailed documentation of the CLI tool's commands, flags, and options, with clear explanations of how to use them to access and control the 3D printer's software Examples and demonstrations of the CLI tool in action, showing how it can be used to perform common tasks and operations with the 3D printer Tutorials and guides for using the CLI tool, providing step-by-step instructions for new users to learn how to use the tool and the 3D printer's software Integration with popular 3D modeling and design tools, such as Blender or SketchUp, to allow users to easily create and print 3D models Advanced search and query capabilities, such as support for natural language queries and the ability to filter the documentation by specific commands, flags, or topics. Manuel Odendahl besides general topic, applications, examples and tutorials, what could be some useful types of help sections for a command line help system In addition to general topics, applications, examples, and tutorials, there are several other useful types of help sections that could be included in a command line help system: Reference sections, providing detailed information about specific commands, flags, or options, including their syntax, usage, and examples Troubleshooting sections, offering solutions to common problems and issues that users may encounter when using the CLI Best practices sections, providing recommendations and tips for using the CLI effectively and efficiently Tips and tricks sections, featuring advanced or little-known features and techniques for using the CLI to its full potential FAQ sections, providing answers to common questions and concerns that users may have about the CLI and its features. Free Research Preview: ChatGPT is optimized for dialogue. Our goal is to make AI systems more natural to interact with, and your feedback will help us improve our systems and make them safer. New Thread Dark Mode [OpenAI Discord](https://discord.gg/openai) [Updates & FAQ](https://help.openai.com/en/collections/3742473-chatgpt) Log out
ISSUE,  software quality, Bad performance during TestStep creation ### üëì What did you see? On my Java project with about 400 Cucumber test scenarios and about 150 step definitions, the IntelliJ flame graph shows that creating the TestStep takes about 10% of the total time (that's about 1 second on my project). Most of the time is consumed in the [`TestAbortedExceptions.createIsTestAbortedExceptionPredicate()`](https://github.com/cucumber/cucumber-jvm/blob/main/cucumber-core/src/main/java/io/cucumber/core/runner/TestAbortedExceptions.java) method. See the `flamegraph.png` file in the annexed ZIP file. ### ‚úÖ What did you expect to see? I expect the [`TestAbortedExceptions.createIsTestAbortedExceptionPredicate()`](https://github.com/cucumber/cucumber-jvm/blob/main/cucumber-core/src/main/java/io/cucumber/core/runner/TestAbortedExceptions.java) `Predicate<Throwable>` creation to be lightweight, with no noticeable impact on the overall execution time. I implemented some variants and did some benchmarking to determine what is the best alternative. Note: some optimisations have been produced using ChatGPT (on 19.12.2022). I could have written the code by myself but wanted to compare what an IA could produce. The produced code is impressive and could have been produced by human developers. But as human developers, ChatGPT makes mistakes (e.g. the first produced code was not able to identify subclasses), so it needs some code review and test harness. All variants have been tested using the following JUnit5 test (contains a two positive and one negative example):     @ParameterizedTest     @MethodSource("methods")     void testPredicate(String name, Predicate<Throwable> myMethod) {         assertFalse(myMethod.test(new RuntimeException()));         assertTrue(myMethod.test(new SkipException()));         assertTrue(myMethod.test(new SkipExceptionSubClass()));     } Even if the implemented variants pass this tests, there are slight differences with the classloading behavior: - **classloader**: the original Cucumber version takes the current thread classloader (when existing). This way, one Thread could have a classloader with JUnit4 AssumptionViolatedException and another Thread could have a classloader with TestNG SkipException. Such cases are probably rare, but the original code support it. Variants which use a static classloader are not able to support this use-case. - **caching classes**: the original Cucumber version loads the aborted exceptions classes each time the method is called. Thus, Cucumber is able to manage dynamic classloading (e.g. at first the classloader doesn't know a class, but it loads the class afterwards). Again, this dynamic classloader behavior is probably rare, but the original code support it.  As these use-cases are very rare, we could agree on changing the original behavior if the performance improvement is significative (to be decided by Cucumber core team). The table below summarizes the different variants: | Name                                   | Description                                                                                                                      | classloader | cached classes | code readability | ops/s      | |----------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|-------------|----------------|------------------|------------| | createIsTestAbortedExceptionPredicate0 | Original Cucumber 7.10.0 version                                                                                                 | by thread   | no             | low              | 5'517      | | createIsTestAbortedExceptionPredicate1 | ChatGPT (2nd trial) + human code review + minor human improvements                                                               | by thread   | no             | average          | 261'792'878 | | createIsTestAbortedExceptionPredicate6 | ChatGPT (3rd trial) + human code review + IntelliJ refactoring + minor human refactoring to make it compile/behave like original | by thread   | no             | high             | 266'445'123 | | createIsTestAbortedExceptionPredicate5 | Optimized by a human developer, with ThreadLocal cached Predicate                                                                | by thread   | yes            | average          | 260'493'066 | | createIsTestAbortedExceptionPredicate2 | ChatGPT (3rd trial) + human code review + IntelliJ refactoring + minor human refactoring to make it compile                      | static      | no             | high             | 443'061'213 | | createIsTestAbortedExceptionPredicate3 | ChatGPT (4rth trial with caching) + human code review + minor human refactoring to make it compile  + minor human improvements   | static      | yes            | low              | 443'764'493 | | createIsTestAbortedExceptionPredicate4 | Optimized by a human developer                                                                                                   | static      | yes            | average          | 443'207'453 | Based on the results above, my recommendation would be to use `createIsTestAbortedExceptionPredicate6` which keeps the same classloader behavior ("by thread"), without cached classes. This way, the original behavior is conserved, the code readability is improved and the performance is improved (about 48'000 times faster). We could have even better performance, but this would require to change the original behavior (e.g. `createIsTestAbortedExceptionPredicate5` or `createIsTestAbortedExceptionPredicate3`), which may cause side effects on Cucumber users. Below is the proposed method:     static Predicate<Throwable> createIsTestAbortedExceptionPredicate6() {         ClassLoader defaultClassLoader = ClassLoaders.getDefaultClassLoader();         return throwable -> Arrays.stream(TEST_ABORTED_EXCEPTIONS)                 .anyMatch(s -> {                     try {                         Class<?> aClass = defaultClassLoader.loadClass(s);                         return aClass.isInstance(throwable);                     } catch (Throwable t) {                         rethrowIfUnrecoverable(t);                         log.debug(t,                                 () -> String.format("Failed to load class %s: will not be supported for aborted executions.", s));                     }                     return false;                 });     } I can do a PR with the solution preferred by the Cucumber core team. ### üì¶ Which tool/library version are you using? Cucumber 7.10.0 ### üî¨ How could we reproduce it? The ZIP file in annex contains the JMH benchmark and the implemented variants.  [runner.zip](https://github.com/cucumber/cucumber-jvm/files/10268941/runner.zip) These files can be copied in a minimal Maven project with the following dependencies:         <dependency>             <groupId>io.cucumber</groupId>             <artifactId>cucumber-java</artifactId>             <version>${cucumber.version}</version>             <scope>test</scope>         </dependency>         <dependency>             <groupId>io.cucumber</groupId>             <artifactId>cucumber-junit-platform-engine</artifactId>             <version>${cucumber.version}</version>             <scope>test</scope>             <exclusions>                 <exclusion><!-- conflicts with the version from junit-platform-suite -->                     <groupId>org.junit.platform</groupId>                     <artifactId>junit-platform-engine</artifactId>                 </exclusion>             </exclusions>         </dependency>         <dependency>             <groupId>io.cucumber</groupId>             <artifactId>cucumber-picocontainer</artifactId>             <version>${cucumber.version}</version>             <scope>test</scope>         </dependency>         <dependency>             <groupId>org.openjdk.jmh</groupId>             <artifactId>jmh-generator-annprocess</artifactId>             <version>1.36</version>             <scope>test</scope>         </dependency> 
ISSUE,  learning, Tab Titles not showing up in vscode jupyter notebooks ## Description this code should make two tabs both with individual buttons, and labeled appropriately.  yet it seems it doesnt show the label on the tabs for some reason. ``` import ipywidgets as widgets # Create the tab widget tab = widgets.Tab() # Create the first tab and its content - a button tab1 = widgets.VBox() button1 = widgets.Button(description='Button 1') tab1.children = [button1] # Create the second tab and its content - a button tab2 = widgets.VBox() button2 = widgets.Button(description='Button 2') tab2.children = [button2] # Add the tabs to the tab widget tab.children = [tab1, tab2] # Set the tab titles tab.set_title(0, 'Tab 1') tab.set_title(1, 'Tab 2') # Display the tab widget tab ``` A screenshot for clarification ![image](https://user-images.githubusercontent.com/16056818/209394017-dbb5271e-dd9b-4abc-9f63-dfd04046ed19.png) As you probably can see, the titles are empty in this screenshot, ive tryed to reproduce this in colab, jupyterlite and these ides dont seem to share the same issue, though every time i try in vscode the tabs are unlabeled, or atleast the label wont show. ## Reproduce 1. copy code & run.  (1. create a tab with a little content using widgets.Tab(), widgets.Button() and tab.children = buttons 2. display the tab in vscode with jupyter notebook extention ## What i did  1. Ask chatgpt for some sample code on how to make a very simple gui to try to learn some basics for making gui in python 2. ctrl-c, ctrl-v 3. run into this error 4. problem solve for a couple of hours trying diffrent ides and packages for gui in python, only to find that this package is what you are looking for but it doesn't work in the ide you preferr "grrr..." ## Expected behavior labeled tabs like the documentation shows but then in vscode : https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20List.html#Tabs ## Context <!--Complete the following for context, and add any other relevant context--> - ipywidgets version 8.0.4 - Operating System and version: windows 10, fresh reset (harddrive malfunction) - Browser and version: browser is chrome but not necessary for the ide.
ISSUE,  learning, little curiosity I was using ChatGPT to check a possibility using the Zeep library... And then I received the following responses: ![Screenshot from 2023-01-02 10-41-29](https://user-images.githubusercontent.com/32274515/210239736-a294c9a0-13c9-4674-8a9a-2d1985332308.png) ![Screenshot from 2023-01-02 10-53-57](https://user-images.githubusercontent.com/32274515/210240625-39e42869-684b-4d15-874b-b12585d24143.png) ![Screenshot from 2023-01-02 10-45-17](https://user-images.githubusercontent.com/32274515/210239749-8d061424-de7c-4458-baf1-c34df9cf9b1a.png) ![Screenshot from 2023-01-02 10-46-15](https://user-images.githubusercontent.com/32274515/210239751-5517479b-31c3-4c13-9fc4-72226ccff14c.png) I couldn't find anything about the real existence of this method... Did it really exist or does it exist? 
ISSUE,  development environment, Unable to build microROS_agent inside docker container Hi Dusty! First of all, thank you for all the efforts you put into building these containers, the easy-to-follow tutorials, and all the community engagement you do! I'm really grateful for such a good source of information. For weeks now, I have been struggling to get the Jetson Nano Developer Kit (4GB, B01 with 2 MIPI-ports) to work properly. I'm trying to get a docker container with ROS2 running. Ideally that would be Humble, but I guess I will have to get by with Foxy, as Galactic is EOL. This has been tried on multiple Jetson Nano Developer Kits, as I have two of them. Here are the steps outlined of what has been done: REGULAR DOCKER IMAGE WORKFLOW Flash the device with the latest sdkmanager to JetPack 4.6.3. (I have also tried flashing it to 32.7.1, but the steps below still fail. If I opt for DeepStream or not also does not change the outcome.) After the device is finished, log in and run apt-get update && upgrade (In some trials, this step was omitted and yet the following procedure still failed.) Clone the GitHub repository (git clone https://github.com/dusty-nv/jetson-containers.git) Ensure "default-runtime": "nvidia" is entered in the /etc/docker/daemon.json, properly formatted and comma added after the prior entry! Then run one of the following scripts: ```     scripts/docker_run.sh -c dustynv/ros:foxy-ros-base-l4t-r32.7.1     scripts/docker_run.sh -c dustynv/ros:foxy-pytorch-l4t-r32.7.1 ``` This fails because I'm already on 32.7.3. Modified the l4t_version.sh to fake the output to show R32.7.1 (Even if this step is not needed due to being on Jetpack 4.6.1 and R32.7.1, as mentioned above, the process fails) Then attempt to run the docker command from step 5 again, and it actually works, loading the docker. Following the tutorial for microROS for Teensy  `(https://micro.ros.org/docs/tutorials/core/teensy_with_arduino/),` the process fails at the following step: ``` # Build step ros2 run micro_ros_setup build_agent.sh ``` Also, the very first source command to source the environment is wrong. It is: ``` # Source the ROS 2 installation source /opt/ros/foxy/setup.bash ``` But it should be: source /opt/ros/foxy/install/setup.bash However, despite this small oversight, the agent build still fails. I know this is not related to the Jetpack version, as mentioned it fails with 4.6.1 as well. At some point, I was able to get it working, but I'm unsure what I have done differently and why it suddenly wouldn't work anymore. Output logs are to follow below. BUILD DOCKER IMAGE WORKFLOW Then I decided to build the docker container from source, as I thought maybe there is something intrinsically different in my system, and building from source will sort all dependencies properly. This has been tried under the following circumstances: 1. With the device flashed to JetPack 4.6.1 (R32.7.1) - Directly after flashing (no apt-get update && upgrade) 2. With the device flashed to JetPack 4.6.1 (R32.7.1) - With apt-get update && upgrade 3. With the device flashed to JetPack 4.6.3 (R32.7.2) and the modified l4t_version.sh to fake the output to show R32.7.1 - Directly after flashing (no apt-get update && upgrade) 4. With the device flashed to JetPack 4.6.3 (R32.7.2) and the modified l4t_version.sh to fake the output to show R32.7.1 - With apt-get update && upgrade No matter what I attempt to do, the build process also fails. So to summarize, my situation is as follows:     Unable to get the microROS agent to build (ros2 run micro_ros_setup build_agent.sh)     Unable to build the docker image from source Output logs are to follow below. Thank you for your time! Here the entire output from an attempt on a freshly flashed Jetson Nano with Jetpack 4.6.1 (R32.7.1). entire trace below: Because I have done this process so many times and am getting tired of typing / copying and pasting all commands, I have thrown them into a shell script. But if I do it manually of with the script, the outcome is the same. Here the script: ``` #! /bin/bash # VARIABLES: # ---------- DIRECTORY="microros_ws" ENV_SETUP="setup.bash" # 1. check if directory exists: if [ ! -d "$DIRECTORY" ]; then   echo "$DIRECTORY does not exist. Creating"   mkdir $DIRECTORY else   echo "$DIRECTORY does exist." fi # 2. enter the directory cd $DIRECTORY pwd # Source the ROS 2 installation if [ -e /opt/ros/$ROS_DISTRO/setup.bash ]; then   echo "setup.bash found at path 1"   source /opt/ros/$ROS_DISTRO/setup.bash elif [ -e /opt/ros/$ROS_DISTRO/install/setup.bash ]; then   echo "setup.bash found at path 2"   source /opt/ros/$ROS_DISTRO/install/setup.bash fi # clone microROS repository git clone -b $ROS_DISTRO https://github.com/micro-ROS/micro_ros_setup.git src/micro_ros_setup # Update dependencies using rosdep sudo apt update && rosdep update rosdep install --from-paths src --ignore-src -y # Install pip sudo apt-get install python3-pip # Build micro-ROS tools and source them colcon build source install/local_setup.bash # Download micro-ROS agent packages ros2 run micro_ros_setup create_agent_ws.sh # Build step ros2 run micro_ros_setup build_agent.sh source install/local_setup.bash ``` And here the terminal trace from executing the script: ``` root@jetson2:/# nano microROS_agent.sh root@jetson2:/# clear root@jetson2:/# chmod 755 microROS_agent.sh  root@jetson2:/# ./microROS_agent.sh  microros_ws does not exist. Creating /microros_ws setup.bash found at path 2 Cloning into 'src/micro_ros_setup'... remote: Enumerating objects: 3385, done. remote: Counting objects: 100% (1180/1180), done. remote: Compressing objects: 100% (507/507), done. remote: Total 3385 (delta 843), reused 957 (delta 666), pack-reused 2205 Receiving objects: 100% (3385/3385), 794.48 KiB | 2.47 MiB/s, done. Resolving deltas: 100% (2341/2341), done. Hit:1 http://packages.ros.org/ros2/ubuntu bionic InRelease Hit:2 http://ports.ubuntu.com/ubuntu-ports bionic InRelease                                        Hit:3 http://ports.ubuntu.com/ubuntu-ports bionic-updates InRelease                                Hit:4 https://apt.kitware.com/ubuntu bionic InRelease                                             Hit:5 http://ports.ubuntu.com/ubuntu-ports bionic-backports InRelease    Hit:6 http://ports.ubuntu.com/ubuntu-ports bionic-security InRelease Reading package lists... Done                       Building dependency tree        Reading state information... Done 183 packages can be upgraded. Run 'apt list --upgradable' to see them. reading in sources list data from /etc/ros/rosdep/sources.list.d Warning: running 'rosdep update' as root is not recommended.   You should run 'sudo rosdep fix-permissions' and invoke 'rosdep update' again without sudo. Hit https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/osx-homebrew.yaml Hit https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/base.yaml Hit https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/python.yaml Hit https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/ruby.yaml Hit https://raw.githubusercontent.com/ros/rosdistro/master/releases/fuerte.yaml Query rosdistro index https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml Skip end-of-life distro "ardent" Skip end-of-life distro "bouncy" Skip end-of-life distro "crystal" Skip end-of-life distro "dashing" Skip end-of-life distro "eloquent" Add distro "foxy" Skip end-of-life distro "galactic" Skip end-of-life distro "groovy" Add distro "humble" Skip end-of-life distro "hydro" Skip end-of-life distro "indigo" Skip end-of-life distro "jade" Skip end-of-life distro "kinetic" Skip end-of-life distro "lunar" Add distro "melodic" Add distro "noetic" Add distro "rolling" updated cache in /root/.ros/rosdep/sources.cache executing command [apt-get install -y clang-tidy] Reading package lists... Done Building dependency tree        Reading state information... Done The following additional packages will be installed:   clang-tidy-6.0 libllvm6.0 The following NEW packages will be installed:   clang-tidy clang-tidy-6.0 libllvm6.0 0 upgraded, 3 newly installed, 0 to remove and 183 not upgraded. Need to get 18.5 MB of archives. After this operation, 78.2 MB of additional disk space will be used. Get:1 http://ports.ubuntu.com/ubuntu-ports bionic/main arm64 libllvm6.0 arm64 1:6.0-1ubuntu2 [12.5 MB] Get:2 http://ports.ubuntu.com/ubuntu-ports bionic/universe arm64 clang-tidy-6.0 arm64 1:6.0-1ubuntu2 [5,916 kB]                     Get:3 http://ports.ubuntu.com/ubuntu-ports bionic-updates/universe arm64 clang-tidy arm64 1:6.0-41~exp5~ubuntu1 [2,984 B]           Fetched 18.5 MB in 15s (1,210 kB/s)                                                                                                 debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package libllvm6.0:arm64. (Reading database ... 81206 files and directories currently installed.) Preparing to unpack .../libllvm6.0_1%3a6.0-1ubuntu2_arm64.deb ... Unpacking libllvm6.0:arm64 (1:6.0-1ubuntu2) ... Selecting previously unselected package clang-tidy-6.0. Preparing to unpack .../clang-tidy-6.0_1%3a6.0-1ubuntu2_arm64.deb ... Unpacking clang-tidy-6.0 (1:6.0-1ubuntu2) ... Selecting previously unselected package clang-tidy. Preparing to unpack .../clang-tidy_1%3a6.0-41~exp5~ubuntu1_arm64.deb ... Unpacking clang-tidy (1:6.0-41~exp5~ubuntu1) ... Setting up libllvm6.0:arm64 (1:6.0-1ubuntu2) ... Setting up clang-tidy-6.0 (1:6.0-1ubuntu2) ... Setting up clang-tidy (1:6.0-41~exp5~ubuntu1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Processing triggers for libc-bin (2.27-3ubuntu1.4) ... /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-rmapi-tegra.so.418.00 is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libv4l2_nvcuvidvideocodec.so is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-glcore.so.418.00 is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-glvkspirv.so.418.00 is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-tls.so.418.00 is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-glsi.so.418.00 is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvcapture.so is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvpva.so is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvisp_utils.so is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvcuvidv4l2.so is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-eglcore.so.418.00 is empty, not checked. executing command [apt-get install -y flex] Reading package lists... Done Building dependency tree        Reading state information... Done The following additional packages will be installed:   libfl-dev libfl2 Suggested packages:   bison flex-doc The following NEW packages will be installed:   flex libfl-dev libfl2 0 upgraded, 3 newly installed, 0 to remove and 183 not upgraded. Need to get 320 kB of archives. After this operation, 1,129 kB of additional disk space will be used. Get:1 http://ports.ubuntu.com/ubuntu-ports bionic/main arm64 flex arm64 2.6.4-6 [302 kB] Get:2 http://ports.ubuntu.com/ubuntu-ports bionic/main arm64 libfl2 arm64 2.6.4-6 [11.3 kB] Get:3 http://ports.ubuntu.com/ubuntu-ports bionic/main arm64 libfl-dev arm64 2.6.4-6 [6,232 B] Fetched 320 kB in 2s (137 kB/s)       debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package flex. (Reading database ... 81237 files and directories currently installed.) Preparing to unpack .../flex_2.6.4-6_arm64.deb ... Unpacking flex (2.6.4-6) ... Selecting previously unselected package libfl2:arm64. Preparing to unpack .../libfl2_2.6.4-6_arm64.deb ... Unpacking libfl2:arm64 (2.6.4-6) ... Selecting previously unselected package libfl-dev:arm64. Preparing to unpack .../libfl-dev_2.6.4-6_arm64.deb ... Unpacking libfl-dev:arm64 (2.6.4-6) ... Setting up flex (2.6.4-6) ... Setting up libfl2:arm64 (2.6.4-6) ... Setting up libfl-dev:arm64 (2.6.4-6) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Processing triggers for libc-bin (2.27-3ubuntu1.4) ... /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-rmapi-tegra.so.418.00 is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libv4l2_nvcuvidvideocodec.so is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-glcore.so.418.00 is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-glvkspirv.so.418.00 is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-tls.so.418.00 is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-glsi.so.418.00 is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvcapture.so is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvpva.so is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvisp_utils.so is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvcuvidv4l2.so is empty, not checked. /sbin/ldconfig.real: File /usr/lib/aarch64-linux-gnu/tegra/libnvidia-eglcore.so.418.00 is empty, not checked. executing command [apt-get install -y bison] Reading package lists... Done Building dependency tree        Reading state information... Done The following additional packages will be installed:   libbison-dev Suggested packages:   bison-doc The following NEW packages will be installed:   bison libbison-dev 0 upgraded, 2 newly installed, 0 to remove and 183 not upgraded. Need to get 568 kB of archives. After this operation, 1,741 kB of additional disk space will be used. Get:1 http://ports.ubuntu.com/ubuntu-ports bionic/main arm64 libbison-dev arm64 2:3.0.4.dfsg-1build1 [339 kB] Get:2 http://ports.ubuntu.com/ubuntu-ports bionic/main arm64 bison arm64 2:3.0.4.dfsg-1build1 [230 kB] Fetched 568 kB in 3s (227 kB/s) debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package libbison-dev:arm64. (Reading database ... 81327 files and directories currently installed.) Preparing to unpack .../libbison-dev_2%3a3.0.4.dfsg-1build1_arm64.deb ... Unpacking libbison-dev:arm64 (2:3.0.4.dfsg-1build1) ... Selecting previously unselected package bison. Preparing to unpack .../bison_2%3a3.0.4.dfsg-1build1_arm64.deb ... Unpacking bison (2:3.0.4.dfsg-1build1) ... Setting up libbison-dev:arm64 (2:3.0.4.dfsg-1build1) ... Setting up bison (2:3.0.4.dfsg-1build1) ... update-alternatives: using /usr/bin/bison.yacc to provide /usr/bin/yacc (yacc) in auto mode update-alternatives: warning: skip creation of /usr/share/man/man1/yacc.1.gz because associated file /usr/share/man/man1/bison.yacc.1.gz (of link group yacc) doesn't exist Processing triggers for man-db (2.8.3-2ubuntu0.1) ... executing command [apt-get install -y libncurses5-dev] Reading package lists... Done Building dependency tree        Reading state information... Done The following additional packages will be installed:   libtinfo-dev Suggested packages:   ncurses-doc The following NEW packages will be installed:   libncurses5-dev libtinfo-dev 0 upgraded, 2 newly installed, 0 to remove and 183 not upgraded. Need to get 237 kB of archives. After this operation, 1,322 kB of additional disk space will be used. Get:1 http://ports.ubuntu.com/ubuntu-ports bionic-updates/main arm64 libtinfo-dev arm64 6.1-1ubuntu1.18.04 [74.2 kB] Get:2 http://ports.ubuntu.com/ubuntu-ports bionic-updates/main arm64 libncurses5-dev arm64 6.1-1ubuntu1.18.04 [163 kB] Fetched 237 kB in 2s (109 kB/s)           debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package libtinfo-dev:arm64. (Reading database ... 81379 files and directories currently installed.) Preparing to unpack .../libtinfo-dev_6.1-1ubuntu1.18.04_arm64.deb ... Unpacking libtinfo-dev:arm64 (6.1-1ubuntu1.18.04) ... Selecting previously unselected package libncurses5-dev:arm64. Preparing to unpack .../libncurses5-dev_6.1-1ubuntu1.18.04_arm64.deb ... Unpacking libncurses5-dev:arm64 (6.1-1ubuntu1.18.04) ... Setting up libtinfo-dev:arm64 (6.1-1ubuntu1.18.04) ... Setting up libncurses5-dev:arm64 (6.1-1ubuntu1.18.04) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... executing command [apt-get install -y usbutils] Reading package lists... Done Building dependency tree        Reading state information... Done The following NEW packages will be installed:   usbutils 0 upgraded, 1 newly installed, 0 to remove and 183 not upgraded. Need to get 202 kB of archives. After this operation, 717 kB of additional disk space will be used. Get:1 http://ports.ubuntu.com/ubuntu-ports bionic/main arm64 usbutils arm64 1:007-4build1 [202 kB] Fetched 202 kB in 2s (109 kB/s)     debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package usbutils. (Reading database ... 81428 files and directories currently installed.) Preparing to unpack .../usbutils_1%3a007-4build1_arm64.deb ... Unpacking usbutils (1:007-4build1) ... Setting up usbutils (1:007-4build1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... #All required rosdeps installed successfully Reading package lists... Done Building dependency tree        Reading state information... Done python3-pip is already the newest version (9.0.1-2.3~ubuntu1.18.04.5). 0 upgraded, 0 newly installed, 0 to remove and 183 not upgraded. Starting >>> micro_ros_setup Finished <<< micro_ros_setup [6.20s]                                         Summary: 1 package finished [7.72s] ... === ./uros/drive_base (git) === Cloning into '.'... === ./uros/micro-ROS-Agent (git) === Cloning into '.'... === ./uros/micro_ros_msgs (git) === Cloning into '.'... #All required rosdeps installed successfully Building micro-ROS Agent Starting >>> micro_ros_msgs [Processing: micro_ros_msgs]                              Finished <<< micro_ros_msgs [46.3s]                        Starting >>> micro_ros_agent [Processing: micro_ros_agent]                                   [Processing: micro_ros_agent]                                   [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                         [Processing: micro_ros_agent]                                       [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                        [Processing: micro_ros_agent]                                          [Processing: micro_ros_agent]                                         --- stderr: micro_ros_agent                                             Cloning into 'xrceagent'... Switched to a new branch 'ros2' HEAD is now at 3eb56b5 Release v2.3.0 Cloning into 'spdlog'... HEAD is now at eb322062 Bump version to 1.9.2 CMake Warning (dev) at /usr/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):   The package name passed to `find_package_handle_standard_args` (tinyxml2)   does not match the name of the calling package (TinyXML2).  This can lead   to problems in calling code that expects `find_package` result variables   (e.g., `_FOUND`) to follow a certain pattern. Call Stack (most recent call first):   cmake/modules/FindTinyXML2.cmake:40 (find_package_handle_standard_args)   /opt/ros/foxy/install/share/fastrtps/cmake/fastrtps-config.cmake:51 (find_package)   CMakeLists.txt:153 (find_package) This warning is for project developers.  Use -Wno-dev to suppress it. /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `fmt::v6::internal::basic_data<void>::signs' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `vtable for fmt::v6::format_error' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `int fmt::v6::internal::snprintf_float<double>(double, int, fmt::v6::internal::float_specs, fmt::v6::internal::buffer<char>&)' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `fmt::v6::internal::error_handler::on_error(char const*)' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `char fmt::v6::internal::decimal_point_impl<char>(fmt::v6::internal::locale_ref)' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `fmt::v6::internal::basic_data<void>::zero_or_powers_of_10_32' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `char fmt::v6::internal::thousands_sep_impl<char>(fmt::v6::internal::locale_ref)' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `std::locale fmt::v6::internal::locale_ref::get<std::locale>() const' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `typeinfo for fmt::v6::format_error' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > fmt::v6::internal::grouping_impl<char>(fmt::v6::internal::locale_ref)' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `int fmt::v6::internal::format_float<long double>(long double, int, fmt::v6::internal::float_specs, fmt::v6::internal::buffer<char>&)' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `spdlog::details::log_msg::log_msg(spdlog::source_loc, fmt::v6::basic_string_view<char>, spdlog::level::level_enum, fmt::v6::basic_string_view<char>)' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `int fmt::v6::internal::snprintf_float<long double>(long double, int, fmt::v6::internal::float_specs, fmt::v6::internal::buffer<char>&)' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `fmt::v6::format_error::~format_error()' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `fmt::v6::internal::basic_data<void>::hex_digits' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `fmt::v6::internal::basic_data<void>::digits' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `int fmt::v6::internal::format_float<double>(double, int, fmt::v6::internal::float_specs, fmt::v6::internal::buffer<char>&)' /microros_ws/install/micro_ros_agent/lib/libmicroxrcedds_agent.so.2.3.0: undefined reference to `fmt::v6::internal::basic_data<void>::zero_or_powers_of_10_64' collect2: error: ld returned 1 exit status make[5]: *** [micro_ros_agent] Error 1 make[4]: *** [CMakeFiles/micro_ros_agent.dir/all] Error 2 make[3]: *** [all] Error 2 make[2]: *** [micro_ros_agent-prefix/src/micro_ros_agent-stamp/micro_ros_agent-build] Error 2 make[1]: *** [CMakeFiles/micro_ros_agent.dir/all] Error 2 make: *** [all] Error 2 --- Failed   <<< micro_ros_agent [10min 14s, exited with code 2] Summary: 1 package finished [11min 1s]   1 package failed: micro_ros_agent   1 package had stderr output: micro_ros_agent root@jetson2:/#  ``` I'm so desperate for a solution and actually have chatted with chatGPT :-/ which suggested to: ``` apt-get update apt-get install -y build-essential apt-get install libfmt-dev ``` Which I have done prior to running the script, but to little success, as above shows :-/ Would you kindly advice? 
ISSUE,  process, Chunks in the Nether are offset downwards ### Description: I've noticed that chunks in the Nether seem to be offset downwards, while otherwise being mostly fine. This is causing issues with players being able to build and navigate in the Nether.  ### Steps to reproduce: 1. Enter the Nether dimension in the game 2. Notice that chunks seem to be offset downwards ### Expected outcome: Chunks should be at the same level as the surrounding chunks, allowing for normal building and navigation. ### Actual outcome: Chunks are offset downwards, making it difficult to build and navigate in the Nether. ### Notes: Issue made with chatgpt lol ### Environment: - Game version:  Commit a9dc10aacd736c5661f0f86a63c725d2a7320eac - Operating system: Windows 11 - Hardware: RTX 3070 Ti, i7 12700F
ISSUE,  software quality, Can't get deploy my app as a google cloud function. ('app' needs to be of type function. Got: object) ### Description Has anyone been able to deploy to Google cloud function? Google cloud require the parameter --entry-point and it needs to be a function. if I export my bolt app: `exports.app = app` and run  `gcloud functions deploy slack --runtime nodejs16 --trigger-http --entry-point app ` I get the following error from google cloud: **'app' needs to be of type function. Got: object** ChatGPT has been trying to solve the issue but it's also not working. ```javascript const { App } = require("@slack/bolt"); const dotenv = require('dotenv'); dotenv.config(); const app = new App({ token: process.env.SLACK_BOT_TOKEN, signingSecret: process.env.SLACK_SIGNING_SECRET }); exports.slackBot = app.requestHandler; ``` I get then the following error: **Function 'slackBot' is not defined in the provided module.** ### What type of issue is this? (place an `x` in one of the `[ ]`) - [ ] bug - [ ] enhancement (feature request) - [x] question - [ ] documentation related - [ ] example code related - [ ] testing related - [ ] discussion ### Requirements (place an `x` in each of the `[ ]`) * [x] I've read and understood the [Contributing guidelines](https://github.com/slackapi/bolt/blob/main/.github/contributing.md) and have done my best effort to follow them. * [x] I've read and agree to the [Code of Conduct](https://slackhq.github.io/code-of-conduct). * [x] I've searched for any related issues and avoided creating a duplicate issue. 
ISSUE,  process, üêõ [Bug]: BodyParser doesn't work when receiving form data where the keys contain underscores ### Bug Description I was having issues signing up a user as their `first_name` and `last_name` weren't being stored on the struct as required after running it through `BodyParser`. I noticed the `email` and `password` fields were being filled in so I modified the codebase to use `firstname` and `lastname`. The issue then went away. I'm not sure if this is how it is supposed to be. Asking ChatGPT suggested it to be a bug. When sending over the data in the form of JSON everything worked fine so the problem is just with handling form data. ### How to Reproduce Steps to reproduce the behavior: 1. Send a request where the keys use underscores to a controller that runs BodyParser on the request body to store in a struct. 2. Send the request in JSON 3. Send the request as form data. 4. Notice how underscore'd fields aren't being filled in when using form data. ### Expected Behavior I expect form data to allow underscores in the keys. ### Fiber Version v2.41.0 ### Code Snippet (optional) ```go type User struct { 	gorm.Model 	Email     string `json:"email"` 	FirstName string `json:"first_name"` 	LastName  string `json:"last_name"` 	Password  string `json:"password"` } # issue is below func UserCreate(c *fiber.Ctx) error { 	user := new(models.User) 	if err := c.BodyParser(user); err != nil { 		return err 	} 	return c.JSON(user) } ``` ## Example > Works ``` POST http://localhost:8080/signup/ Content-Type: application/x-www-form-urlencoded firstname=John&lastname=Doe&email=johndoe@example.com&password=my_secure_password ``` > Doesn't work ``` POST http://localhost:8080/signup/ Content-Type: application/x-www-form-urlencoded first_name=John&last_name=Doe&email=johndoe@example.com&password=my_secure_password ``` ### Checklist: - [X] I agree to follow Fiber's [Code of Conduct](https://github.com/gofiber/fiber/blob/master/.github/CODE_OF_CONDUCT.md). - [X] I have checked for existing issues that describe my problem prior to opening this one. - [X] I understand that improperly formatted bug reports may be closed without explanation.
ISSUE,  software quality, fix to the: AttributeError: 'JSON' object has no attribute 'value' Was getting the error after automatic1111's update so i used chatgpt to fix it ... not sure how Github works with fixes to errors so i gonna put it here: ```` import os import glob import zipfile import json import stat import sys import inspect from collections import OrderedDict import torch import modules.scripts as scripts from modules import shared, script_callbacks import gradio as gr from modules.processing import Processed, process_images from modules import sd_models import modules.ui from scripts import lora_compvis MAX_MODEL_COUNT = 5 LORA_MODEL_EXTS = [".pt", ".ckpt", ".safetensors"] lora_models = {}      # "My_Lora(abcd1234)" -> C:/path/to/model.safetensors lora_model_names = {} # "my_lora" -> "My_Lora(abcd1234)" lora_models_dir = os.path.join(scripts.basedir(), "models/lora") os.makedirs(lora_models_dir, exist_ok=True) def traverse_all_files(curr_path, model_list):   f_list = [(os.path.join(curr_path, entry.name), entry.stat()) for entry in os.scandir(curr_path)]   for f_info in f_list:     fname, fstat = f_info     if os.path.splitext(fname)[1] in LORA_MODEL_EXTS:       model_list.append(f_info)     elif stat.S_ISDIR(fstat.st_mode):       model_list = traverse_all_files(fname, model_list)   return model_list def get_all_models(sort_by, filter_by, path):   res = OrderedDict()   fileinfos = traverse_all_files(path, [])   filter_by = filter_by.strip(" ")   if len(filter_by) != 0:     fileinfos = [x for x in fileinfos if filter_by.lower() in os.path.basename(x[0]).lower()]   if sort_by == "name":     fileinfos = sorted(fileinfos, key=lambda x: os.path.basename(x[0]))   elif sort_by == "date":     fileinfos = sorted(fileinfos, key=lambda x: -x[1].st_mtime)   elif sort_by == "path name":     fileinfos = sorted(fileinfos)   for finfo in fileinfos:     filename = finfo[0]     name = os.path.splitext(os.path.basename(filename))[0]     # Prevent a hypothetical "None.pt" from being listed.     if name != "None":       res[name + f"({sd_models.model_hash(filename)})"] = filename   return res def find_closest_lora_model_name(search: str):     if not search:         return None     if search in lora_models:         return search     search = search.lower()     if search in lora_model_names:         return lora_model_names.get(search)     applicable = [name for name in lora_model_names.keys() if search in name.lower()]     if not applicable:         return None     applicable = sorted(applicable, key=lambda name: len(name))     return lora_model_names[applicable[0]] def update_lora_models():   global lora_models, lora_model_names   res = OrderedDict()   paths = [lora_models_dir]   extra_lora_path = shared.opts.data.get("additional_networks_extra_lora_path", None)   if extra_lora_path and os.path.exists(extra_lora_path):     paths.append(extra_lora_path)   for path in paths:     sort_by = shared.opts.data.get("additional_networks_sort_models_by", "name")     filter_by = shared.opts.data.get("additional_networks_model_name_filter", "")     found = get_all_models(sort_by, filter_by, path)     res = {**found, **res}   lora_models = OrderedDict(**{"None": None}, **res)   lora_model_names = {}   for name_and_hash, filename in lora_models.items():       if filename == None:           continue       name = os.path.splitext(os.path.basename(filename))[0].lower()       lora_model_names[name] = name_and_hash update_lora_models() class Script(scripts.Script):   def __init__(self) -> None:     super().__init__()     self.latest_params = [(None, None, None)] * MAX_MODEL_COUNT     self.latest_networks = []     self.latest_model_hash = ""   def title(self):     return "Additional networks for generating"   def show(self, is_img2img):     return scripts.AlwaysVisible   def ui(self, is_img2img):     # NOTE: Changing the contents of `ctrls` means the XY Grid support may need     # to be updated, see end of file     ctrls = []     model_dropdowns = []     self.infotext_fields = []     with gr.Group():       with gr.Accordion('Additional Networks', open=False):         enabled = gr.Checkbox(label='Enable', value=False)         ctrls.append(enabled)         self.infotext_fields.append((enabled, "AddNet Enabled"))         for i in range(MAX_MODEL_COUNT):           with gr.Row():             module = gr.Dropdown(["LoRA"], label=f"Network module {i+1}", value="LoRA")             model = gr.Dropdown(list(lora_models.keys()),                                 label=f"Model {i+1}",                                 value="None")             weight = gr.Slider(label=f"Weight {i+1}", value=1.0, minimum=-1.0, maximum=2.0, step=.05)           ctrls.extend((module, model, weight))           model_dropdowns.append(model)           self.infotext_fields.extend([               (module, f"AddNet Module {i+1}"),               (model, f"AddNet Model {i+1}"),               (weight, f"AddNet Weight {i+1}"),           ])         def refresh_all_models(*dropdowns):           update_lora_models()           updates = []           for dd in dropdowns:             if dd in lora_models:               selected = dd             else:               selected = "None"             update = gr.Dropdown.update(value=selected, choices=list(lora_models.keys()))             updates.append(update)           return updates         refresh_models = gr.Button(value='Refresh models')         refresh_models.click(refresh_all_models, inputs=model_dropdowns, outputs=model_dropdowns)         ctrls.append(refresh_models)     return ctrls   def set_infotext_fields(self, p, params):     for i, t in enumerate(params):       module, model, weight = t       if model is None or model == "None" or len(model) == 0 or weight == 0:         continue       p.extra_generation_params.update({           "AddNet Enabled": True,           f"AddNet Module {i+1}": module,           f"AddNet Model {i+1}": model,           f"AddNet Weight {i+1}": weight,       })   def process(self, p, *args):     unet = p.sd_model.model.diffusion_model     text_encoder = p.sd_model.cond_stage_model     def restore_networks():       if len(self.latest_networks) > 0:         print("restoring last networks")         for network, _ in self.latest_networks[::-1]:           network.restore(text_encoder, unet)         self.latest_networks.clear()     if not args[0]:       restore_networks()       return     params = []     for i, ctrl in enumerate(args[1:]):       if i % 3 == 0:         param = [ctrl]       else:         param.append(ctrl)         if i % 3 == 2:           params.append(param)     models_changed = (len(self.latest_networks) == 0)                   # no latest network (cleared by check-off)     models_changed = models_changed or self.latest_model_hash != p.sd_model.sd_model_hash     if not models_changed:       for (l_module, l_model, l_weight), (module, model, weight) in zip(self.latest_params, params):         if l_module != module or l_model != model or l_weight != weight:           models_changed = True           break     if models_changed:       restore_networks()       self.latest_params = params       self.latest_model_hash = p.sd_model.sd_model_hash       for module, model, weight in self.latest_params:         if model is None or model == "None" or len(model) == 0:           continue         if weight == 0:           print(f"ignore because weight is 0: {model}")           continue         model_path = lora_models.get(model, None)         if model_path is None:           raise RuntimeError(f"model not found: {model}")         if model_path.startswith("\"") and model_path.endswith("\""):             # trim '"' at start/end           model_path = model_path[1:-1]         if not os.path.exists(model_path):           print(f"file not found: {model_path}")           continue         print(f"{module} weight: {weight}, model: {model}")         if module == "LoRA":           if os.path.splitext(model_path)[1] == '.safetensors':             from safetensors.torch import load_file             du_state_dict = load_file(model_path)           else:             du_state_dict = torch.load(model_path, map_location='cpu')           network, info = lora_compvis.create_network_and_apply_compvis(du_state_dict, weight, text_encoder, unet)           network.to(p.sd_model.device, dtype=p.sd_model.dtype)         # in medvram, device is different for u-net and sd_model, so use sd_model's           print(f"LoRA model {model} loaded: {info}")           self.latest_networks.append((network, model))       if len(self.latest_networks) > 0:         print("setting (or sd model) changed. new networks created.")     self.set_infotext_fields(p, self.latest_params) def read_lora_metadata(model_path, module):   if model_path.startswith("\"") and model_path.endswith("\""):             # trim '"' at start/end     model_path = model_path[1:-1]   if not os.path.exists(model_path):     return None   metadata = None   if module == "LoRA":     if os.path.splitext(model_path)[1] == '.safetensors':       from safetensors.torch import safe_open       with safe_open(model_path, framework="pt") as f:         metadata = f.metadata()   return metadata def on_ui_tabs():   with gr.Blocks(analytics_enabled=False) as additional_networks_interface:     with gr.Row().style(equal_height=False):       with gr.Column(variant='panel'):         with gr.Row():           module = gr.Dropdown(["LoRA"], label=f"Network module (used throughout this tab)", value="LoRA", interactive=True)           model = gr.Dropdown(list(lora_models.keys()), label=f"Model", value="None", interactive=True)           modules.ui.create_refresh_button(model, update_lora_models, lambda: {"choices": list(lora_models.keys())}, "refresh_lora_models")         with gr.Row():             with gr.Column():               gr.HTML(value="Get comma-separated list of models (for XY Grid)")               model_dir = gr.Textbox("", label=f"Model directory", placeholder="Optional, uses selected model's directory if blank")               model_sort_by = gr.Radio(label="Sort models by", choices=["name", "date", "path name"], value="name", type="value")               get_list_button = gr.Button("Get List")             with gr.Column():               model_list = gr.Textbox(value="", label="Model list", placeholder="Model list will be output here")       with gr.Column():         metadata_view = gr.JSON(data="test", label="Network metadata")     def update_metadata(module, model):       if model == "None":         return {}       model_path = lora_models.get(model, None)       if model_path is None:         metadata_view.data = f"file not found: {model}"       metadata = read_lora_metadata(model_path, module)       if metadata is None:         return "No metadata found."       else:         return metadata     model.change(update_metadata, inputs=[module, model], outputs=[metadata_view])     def output_model_list(module, model, model_dir, sort_by):         if model_dir == "":             # Get list of models with same folder as this one             model_path = lora_models.get(model, None)             if model_path is None:                 model_list.value = f"directory not found: {model_dir}"             model_dir = os.path.dirname(model_path)         if not os.path.isdir(model_dir):             return f"directory not found: {model_dir}"         found = get_all_models(sort_by, "", model_dir)         return ", ".join(found.keys())     get_list_button.click(output_model_list, inputs=[module, model, model_dir, model_sort_by], outputs=[model_list])   return [(additional_networks_interface, "Additional Networks", "additional_networks")] def update_script_args(p, value, arg_idx):     for s in scripts.scripts_txt2img.alwayson_scripts:         if isinstance(s, Script):             args = list(p.script_args)             # print(f"Changed arg {arg_idx} from {args[s.args_from + arg_idx - 1]} to {value}")             args[s.args_from + arg_idx] = value             p.script_args = tuple(args)             break def confirm_models(p, xs):     for x in xs:         if x in ["", "None"]:             continue         if not find_closest_lora_model_name(x):             raise RuntimeError(f"Unknown LoRA model: {x}") def apply_module(p, x, xs, i):     update_script_args(p, True, 0)      # set Enabled to True     update_script_args(p, x, 1 + 3 * i) # enabled, ({module}, model, weight), ... def apply_model(p, x, xs, i):     name = find_closest_lora_model_name(x)     update_script_args(p, True, 0)     update_script_args(p, name, 2 + 3 * i) # enabled, (module, {model}, weight), ... def apply_weight(p, x, xs, i):     update_script_args(p, True, 0)     update_script_args(p, x, 3 + 3 * i) # enabled, (module, model, {weight), ... LORA_METADATA_NAMES = {     "ss_learning_rate": "Learning rate",     "ss_text_encoder_lr": "Text encoder LR",     "ss_unet_lr": "UNet LR",     "ss_num_train_images": "# of training images",     "ss_num_reg_images": "# of reg images",     "ss_num_batches_per_epoch": "Batches per epoch",     "ss_num_epochs": "Total epochs",     "ss_batch_size_per_device": "Batch size/device",     "ss_total_batch_size": "Total batch size",     "ss_gradient_accumulation_steps": "Gradient accum. steps",     "ss_max_train_steps": "Max train steps",     "ss_lr_warmup_steps": "LR warmup steps",     "ss_lr_scheduler": "LR scheduler",     "ss_network_module": "Network module",     "ss_network_dim": "Network dim",     "ss_mixed_precision": "Mixed precision",      "ss_full_fp16": "Full FP16",     "ss_v2": "V2",     "ss_resolution": "Resolution",     "ss_clip_skip": "Clip skip",     "ss_max_token_length": "Max token length",     "ss_color_aug": "Color aug",     "ss_flip_aug": "Flip aug",     "ss_random_crop": "Random crop",     "ss_shuffle_caption": "Shuffle caption",     "ss_cache_latents": "Cache latents",     "ss_enable_bucket": "Enable bucket",     "ss_min_bucket_reso": "Min bucket reso.",     "ss_max_bucket_reso": "Max bucket reso.",     "ss_seed": "Seed",      "ss_sd_model_name": "SD model name",     "ss_vae_name": "VAE name" } def format_lora_model(p, opt, x):     model = find_closest_lora_model_name(x)     if model is None or model.lower() in ["", "none"]:         return "None"     value = xy_grid.format_value(p, opt, model)     model_path = lora_models.get(model)     metadata = read_lora_metadata(model_path, "LoRA")     if not metadata:         return value     metadata_names = shared.opts.data.get("additional_networks_xy_grid_model_metadata", "").split(",")     if not metadata_names:         return value     for name in metadata_names:         name = name.strip()         if name in metadata:             formatted_name = LORA_METADATA_NAMES.get(name, name)             value += f"\n{formatted_name}: {metadata[name]}, "     return value.strip(" ").strip(",") for scriptDataTuple in scripts.scripts_data:     if os.path.basename(scriptDataTuple.path) == "xy_grid.py":         xy_grid = scriptDataTuple.module         for i in range(MAX_MODEL_COUNT):            model = xy_grid.AxisOption(f"AddNet Model {i+1}", str, lambda p, x, xs, i=i: apply_model(p, x, xs, i), format_lora_model, confirm_models)            weight = xy_grid.AxisOption(f"AddNet Weight {i+1}", float, lambda p, x, xs, i=i: apply_weight(p, x, xs, i), xy_grid.format_value_add_label, None)            xy_grid.axis_options.extend([model, weight]) def on_ui_settings():     section = ('additional_networks', "Additional Networks")     shared.opts.add_option("additional_networks_extra_lora_path", shared.OptionInfo("", "Extra path to scan for LoRA models (e.g. training output directory)", section=section))     shared.opts.add_option("additional_networks_sort_models_by", shared.OptionInfo("name", "Sort LoRA models by", gr.Radio, {"choices": ["name", "date", "path name"]}, section=section))     shared.opts.add_option("additional_networks_model_name_filter", shared.OptionInfo("", "LoRA model name filter", section=section))     shared.opts.add_option("additional_networks_xy_grid_model_metadata", shared.OptionInfo("", "Metadata to show in XY-Grid label for Model axes, comma-separated (example: \"ss_learning_rate, ss_num_epochs\")", section=section)) def on_infotext_pasted(infotext, params):     for i in range(MAX_MODEL_COUNT):         if f"AddNet Module {i+1}" not in params:             params[f"AddNet Module {i+1}"] = "LoRA"         if f"AddNet Model {i+1}" not in params:             params[f"AddNet Model {i+1}"] = "None"         if f"AddNet Weight {i+1}" not in params:             params[f"AddNet Weight {i+1}"] = "0" script_callbacks.on_ui_tabs(on_ui_tabs) script_callbacks.on_ui_settings(on_ui_settings) script_callbacks.on_infotext_pasted(on_infotext_pasted) ``` 
ISSUE,  software quality, [Code Quality + Security] capture-app-ionic. Validate the origin of the message event (iframe postMessage), this is to ensure that the event is sent from a trusted source. Based on ChatGPT [suggestion](https://github.com/numbersprotocol/capture-lite/pull/2498/commits/6c6d81d3d815718fa9daa71c9c56cc883be1b96b#diff-9a484019f1f930f58d733f8371bf7106f82293b2ca1b60660ea0b67edd3bd8a0R45). ChatGPT answer for "JS validate the origin of the postMessage event" `window.addEventListener('message', (event) => {` ` // Check if the origin of the event matches the expected origin ` ` if (event.origin !== 'https://example.com') { ` ` console.log('Invalid event origin: ' + event.origin); ` ` return; ` ` } ` ` // Handle the event data ` ` console.log(event.data); ` `});` Also good [read](https://developer.mozilla.org/en-US/docs/Web/API/Window/postMessage#security_concerns) for postMessage security concerns. P.S: as an extra layer of security we can encode/decode sent message ‚îÜIssue is synchronized with this [Asana task](https://app.asana.com/0/1201016280880500/1203794276110740) by [Unito](https://www.unito.io) ‚îÜCreated By: Sam 
ISSUE,  software quality, [Code Quality + Security] capture-app-ionic. Unify iframe postMessage format and extract to ENUMs Based on ChatGPT [suggestion](https://github.com/numbersprotocol/capture-lite/pull/2498/commits/45109709260b491fed315db67b6f402c5ef08824#diff-e694909fc637e26a97335ce8d4483490e0d352995d3158043405178df31a5c20R45) some places use [kebab-case](https://github.com/numbersprotocol/capture-lite/pull/2498/commits/45109709260b491fed315db67b6f402c5ef08824#diff-e694909fc637e26a97335ce8d4483490e0d352995d3158043405178df31a5c20R34) some places use [camleCase](https://github.com/numbersprotocol/capture-lite/pull/2498/commits/45109709260b491fed315db67b6f402c5ef08824#diff-e694909fc637e26a97335ce8d4483490e0d352995d3158043405178df31a5c20R37). For capture-app-ionic * find all postMessage.data and make sure all are kebab-case * find all postMessage.data and use ENUMs instead of plain string ‚îÜIssue is synchronized with this [Asana task](https://app.asana.com/0/1201016280880500/1203794276110745) by [Unito](https://www.unito.io) ‚îÜCreated By: Sam 
ISSUE,  learning, How to share a google colab version that accepts a folder of images for create videos? Hi! I changed a bit the Google Colab version: [https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/InstructPix2Pix_using_diffusers.ipynb](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/InstructPix2Pix_using_diffusers.ipynb) I made it so it accepts a folder of images (frames) to after create a video. I would like to share it publicly so others can use it but without changing the main notebook. Also that when they load again the notebook it doesn't loose their changes. And also that it doesn't popup when running the cells telling it isn't secure and my showing my email as The Owner. Just like the link I provided. Any suggestions? Have searched everywhere and the google colab support doesn't work. Also asked ChatGPT and doesn't give me a proper answer. Any suggestion will be much appreciated!
ISSUE,  development environment, Is it possible to save files uploaded by users on the phishing page (landing page)? Is it possible to save files uploaded by users on the phishing page (landing page)? I asked ChatGPT and he found that the configuration that can do this uses the following configuration: ```json {   "admin_server": {     "listen_url": "0.0.0.0:3333",     "use_tls": false,     "cert_path": "",     "key_path": ""   },   "phish_server": {     "listen_url": "0.0.0.0:443",     "use_tls": true,     "cert_path": "",     "key_path": "",     "redirect_url": "",     "redirect_path": ""   },   "db_name": "sqlite3",   "db_path": "gophish.db",   "migrations_path": "./db/migrations",   "uploaded_file_path": "/var/gophish/uploaded_files" # <---- Like this } ``` But it doesn't work. I can't find it in the Gophish documentation either. Is it possible to save user uploaded files?
ISSUE,  documentation, Create documentation with ChatGPT It just very quickly generated: # Challenges Directory Structure Here is an example of how the file system for the "babyshell" category might be laid out: ``` /opt/dojo/data/challenges/     babyshell/         challenges.yml         _global/             # files inserted for all students at the category level         level1/             _global/                 # files inserted for all students at the challenge level             0/                 # files specific to this instance of the challenge             1/                 # files specific to this instance of the challenge             2/                 # files specific to this instance of the challenge         level2/             _global/                 # files inserted for all students at the challenge level             0/                 # files specific to this instance of the challenge             1/                 # files specific to this instance of the challenge             2/                 # files specific to this instance of the challenge ``` Here is an example of what the files within an instance directory might look like for the "babyshell" category: ``` /opt/dojo/data/challenges/babyshell/level1/0/     babyshell_level1     babyshell_level1.c ``` Another example, in this case the "babyweb" category: ``` /opt/dojo/data/challenges/     babyweb/         challenges.yml         _global/             # files inserted for all students at the category level         level1/             _global/                 # files inserted for all students at the challenge level             0/                 # files specific to this instance of the challenge             1/                 # files specific to this instance of the challenge             2/                 # files specific to this instance of the challenge         level2/             _global/                 # files inserted for all students at the challenge level             0/                 # files specific to this instance of the challenge             1/                 # files specific to this instance of the challenge             2/                 # files specific to this instance of the challenge ``` You can see that in both examples the structure is the same, the difference is in the name of the category, and the files that are in the different instances. 
ISSUE,  software quality, [Code Quality + Security] capture-app-iframe. Unify iframe postMessage format Based on ChatGPT [suggestion](https://github.com/numbersprotocol/capture-lite/pull/2498/commits/45109709260b491fed315db67b6f402c5ef08824#diff-e694909fc637e26a97335ce8d4483490e0d352995d3158043405178df31a5c20R45) some places use [kebab-case](https://github.com/numbersprotocol/capture-lite/pull/2498/commits/45109709260b491fed315db67b6f402c5ef08824#diff-e694909fc637e26a97335ce8d4483490e0d352995d3158043405178df31a5c20R34) some places use [camleCase](https://github.com/numbersprotocol/capture-lite/pull/2498/commits/45109709260b491fed315db67b6f402c5ef08824#diff-e694909fc637e26a97335ce8d4483490e0d352995d3158043405178df31a5c20R37). For capture-app-iframe * find all postMessage.data and make sure all are kebab-case QA verified method: iframe back button ( Top left corner ) QA summarised the testing cases about it. [https://miro.com/app/board/o9J_knEiIYo=/?moveToWidget=3458764546039920899&cot=14](https://miro.com/app/board/o9J_knEiIYo=/?moveToWidget=3458764546039920899&amp;cot=14) Testing case A : fixed on android, iOS not yet. ‚Üí Also happened on live version fixed in the future. Testing case B:  fixed Testing case C:  1. If v230209-iframe deploy and v230209-ionic haven‚Äôt been public -> the back button can‚Äôt work in data & terms. 1. Before iframe-v230209 deploy, the back button can work normally in data & terms. 1. v230131-iframe & ionic should not influence the back button. Testing case D: known issue [‚úì [issue] back button will back to wrong page (should in the explore tab not my capture tab)](https://app.asana.com/0/1201016280880500/1203600983882519/f) At least fixed **B & C**, A & D could be fixed in the future sprint. ‚îÜIssue is synchronized with this [Asana task](https://app.asana.com/0/1201016280880500/1203794276110749) by [Unito](https://www.unito.io) ‚îÜCreated By: Sam 
ISSUE,  learning, Needing help using Mega.nz SDK for scripting Im not really familiar with c++ , so i have been using chatgpt to help me make a script that will get a temporary link of a folder that last 1 hour, so this is what i got: #include "C:\Users\Uros\Downloads\sdk-master\sdk-master\include\megaapi.h" #include <iostream> int main(int argc, char *argv[]) {     // Initialize the MegaApi object     mega::MegaApi* megaApi = new mega::MegaApi(NULL, "APIKEY");     // Login to your account     megaApi->login("MAIL", "PASS");     megaApi->fetchNodes();     // Get a handle for the folder you want to share     mega::MegaNode *folderNode = megaApi->getNodeByPath("FOLDERPATH");     if(folderNode==NULL)     {         std::cout<<"Folder not found"<<std::endl;         return 1;     }     // Get a public link for the folder     megaApi->getPublicLinks(folderNode);     mega::MegaStringMap *links = megaApi->getPublicLinks(folderNode);     std::string exportLink = links->get("0");     // Print the link     std::cout << "Public Link for the folder: " << exportLink << std::endl;     // Clean up     delete megaApi;     return 0; } I have been getting this error on line 22 and 23 :  invalid conversion from 'mega::MegaNode*' to 'int' [-fpermissive]| If anyone can help me , or just give me anyother way to automate getting temporary links from Mega , would be awesome
ISSUE,  learning, Test Workflow Does Not Test Against Expectations - Use Mocks Instead of GitHub App ## Problem Respectfully, [pull request 64](https://github.com/tibdex/github-app-token/pull/64) from [pull request 55](https://github.com/tibdex/github-app-token/pull/55) and [issue 54](https://github.com/tibdex/github-app-token/issues/54) just broke **_all_** 78 forks of your project and increased the barrier to entry for contributors to submit pull requests. For example, [here](https://github.com/AntelopeIO/github-app-token-action/commit/56041fab34a6df29f94f4c808b866280c0478205) I pushed a commit to my fork of your project containing my proof that P = NP, and [here](https://github.com/AntelopeIO/github-app-token-action/actions/runs/4022387349/jobs/6912087704#step:8:6) you can see your workflow fails in my fork because I have not installed a GitHub App that satisfies [the new test](https://github.com/AntelopeIO/github-app-token-action/blob/main/.github/workflows/test.yml#L23). This can also happen on base branches, release branches, and tags in forks and people may incorrectly believe your action is broken when it is not. While it may sound simple to you and I to install an app that satisfies your test, that process is undocumented and it requires admin approval in organizations that could be prohibitive for developers to obtain. > A test that relies on a GitHub repo to be configured in a specific way such that it fails on forks is not a robust or reliable test. There are several reasons why a test should not rely on external factors like this: > > 1. Non-deterministic results: Tests that rely on external factors like the configuration of a GitHub repo may produce inconsistent or non-deterministic results, making it difficult to know if the system is working correctly or not. > 1. Environment-specific: Tests that rely on external factors may only work in specific environments and may not be portable across different development, staging, or production environments. > 1. Maintenance: Tests that rely on external factors may require additional maintenance to ensure that they continue to work correctly as the external factors change over time. > 1. Lack of isolation: Tests that rely on external factors are not properly isolated from the environment and other dependencies, and therefore they may be affected by changes that are unrelated to the code being tested. > 1. Lack of scalability: Tests that rely on external factors may not be able to scale as the system or the number of users grows, leading to performance issues or time-consuming tests. > > It is important to design tests that are robust, reliable, and isolated from external factors to ensure that they can produce consistent and accurate results and can be easily maintained over time. Consider the purpose of your test and what it is fundamentally testing. Your existing test does not only test code correctness, it also tests for the existence of a GitHub App. We should write a test that tests for code correctness and nothing else. ## Solution There are a lot of valid ways to solve this problem. I, personally, really **_love_** the [jest](https://jestjs.io) test framework. It allows you to mock libraries including `octokit` very easily. Here is an example written by ChatGPT using `jest` to mock `octokit` and implementing a test against your `fetchInstallationToken` function in [fetch-installation-token.ts](https://github.com/tibdex/github-app-token/blob/44d4a52dd8146ce3a0a0a41e84589c102cbd347a/src/fetch-installation-token.ts). ```ts import { fetchInstallationToken } from "./fetchInstallationToken"; import { getOctokit } from "@actions/github"; import { request } from "@octokit/request"; // mocks jest.mock("@octokit/rest", () => {   return {     apps: {       getRepoInstallation: jest.fn().mockResolvedValue({         data: { id: 123 }       }),       createInstallationAccessToken: jest.fn().mockResolvedValue({         data: { token: 'installation-token' }       })     }   } }); jest.mock("@actions/github", () => {   return {     getOctokit: jest.fn()   } }); jest.mock("@octokit/request", () => {   return {     defaults: jest.fn()   } }); // clean up after each test afterEach(() => {   jest.clearAllMocks(); }); // test suite describe("fetchInstallationToken", () => {   test("should fetch the installation token", async () => {     const result = await fetchInstallationToken({       appId: "123",       githubApiUrl: new URL("https://api.github.com"),       owner: "owner",       repo: "repo",       privateKey: "private-key"     });     expect(result).toBe("installation-token");     expect(getOctokit).toHaveBeenCalledWith("installation-token");     expect(request.defaults).toHaveBeenCalledWith({       baseUrl: "https://api.github.com"     });   }); }); ``` *** Instead of `jest` mocks, you could also use [nock](https://github.com/nock/nock) to intercept the REST API calls `octokit` makes to the Internet. Here is an example of that from ChatGPT. ```ts import nock from "nock"; import { fetchInstallationToken } from "./fetchInstallationToken"; describe("fetchInstallationToken", () => {   afterEach(() => {     jest.clearAllMocks();     nock.cleanAll();   });   test("should return installation token", async () => {     const appId = "123";     const githubApiUrl = new URL("https://api.github.com");     const installationId = 1234;     const owner = "octocat";     const repo = "hello-world";     const privateKey = "-----BEGIN RSA PRIVATE KEY-----\n...";     const installationAccessToken = "abcdefghijklmnopqrstuvwxyz";     nock(githubApiUrl.toString())       .post("/app/installations/1234/access_tokens", {})       .reply(200, {         token: installationAccessToken,       });     const token = await fetchInstallationToken({       appId,       githubApiUrl,       installationId,       owner,       privateKey,       repo,     });     expect(token).toEqual(installationAccessToken);   }); }); ``` I strongly recommend against this solution because the purpose of the test is to test this GitHub Action for correctness, not to test `octokit` for correctness. The `jest` mock example solely tests this GitHub Action for correctness, whereas the `nock` example tests both the GitHub Action and `octokit` for correctness. If GitHub changed the implementation of their REST API and `octokit` changed in kind, this test would fail on passing code. I included an example with `nock` because it does remove the GitHub repo configuration as a variable and it is a tool worth knowing about. *** I also like to add this to my `package.json` when working with `jest`. ```json {   "scripts": {     "test": "jest --coverage"   },   "jest": {     "clearMocks": true,     "collectCoverage": true,     "collectCoverageFrom": [       "**/*.js",       "**/*.ts"     ],     "coveragePathIgnorePatterns": [       "coverage",       "dist",       "node_modules"     ],     "coverageThreshold": {       "global": {         "branches": 100,         "functions": 100,         "lines": 100,         "statements": 100       }     },     "testPathIgnorePatterns": [       "coverage",       "dist",       "node_modules"     ]   } } ``` This allows you to run `jest` using `yarn test`, and requires full test coverage. Your GitHub Action test workflow would become: ```yaml name: Test on:   push:     branches-ignore:       - main jobs:   test:     name: Test     runs-on: ubuntu-latest     steps:       - uses: actions/checkout@v3       - run: yarn install --frozen-lockfile       - run: yarn run yarn-deduplicate --fail --strategy fewer       - run: yarn run build       - run: yarn run prettier --check       - run: yarn run xo       - run: yarn test ``` *** **_Legal notice_:**   This document was generated in collaboration with the 2023-01-09 version of ChatGPT from OpenAI, a machine learning algorithm or weak artificial intelligence (AI). At the time of this writing, the [OpenAI terms of service agreement](https://openai.com/terms) ¬ß3.a states: > Your Content. You may provide input to the Services (‚ÄúInput‚Äù), and receive output generated and returned by the Services based on the Input (‚ÄúOutput‚Äù). Input and Output are collectively ‚ÄúContent.‚Äù As between the parties and to the extent permitted by applicable law, you own all Input, and subject to your compliance with these Terms, OpenAI hereby assigns to you all its right, title and interest in and to Output. I release this content under the MIT license. This notice is required in some countries. 
ISSUE,  software quality, some issue related to "localNetworkNames" in the main.xxx.js file Hi  While trying to run on Docker, i am getting an error in the browser console: --------------------- ypeError: Cannot read properties of undefined (reading 'filter')     at main.cc1ec017.js:2:255383     at e (main.cc1ec017.js:2:154208)     at main.cc1ec017.js:2:154413     at Object.useSyncExternalStore (main.cc1ec017.js:2:78270)     at t.useSyncExternalStore (main.cc1ec017.js:2:145742)     at t.useSyncExternalStoreWithSelector (main.cc1ec017.js:2:154483)     at main.cc1ec017.js:2:156142     at Fo (main.cc1ec017.js:2:255357)     at ki (main.cc1ec017.js:2:71227)     at xs (main.cc1ec017.js:2:128467) dl @ main.cc1ec017.js:2 n.callback @ main.cc1ec017.js:2 Fa @ main.cc1ec017.js:2 ws @ main.cc1ec017.js:2 bs @ main.cc1ec017.js:2 ys @ main.cc1ec017.js:2 (anonymous) @ main.cc1ec017.js:2 Su @ main.cc1ec017.js:2 ou @ main.cc1ec017.js:2 E @ main.cc1ec017.js:2 R @ main.cc1ec017.js:2 main.cc1ec017.js:2 Uncaught TypeError: Cannot read properties of undefined (reading 'filter')     at main.cc1ec017.js:2:255383     at e (main.cc1ec017.js:2:154208)     at main.cc1ec017.js:2:154413     at Object.useSyncExternalStore (main.cc1ec017.js:2:78270)     at t.useSyncExternalStore (main.cc1ec017.js:2:145742)     at t.useSyncExternalStoreWithSelector (main.cc1ec017.js:2:154483)     at main.cc1ec017.js:2:156142     at Fo (main.cc1ec017.js:2:255357)     at ki (main.cc1ec017.js:2:71227)     at xs (main.cc1ec017.js:2:128467) ------------------- Screen is Black an nothings appeares. while trying to point to the error in the main.js, it jumps to this section: -------------- e.localNetworkNames.filter((t=>t!==e.networkName))));return 0===n.length?null:(0,We.jsxs) ------------------- i am not sure, but if i am searchinf for "NetworkNames" in the code, it is point me to out to: filedrop-web\src\sections\OtherNetworks\index.tsx ChatGPT [  ;)  ] says it is related to the setion: -----  const localNetworkNames = useSelector((store: StateType) =>     store.localNetworkNames.filter(name => name !== store.networkName)   );   if (localNetworkNames.length === 0) {     return null;   } -------------- triyng to use chatGPT advise replacing this with another code, ------------------ const localNetworkNames = useSelector((store: StateType) => {   if (!store.localNetworkNames) return [];   return store.localNetworkNames.filter(name => name !== store.networkName); }); -----------------  "resolve" the issue but created another one, the website loded with no "errors" in the console, but nothing worked acctually. Kindly please advise. best regards. 
ISSUE,  software quality, Mempool and Thunderhub can not be installed My issue is that every installation try of a Mempool and Tunderhub has failed on **V1.8 patched**  The 32GB SD card is new, fast, and durable. I'm running Blitz over wifi on Raspberry 4b 8Gb, 2TB SSD As I said. installing Mempool or Thunderhub always failed. ![Screenshot (199)](https://user-images.githubusercontent.com/34000272/215330382-09416be3-1cde-4a0c-901c-4d36972f9382.png) ******************************************************** ![Screenshot (201)](https://user-images.githubusercontent.com/34000272/215330493-9e7100cc-3548-473f-a51d-a51670f24ebe.png) Here are the logs... `sudo cat /var/cache/raspiblitz/debug.log | nc termbin.com 9999` raspiblitz debug logs: https://termbin.com/3oefe  Mempool installation log: https://termbin.com/zkux Thunderhub installation log: https://termbin.com/ricl7 (Line 3282 gives an error about connection but I am sure the connection is just fine and I'm not behind any proxy) Then I upgraded the Raspiblitz to **v1.9.0rc1** as Openoms suggested Raspiblitz debug logs: https://termbin.com/tlqu I could install Thunderhub (The status on webUI says installation successful and even got the links to the service) but couldn't run it (I reinstalled it two times and could not get it to run. Log was always the same "This site can‚Äôt be reached") thunderbird log: https://termbin.com/atio ![Screenshot (206)](https://user-images.githubusercontent.com/34000272/215343648-9814ed60-7335-4b47-822f-b37e92a12a96.png) When trying to install Mempool, it said it is already installed but it wasn't (No logs, just a picture) ![Screenshot (202)](https://user-images.githubusercontent.com/34000272/215330628-acbf5ee2-a201-42c0-be4d-a2593db8ffc0.png) Openoms also suggested updating node version with `config.scripts/bonus.nodejs.sh on` Old node version: v16.14.2 node version after update: 18.12.0 I also did `config.scripts/bonus.nodejs.sh off` and "on" again There were no changes doing that. I still can't install mempool and thundehub . For example what Thunderhug log is complaining about after all that... *** LAST 20 Thunderhub LOGS *** sudo journalctl -u thunderhub -b --no-pager -n20 -- Journal begins at Sun 2023-01-29 04:34:17 GMT, ends at Sun 2023-01-29 16:03:2        6 GMT. -- Jan 29 16:02:45 raspberrypi systemd[1]: thunderhub.service: Scheduled restart jo        b, restart counter is at 4. Jan 29 16:02:45 raspberrypi systemd[1]: Stopped ThunderHub daemon. Jan 29 16:02:45 raspberrypi systemd[1]: thunderhub.service: Consumed 1.603s CPU         time. Jan 29 16:02:45 raspberrypi systemd[1]: Started ThunderHub daemon. Jan 29 16:02:46 raspberrypi npm[679854]: > thunderhub@0.13.6 start Jan 29 16:02:46 raspberrypi npm[679854]: > cross-env NODE_ENV=production nest start Jan 29 16:02:46 raspberrypi npm[679978]: sh: 1: cross-env: not found Jan 29 16:02:46 raspberrypi systemd[1]: thunderhub.service: Main process exited,code=exited, status=127/n/a Jan 29 16:02:46 raspberrypi systemd[1]: thunderhub.service: Failed with result 'exit-code'. Jan 29 16:02:46 raspberrypi systemd[1]: thunderhub.service: Consumed 1.690s CPUtime. Jan 29 16:03:17 raspberrypi systemd[1]: thunderhub.service: Scheduled restart job, restart counter is at 5. Jan 29 16:03:17 raspberrypi systemd[1]: Stopped ThunderHub daemon. Jan 29 16:03:17 raspberrypi systemd[1]: thunderhub.service: Consumed 1.690s CPU time. Jan 29 16:03:17 raspberrypi systemd[1]: Started ThunderHub daemon. Jan 29 16:03:18 raspberrypi npm[684376]: > thunderhub@0.13.6 start Jan 29 16:03:18 raspberrypi npm[684376]: > cross-env NODE_ENV=production nest start Jan 29 16:03:19 raspberrypi npm[684728]: sh: 1: cross-env: not found Jan 29 16:03:19 raspberrypi systemd[1]: thunderhub.service: Main process exited,code=exited, status=127/n/a Jan 29 16:03:19 raspberrypi systemd[1]: thunderhub.service: Failed with result 'exit-code'. Jan 29 16:03:19 raspberrypi systemd[1]: thunderhub.service: Consumed 1.896s CPU time. Next, I will start fresh and update node to v18.12.9 before installing other apps. _To be continued..._ **Starting fresh on v1.8...** The first app that I tried to install is Mempool and got the same error as before.  ("Try to update to npm@9.4.0") So I did that and came across a blockade about some permissions and here's the log of a mempool install: https://termbin.com/zw37 ![Screenshot (222)](https://user-images.githubusercontent.com/34000272/215351425-9bab1219-c608-420a-9fb0-3db2e79113b4.png) Updating npm with `npm install -g npm@9.4.0`  give this results:  https://termbin.com/u9v6n ![Screenshot (224)](https://user-images.githubusercontent.com/34000272/215351588-903f4197-0cb3-4732-8c2b-7e8c7656a213.png) I've tried to update npm with `sudo npm install -g npm@9.4.0` and the new version is installed but that still didn't solve the problem with these two apps unfortunately.  The Network error has gone after an npm update and there's another one https://termbin.com/vqq8 ![Screenshot (225)](https://user-images.githubusercontent.com/34000272/215355590-6a147caa-e24a-4181-b93d-2d3c65a59c24.png) ********************************** **MEMPOOL has been installed** I asked chatGPT what this error could be, and the AI gave me an important clue... ![Screenshot (226)](https://user-images.githubusercontent.com/34000272/215565568-cc987a9b-9105-4b78-a8af-0a8ca655005a.png)  Deleting  `sudo rm -rf /home/mempool/mempool/frontend/node_modules`  folder lets through a clean installation of  Mempool.  The mempool operation is completed :) Next is Thunderhub... 
ISSUE,  feature implementation/enhancement, Rewrite in Rust I think Rust would be much more maintainable than C++. I asked ChatGPT to convert the existing C++ to Rust. That may be a good starting point: ```rust extern crate com; use com::{com_interface, ComRc, ComItf}; use std::ffi::OsString; use std::io::{self, Write}; use winapi::{LPWSTR, MAX_PATH, WPSTYLE_SPAN, WPSTYLE_CENTER, WPSTYLE_TILE, WPSTYLE_STRETCH, WPSTYLE_KEEPASPECT}; #[com_interface("6D5140C1-7436-11CE-8034-00AA006009FA")] pub trait IActiveDesktop: ComItf { fn GetWallpaper(&self, wp: LPWSTR, len: i32, flags: i32) -> HRESULT; fn SetWallpaper(&self, wp: LPWSTR, flags: i32) -> HRESULT; fn SetWallpaperOptions(&self, opts: &WALLPAPEROPT, flags: i32) -> HRESULT; fn ApplyChanges(&self, flags: i32) -> HRESULT; } #[repr(C)] #[derive(Debug, Copy, Clone)] struct WALLPAPEROPT { dwSize: u32, dwStyle: i32, } fn get_wallpaper(sz_wallpaper: &mut [u16]) -> io::Result<()> { unsafe { let result = CoInitialize(std::ptr::null_mut()); if result != S_OK { return Err(io::Error::new( io::ErrorKind::Other, "Failed to initialize COM", )); } let mut p_desk: *mut IActiveDesktop = std::ptr::null_mut(); let hr = CoCreateInstance( &CLSID_ActiveDesktop, std::ptr::null_mut(), CLSCTX_INPROC_SERVER, &IID_IActiveDesktop, &mut p_desk as *mut *mut IActiveDesktop as *mut *mut std::ffi::c_void, ); if hr != S_OK { return Err(io::Error::new( io::ErrorKind::Other, "Failed to get the desktop wallpaper", )); } (*p_desk).GetWallpaper(sz_wallpaper.as_mut_ptr(), MAX_PATH, 0); (*p_desk).Release(); CoFreeUnusedLibraries(); CoUninitialize(); } Ok(()) } fn set_wallpaper(full_path: &[u16], wp_style: i32) -> io::Result<()> { unsafe { let result = CoInitialize(std::ptr::null_mut()); if result != S_OK { return Err(io::Error::new( io::ErrorKind::Other, "Failed to initialize COM", )); } let mut p_desk: *mut IActiveDesktop = std::ptr::null_mut(); let hr = CoCreateInstance( &CLSID_ActiveDesktop, std::ptr::null_mut(), ``` *(It's a little bit truncated)*
ISSUE,  software quality, Error 429 Please translate it to English. Thank you for your great work in advance! Once I send a request with !gpt, I receive the response  "An error occurred, please contact the administrator. (ChatGPTAPI error 429)".  I immediately asked ChatGPT about the error message and received the response that too many requests were made. Well, the openai account is new and only four chats have been started with it. Any ideas? `[Whatsapp ChatGPT] Client is ready! [Whatsapp ChatGPT] Received prompt from 492212123456799@c.us: Wie viele Zeitzonen gibt es? An error occured ChatGPTError: ChatGPTAPI error 429     at ChatGPTAPIBrowser.sendMessage (file:///C:/Users/adm/whatsapp-chatgpt/node_modules/chatgpt/src/chatgpt-api-browser.ts:563:23)     at processTicksAndRejections (node:internal/process/task_queues:95:5)     at handleMessage (C:\Users\adm\whatsapp-chatgpt\src\index.ts:48:22)     at Client.<anonymous> (C:\Users\adm\whatsapp-chatgpt\src\index.ts:36:9) {   statusCode: 429,   statusText: '' }`
ISSUE,  feature implementation/enhancement, Add github image support Currently no attributes are supported (width, height), it would be nice to support these. An example how this can be shown: ```python import re import jira2markdown from jira2markdown.elements import MarkupElements from jira2markdown.markup.base import AbstractMarkup from jira2markdown.markup.images import Image from pyparsing import (     Combine,     Optional,     ParserElement,     ParseResults,     PrecededBy,     Regex,     StringStart,     Word,     printables, ) class CustomImage(AbstractMarkup):     def action(self, tokens: ParseResults) -> str:         url = tokens.url         attr_str = self._create_attribute_dict(tokens.attrs)         return f'<img src="{url}" {attr_str} />'     @property     def expr(self) -> ParserElement:         return (StringStart() | PrecededBy(Regex(r"\W", flags=re.UNICODE), retreat=1)) + Combine(             "!"             + Word(printables + " ", min=3, exclude_chars="|!").set_results_name("url")             + Optional("|")             + Word(printables + ",", exclude_chars="!").set_results_name("attrs")             + Optional("!")         ).set_parse_action(self.action)     @staticmethod     def _create_attribute_dict(attrs_str: str) -> str:         attrs = {}         for attr in attrs_str.split(','):             key, value = attr.split('=')             attrs[key] = value         attr_str = " ".join([f'{k}="{v}"' for k, v in attrs.items()])         return attr_str elements = MarkupElements() elements.replace(Image, CustomImage) markdown_text = jira2markdown.convert("!image.png|width=200,height=400!", elements=elements) print(markdown_text) ``` This will print ``` <img src="image.png" width="200" height="400" /> ``` P.S. ChatGPT to the rescue for helping to find the correct parsing
ISSUE,  process, [Ray 2.3 Release] Common failure in a few release tests `failed to connect to all addresses; last error: UNKNOWN` The following stacktrace appears consistently in the following tests: * air_benchmark_xgboost_cpu_10 * lightgbm_distributed_api_test * xgboost_distributed_api_test BuildKite: https://buildkite.com/ray-project/release-tests-branch/builds/1351#018618c9-8c0d-4102-9420-c5cd8eb29b3d ``` Traceback (most recent call last): -- ¬† | File "ray_release/scripts/run_release_test.py", line 168, in <module> ¬† | main() ¬† | File "/home/ray/anaconda3/lib/python3.7/site-packages/click/core.py", line 1130, in __call__ ¬† | return self.main(*args, **kwargs) ¬† | File "/home/ray/anaconda3/lib/python3.7/site-packages/click/core.py", line 1055, in main ¬† | rv = self.invoke(ctx) ¬† | File "/home/ray/anaconda3/lib/python3.7/site-packages/click/core.py", line 1404, in invoke ¬† | return ctx.invoke(self.callback, **ctx.params) ¬† | File "/home/ray/anaconda3/lib/python3.7/site-packages/click/core.py", line 760, in invoke ¬† | return __callback(*args, **kwargs) ¬† | File "ray_release/scripts/run_release_test.py", line 153, in main ¬† | no_terminate=no_terminate, ¬† | File "/tmp/release-B1APUcfhbQ/release/ray_release/glue.py", line 404, in run_release_test ¬† | raise pipeline_exception ¬† | File "/tmp/release-B1APUcfhbQ/release/ray_release/glue.py", line 311, in run_release_test ¬† | command, env=command_env, timeout=command_timeout ¬† | File "/tmp/release-B1APUcfhbQ/release/ray_release/command_runner/job_runner.py", line 115, in run_command ¬† | full_command, full_env, working_dir=".", timeout=int(timeout) ¬† | File "/tmp/release-B1APUcfhbQ/release/ray_release/job_manager.py", line 113, in run_and_wait ¬† | return self._wait_job(cid, timeout) ¬† | File "/tmp/release-B1APUcfhbQ/release/ray_release/job_manager.py", line 92, in _wait_job ¬† | status = self._get_job_status_with_retry(command_id) ¬† | File "/tmp/release-B1APUcfhbQ/release/ray_release/job_manager.py", line 69, in _get_job_status_with_retry ¬† | max_retries=3, ¬† | File "/tmp/release-B1APUcfhbQ/release/ray_release/util.py", line 119, in exponential_backoff_retry ¬† | return f() ¬† | File "/tmp/release-B1APUcfhbQ/release/ray_release/job_manager.py", line 66, in <lambda> ¬† | lambda: job_client.get_job_status(self.job_id_pool[command_id]), ¬† | File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/sdk.py", line 396, in get_job_status ¬† | return self.get_job_info(job_id).status ¬† | File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/sdk.py", line 331, in get_job_info ¬† | self._raise_error(r) ¬† | File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/dashboard_sdk.py", line 262, in _raise_error ¬† | f"Request failed with status code {r.status_code}: {r.text}." ¬† | RuntimeError: Request failed with status code 500: {"result": false, "msg": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/optional_utils.py\", line 95, in _handler_route\n    return await handler(bind_info.instance, req)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/job_head.py\", line 391, in get_job_info\n    job_or_submission_id,\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/utils.py\", line 209, in find_job_by_ids\n    driver_jobs, submission_job_drivers = await get_driver_jobs(gcs_aio_client)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/utils.py\", line 154, in get_driver_jobs\n    reply = await gcs_aio_client.get_all_job_info()\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/gcs_utils.py\", line 164, in wrapper\n    return await f(self, *args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/gcs_utils.py\", line 556, in get_all_job_info\n    reply = await self._job_info_stub.GetAllJobInfo(req, timeout=timeout)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/grpc/aio/_call.py\", line 291, in __await__\n    self._cython_call._status)\ngrpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:172.31.254.44:9031: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:172.31.254.44:9031: Failed to connect to remote host: Connection refused {created_time:\"2023-02-03T12:04:49.142935439-08:00\", grpc_status:14}\"\n>\n", "data": {}}. ``` EDIT(archit): here's the last line pretty-printed by chatgpt for readability ``` {     "result": false,     "msg": "Traceback (most recent call last):   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/optional_utils.py\", line 95, in _handler_route     return await handler(bind_info.instance, req)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/job_head.py\", line 391, in get_job_info     job_or_submission_id,   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/utils.py\", line 209, in find_job_by_ids     driver_jobs, submission_job_drivers = await get_driver_jobs(gcs_aio_client)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/utils.py\", line 154, in get_driver_jobs     reply = await gcs_aio_client.get_all_job_info()   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/gcs_utils.py\", line 164, in wrapper     return await f(self, *args, **kwargs)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/gcs_utils.py\", line 556, in get_all_job_info     reply = await self._job_info_stub.GetAllJobInfo(req, timeout=timeout)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/grpc/aio/_call.py\", line 291, in __await__     self._cython_call._status) grpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:     status = StatusCode.UNAVAILABLE     details = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:172.31.254.44:9031: Failed to connect to remote host: Connection refused\"     debug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:172.31.254.44:9031: Failed to connect to remote host: Connection refused {created_time:\"2023-02-03T12:04:49.142935439-08:00\", grpc_status:14}\" > ",     "data": {} } ```
ISSUE,  learning, Question: Why dead-letter-exchange configuration is shared in RabbitCommonProperties? Is there any difference between declaring the dlx configuration in the producer vs in the consumer side? considering that retires are NOT needed I asked ChatGPT and always state that dlx configuration should be declared on both sides (producer and consumer) as a best practice and to avoid message losses.
ISSUE,  learning, Mod auth mellon is NOT checking the SAML Response signature I am using the mod auth mellon on apache for authentication on my SP. I have added on my SP metadata the directives (AuthnRequestsSigned="true" WantAssertionsSigned="true")to require signed responses. So my sp.xml has this line <SPSSODescriptor    AuthnRequestsSigned="true"    WantAssertionsSigned="true"    protocolSupportEnumeration="urn:oasis:names:tc:SAML:2.0:protocol" > I run a burpSuite test when receiving the SAML response from the IdP and I see that removing the signature Mellon still accepts the response instead of rejecting (as it should, because I WantAssertionsSigned, right?), so I am still able to login. The documentation for Mellon is somewhat outdated and other than the readme.doc there are not a lot of resources. **Does anyone know how I can force Mellon to check the SAML Response signature? ** I made sure that all the SAML Responses had a signature value in their payload. I made sure that the signature value matched the public key value of the IDP. FYI: Asking ChatGPT gives answers about directives that do not exist in the code. Neither on this one nor on previous versions ![image](https://user-images.githubusercontent.com/8544353/217510896-61102ff3-8e45-46b5-b614-fe5ee39c829f.png) 
ISSUE,  development environment, Make ReadWriteMany work without nfs. Hi! I recently built my first k3s cluster with my one pi4 and wanted to install drupal ( because its awesome :) ) so with the help of chatGPT and your [drupal.yml](https://github.com/geerlingguy/pi-cluster/blob/master/templates/drupal.yml) I created this manifest file ( Note that the volume that I want must be mounted in /data/drupal-files/ on the raspberry pi and in /var/www/html on the container. ): ``` apiVersion: apps/v1 kind: Deployment metadata:   name: drupal-deployment spec:   replicas: 1   selector:     matchLabels:       app: drupal   template:     metadata:       labels:         app: drupal     spec:       containers:         - name: drupal           image: drupal:latest           ports:             - name: http               containerPort: 80           volumeMounts:             - name: drupal-files               mountPath: /var/www/html       volumes:         - name: drupal-files           persistentVolumeClaim:             claimName: drupal-pvc --- apiVersion: v1 kind: PersistentVolumeClaim metadata:   name: drupal-pvc spec:   accessModes:     - ReadWriteMany   resources:     requests:       storage: 1Gi --- apiVersion: v1 kind: Service metadata:   name: drupal-service spec:   ports:     - name: http       port: 7000       targetPort: 80   selector:     app: drupal   type: LoadBalancer ``` But while the pods and the service work, the persistent volume claim does not want to start, it's stuck on pending. When I run ``` sudo kubectl describe pvc``` it gives me this output: ``` Name:          drupal-pvc Namespace:     default StorageClass:  local-path Status:        Pending Volume: Labels:        <none> Annotations:   volume.beta.kubernetes.io/storage-provisioner: rancher.io/local-path                volume.kubernetes.io/selected-node: raspberrypi4                volume.kubernetes.io/storage-provisioner: rancher.io/local-path Finalizers:    [kubernetes.io/pvc-protection] Capacity: Access Modes: VolumeMode:    Filesystem Used By:       drupal-deployment-8b48bb9cb-2x6bs                drupal-deployment-8b48bb9cb-dffqb Events:   Type    Reason                Age               From                         Message   ----    ------                ----              ----                         -------   Normal  WaitForFirstConsumer  10s               persistentvolume-controller  waiting for first consumer to be created before binding   Normal  ExternalProvisioning  2s (x2 over 10s)  persistentvolume-controller  waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator ``` I tried creating a volume mapped to /data/drupal-files but it still does not work. Lastly I want to note that the ReadWriteOnce mode works fine.  Thanks in advance, RaspberryDeveloper :)
ISSUE,  process, [Tech Debt] Merge & Move build-frontend-url function into one file in shared-utils Currently, the build-frontend-url function is used in multiple files across our codebase, resulting in duplication of code and making it difficult to maintain. As a result, we have identified this as a technical debt that needs to be addressed. To resolve this issue, we propose merging the build-frontend-url function into one file in the shared-utils folder. This will eliminate the duplicate code and make it easier to maintain in the future. We will need to update all references to the build-frontend-url function to use the new location in shared-utils. Please let us know if you have any questions or concerns regarding this proposed change. Note: This description is written by ChatGPT
ISSUE,  software quality, First Login Flow is changed to Fist Broker Login in Identity Provider ### Before reporting an issue - [X] I have searched existing issues - [X] I have reproduced the issue with the latest release ### Area identity-brokering ### Describe the bug I'm having problems with accessing Authentication area. I'm trying to connect to `https://sso.example.com/admin/realms/master/admin-ui-authentication-management/flows` and server is giving HTTP 500 error with JSON response error: unknown error. I was looking into logs and I was wondering what may cause this issue and it was affecting all of my Keycloak servers (3). Then I asked ChatGPT what is causing this at first it didn't give good enough answers but then it hinted that problem is with listIdentityProviders. ... Server Log ``` 2023-02-15 03:14:32,009 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-33) Uncaught server error: java.lang.NullPointerException 	at org.keycloak.admin.ui.rest.model.AuthenticationMapper.lambda$convertToModel$0(AuthenticationMapper.java:25) 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:176) 	at java.base/java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1632) 	at java.base/java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:127) 	at java.base/java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:502) 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:488) 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) 	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) 	at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) 	at org.keycloak.admin.ui.rest.model.AuthenticationMapper.convertToModel(AuthenticationMapper.java:26) 	at org.keycloak.admin.ui.rest.AuthenticationManagementResource.lambda$listIdentityProviders$1(AuthenticationManagementResource.java:68) 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177) 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) 	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) 	at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) 	at org.keycloak.admin.ui.rest.AuthenticationManagementResource.listIdentityProviders(AuthenticationManagementResource.java:69) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.base/java.lang.reflect.Method.invoke(Method.java:566) 	at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:170) 	at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:130) 	at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:660) 	at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:524) 	at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$2(ResourceMethodInvoker.java:474) 	at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364) 	at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:476) 	at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:434) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:192) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:141) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:32) 	at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:492) 	at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:261) 	at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:161) 	at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364) 	at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:164) 	at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:247) 	at io.quarkus.resteasy.runtime.standalone.RequestDispatcher.service(RequestDispatcher.java:73) 	at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.dispatch(VertxRequestHandler.java:151) 	at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:82) 	at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:42) 	at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284) 	at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173) 	at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140) 	at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:84) 	at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:71) 	at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284) 	at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173) 	at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140) 	at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:430) 	at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:408) 	at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284) 	at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173) 	at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140) 	at org.keycloak.quarkus.runtime.integration.web.QuarkusRequestFilter.lambda$createBlockingHandler$0(QuarkusRequestFilter.java:82) 	at io.quarkus.vertx.core.runtime.VertxCoreRecorder$14.runWith(VertxCoreRecorder.java:576) 	at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449) 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478) 	at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29) 	at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29) 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 	at java.base/java.lang.Thread.run(Thread.java:829) ``` ... I opened Identity providers area and opened one of identity provider and tried to disable that it was unsuccessful and I got next server log error. Server Log ``` 2023-02-15 03:23:54,054 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-37) Uncaught server error: org.keycloak.models.ModelException: No available authentication flow with alias: fist broker login 	at org.keycloak.models.utils.RepresentationToModel.toModel(RepresentationToModel.java:842) 	at org.keycloak.services.resources.admin.IdentityProviderResource.updateIdpFromRep(IdentityProviderResource.java:183) 	at org.keycloak.services.resources.admin.IdentityProviderResource.update(IdentityProviderResource.java:156) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.base/java.lang.reflect.Method.invoke(Method.java:566) 	at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:170) 	at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:130) 	at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:660) 	at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:524) 	at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$2(ResourceMethodInvoker.java:474) 	at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364) 	at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:476) 	at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:434) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:192) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:141) 	at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:32) 	at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:492) 	at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:261) 	at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:161) 	at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364) 	at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:164) 	at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:247) 	at io.quarkus.resteasy.runtime.standalone.RequestDispatcher.service(RequestDispatcher.java:73) 	at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.dispatch(VertxRequestHandler.java:151) 	at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:82) 	at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:42) 	at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284) 	at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173) 	at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140) 	at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:84) 	at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:71) 	at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284) 	at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173) 	at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140) 	at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:430) 	at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:408) 	at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284) 	at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173) 	at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140) 	at org.keycloak.quarkus.runtime.integration.web.QuarkusRequestFilter.lambda$createBlockingHandler$0(QuarkusRequestFilter.java:82) 	at io.quarkus.vertx.core.runtime.VertxCoreRecorder$14.runWith(VertxCoreRecorder.java:576) 	at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449) 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478) 	at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29) 	at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29) 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 	at java.base/java.lang.Thread.run(Thread.java:829) ``` Then I realized that there was typoed First login flow and then I corrected it to first broken login. And everything was working correctly afterwards. ### Version 20.0.3 ### Expected behavior It shouldn't be possible to have typoed first login flow in identity provider and updates should make sure that everything is correctly done. PS. This is just my suspicion that at some point update was changing it. ### Actual behavior At some point something changed first login flow in all identity providers to fist login flow. ### How to Reproduce? This is issue is currently not possible to reproduce unless same thing has happened to other people. This can be purposely reproduced by changing first login flow in database but this is not recommended. ### Anything else? _No response_
ISSUE,  learning, How can I insert a jingle / video intro (not strictly speaking an ad) before the main video plays ? <!-- NOTE:   This template is for questions.   If you suspect that your issue might be a bug, but aren't sure, please file the bug template instead.   If you want to know whether we will support a feature, please file the feature request template instead. --> **Have you read the [Tutorials](http://shaka-player-demo.appspot.com/docs/api/tutorial-welcome.html)?** yes **Have you read the [FAQ](https://bit.ly/ShakaFAQ) and checked for duplicate open issues?** yes **What version of Shaka Player are you using?** Latest **Please ask your question** I have searched the documentation, the open issues and the FAQ, but I could not find an answer to this question (I even asked ChatGPT who made up some configuration options that do not exist :) : How can I insert a jingle / video intro before the main video plays? I have the URL of the jingle video, (it is not strictly speaking an ad) - Is it possible to "hijack" the ad management system? - With a metadata track somehow ? Any advice is welcome! Thanks 
ISSUE,  generating/manipulating data, Use ChatGPT to generate potential domain names ChatGPT (or other AI) is capable of identifying patterns in domain names and generating new names based on the patterns. As a data source, this could be used to find additional hostnames. ### Please describe your feature request: Create a precise prompt to generate new names based on a list of existing names, and integrate with subfinder for additional name discovery. Example: ``` Using the rules defined in RFC1034 and the other RFCs which impact fully qualified domain names (FQDN), generate a list of 20 more potential DNS names based on any observable patterns based on the following list. Do not explain.  admin.acrobat.com admin.adobe.acrobat.com admin.adobe.stage.acrobat.com admin.beta.acrobat.com admin.beta.emea.acrobat.com admin.connectpro.acrobat.com admin.emea.acrobat.com admin.emea.adobe.acrobat.com admin.emea.stage.acrobat.com admin.na3.acrobat.com admin.na4.acrobat.com admin.na5.acrobat.com admin.na6.acrobat.com comments.r53.acrobat.com compete.na3.acrobat.com conferencecasting.na5.acrobat.com ``` Response: ``` accounts.na3.acrobat.com analytics.na4.acrobat.com billing.emea.acrobat.com billing.na6.acrobat.com careers.adobe.acrobat.com customersupport.na5.acrobat.com developer.beta.acrobat.com developer.emea.acrobat.com devtools.na4.acrobat.com download.adobe.acrobat.com events.na6.acrobat.com helpdesk.na3.acrobat.com marketing.beta.acrobat.com marketingconnect.na5.acrobat.com myaccount.adobe.acrobat.com partners.emea.acrobat.com partners.na4.acrobat.com payment.emea.acrobat.com press.na5.acrobat.com webinar.connectpro.acrobat.com ``` ### Describe the use case of this feature: Find additional names.
ISSUE,  learning, Paypal Payment not integrated with Django Oscar I have tried so many things, and get help from chatgpt or stackoverflow. But didn't resolve my error. Error with integrating of Paypal payment with Django Oscar. can anyone knows about it? ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. django-oscar-paypal 2.0.0 requires django<2.3,>=2.2, but you have django 3.2.18 which is incompatible.
ISSUE,  learning, [Windows 10]  "conn.bytes_sent" and "conn.bytes_recv"  error OS: { Windows 10 } Architecture: { 64bit} Psutil version: { 5.8.0 } Python version: { python3.9.7} import psutil def get_process_connections():     connections = psutil.net_connections(kind="inet")     for conn in connections:         process = psutil.Process(conn.pid)         print(f"{process.name()}|{conn.pid}|{conn.bytes_sent}|{conn.bytes_recv}") if __name__ == "__main__":     get_process_connections() This code was written by chatGPT to display the process names of all processes PID|Total amount of uploaded data|Total amount of downloaded data But chatGPT mainly encounters this "conn.bytes_sent" and "conn.bytes_recv" will always report errors, how can I rewrite it? errorÔºö Traceback (most recent call last):   File "C:\Users\Wang\PycharmProjects\pythonProject\main.py", line 10, in <module>     get_process_connections()   File "C:\Users\Wang\PycharmProjects\pythonProject\main.py", line 7, in get_process_connections     print(f"{process.name()}|{conn.pid}|{conn.bytes_sent}|{conn.bytes_recv}") AttributeError: 'sconn' object has no attribute 'bytes_sent'
ISSUE,  learning, How to optimize live streaming with ijkplayer? Optimizing live streaming with ijkplayer involves a number of techniques that can help improve the quality of the stream and reduce buffering or lag. Here are some tips to optimize live streaming with ijkplayer: 1. Use adaptive bitrate streaming: Adaptive bitrate streaming (ABR) is a technique that allows the player to automatically switch to a lower or higher quality stream based on the user's network conditions. This can help improve the viewing experience and reduce buffering. Ijkplayer supports ABR through the HLS (HTTP Live Streaming) protocol. 2. Use a content delivery network (CDN): A CDN can help improve the performance and reliability of your live streaming service by distributing your content across multiple servers. This can reduce latency and improve the viewing experience for your users. Many CDNs offer integration with ijkplayer and other media players. 3. Optimize your video encoding: The quality of your video encoding can have a big impact on the viewing experience. Make sure to use a high-quality encoding profile and optimize your settings for the resolution and bitrate of your stream. Also, make sure to use a compatible codec that is supported by ijkplayer. 4. Reduce latency: Latency is the delay between the time when a frame is captured and when it is displayed on the user's device. High latency can lead to buffering and poor user experience. To reduce latency, you can use a low-latency streaming protocol such as WebRTC or RTMP. 5. Use a hardware decoder: If your device supports it, using a hardware decoder can help improve the performance and reduce CPU usage of your live streaming service. Ijkplayer supports hardware decoding on many devices. 6. Optimize your network settings: Make sure to use a reliable and fast internet connection with low latency. You can also optimize your router and network settings to reduce interference and improve performance. These are some of the techniques you can use to optimize live streaming with ijkplayer. Keep in mind that the optimal settings may vary depending on your specific use case and requirements. It's important to test and measure the performance of your live streaming service to ensure the best possible viewing experience for your users. Suggestion by ChatGPT 
ISSUE,  learning, WCF configuration issues: slow connection, netTcpBinding and Transport security give SSPI error I'm still struggling with the configuration of my WCF services and clients - perhaps someone can shed some light on the topic? While porting a few services from .NET Remoting to WCF, I noticed several quirks, which I wasn't able to explain: 1. When using `localhost` in the service endpoint address, connection attempts work very fast (a few ms).  Using the explicit hostname (fully qualified or not) or `127.0.0.1` or `0.0.0.0` slows down each connection attempt dramatically, to about 500ms for every single connection. I couldn't find the reason for this discrepancy.  2. Using netTcpBinding with Transport security, everything works in every configuration I've tried when server and client are running on the same machine.  If the server is running on a different machine (in the same Windows Domain), I get an SSPI error when I use the server machine's **name** in the client endpoint address, but not if I use the server machine's **IP address**.  If I add `<identity><dns value="mydomain.local"/></identity>` to the client endpoint config, connection works, but I don't understand why this setting is required if both machines are part of exactly this domain?  If I add `<identity><servicePrincipalName value="anything"/></identity>` instead, it also works, completely independent of which value I give as SPN. Even an empty value works! But if I remove the node altogether, the SSPI error reappears.  I've even asked ChatGPT what the default value for the SPN is and the reply was `net.tcp/hostname_or_FQDN:port/service_name`, which sounded reasonable. Using this SPN does work, but every other string I tried worked as well, so I cannot understand the claim that this SPN is somehow validated by AD/Kerberos/whatever. I've been testing this with a small example, running both the server and the client under my user account. Environment: Windows 10 Client, Windows 11 Server, .NET Framework 4.8.1. Does anyone of the gurus have an explanation for this and some guidance what the best approach is? Thanks Martin
ISSUE,  feature implementation/enhancement, add support for pytorch cudaStream ## Description I noticed that my current TorchScript model, which includes feature processing (using CPU) and model computation (using GPU), has low GPU utilization (30-40%). I found that PyTorch provides an API similar to cpu multi-threaded called "cudaStream". Is DJL considering introducing this feature? **Will this change the current api? How?** add paramaters like `ai.djl.pytorch.num_threads` **Who will benefit from this enhancement?** Increase GPU utilization. ## References - list reference and related literature  https://github.com/pytorch/pytorch/issues/16614 - list known implementations The following code is a demo generated by ChatGPT ``` #include <torch/torch.h> #include <iostream> #include <thread> // define a function to run inference on a CUDA stream void runInference(torch::jit::script::Module& module,                   torch::Tensor input,                   at::cuda::CUDAStream& stream) {     at::cuda::CUDAGuard guard(input.device());     input = input.to(at::kCUDA, true, stream);     auto output = module.forward({input}).toTensor();     // synchronize stream before exiting     at::cuda::CUDAStreamGuard syncGuard(stream);     syncGuard.synchronize(); } // define a function to create a new CUDA stream at::cuda::CUDAStream newStream() {     at::cuda::CUDAStream stream = at::cuda::getStreamFromPool();     return stream; } int main() {     // load model and move to GPU     torch::jit::script::Module module = torch::jit::load("model.pt");     module.to(at::kCUDA);     // create two input tensors and move to GPU     torch::Tensor input1 = torch::randn({1, 3, 224, 224});     torch::Tensor input2 = torch::randn({1, 3, 224, 224});     input1 = input1.to(at::kCUDA);     input2 = input2.to(at::kCUDA);     // create two CUDA streams     at::cuda::CUDAStream stream1 = newStream();     at::cuda::CUDAStream stream2 = newStream();     // create two threads and run inference on different streams     std::thread thread1(runInference, std::ref(module), std::ref(input1), std::ref(stream1));     std::thread thread2(runInference, std::ref(module), std::ref(input2), std::ref(stream2));     // wait for threads to finish     thread1.join();     thread2.join();     return 0; } ``` 
ISSUE,  documentation, Onboarding Please add onboarding! I have spent 30 minutes installing and it's not easy to find most of the information, if you're new. I have gave up and just asked ChatGPT instead. Here are the steps you can follow to quickly start using yabai effectively: 1. Familiarize yourself with the basic concepts and terminology of yabai. yabai is a tiling window manager for macOS, which means it lets you manage your windows in a more efficient way by organizing them into tiles or "spaces." Some key terms to know include: - Space: A virtual desktop that contains one or more windows. - Window: A graphical user interface element that displays the contents of an application. - Frame: A rectangular area within a space that contains a window. 2. Learn the keyboard shortcuts for yabai. yabai is controlled entirely through keyboard shortcuts, so it's important to know how to use them effectively. Some common shortcuts include: - alt + space: Toggle the layout of the currently focused space between tiling and floating. - alt + h/j/k/l: Change the focus to the window to the left/down/up/right of the currently focused window. - alt + shift + h/j/k/l: Move the currently focused window to the left/down/up/right of its current position. - alt + enter: Make the currently focused window full screen. - alt + \: Switch to the next space. - alt + shift + \: Move the currently focused window to the next space. But most of these are not activated yet in a `skhdrc`. It would be amazing to have one-page guide on how to quickly get started.
ISSUE,  learning, TypeError: bot.getChatMembers is not a function i am facing this problem and i have been going in circles, I don't seem to find the solution. is it possible that there is NO WAY to iterate through a group's members list? even though I am using an admin bot?? how can this be? i even asked chatgpt and it also told me to use: bot   .getChatMembers(chatId)   .then((members) => {    //your code here   })   .catch((error) => {     console.error( error);   }); But this throws "getChatMembers is not a function". Can it be possible that you guys took this function away??? if so, why would you do that? i am going insane, in fact, I went insane for the past 7 hours trying to look for an alternative. please guys, how does this work? THERE MUST BE A WAY TO GET ALL USER ID so I can iterate and add or remove permissions. please help 
ISSUE,  learning, add feature: assign dns server(s) from CLI ### Homework - [X] I took the time to write a good, descriptive issue title - [X] I read `nativefier --help` and [API.md](https://github.com/nativefier/nativefier/blob/master/API.md), no existing option fits my needs. - [X] I checked [CATALOG.md](https://github.com/nativefier/nativefier/blob/master/CATALOG.md) for community suggestions & workarounds. - [X] I searched [existing issues, open & closed](https://github.com/nativefier/nativefier/issues?q=is%3Aissue). Yes, my feature request is new. - [X] I'm running the [latest version](https://github.com/nativefier/nativefier/releases). Yes, the feature I'm requesting isn't in it. ### Problem statement Nativefier should allow assignment of specific dns servers from the CLI. Nativefier, (or Electron) currently assigns google dns? ### Motivation & context Use case scenario: whether an app for personal use or an app built for employees to use in a sandboxed and limited browser experience of certain allowed websites, it would be ideal to have all ads blocked as desired.  PS., I can not get Programmatic-API to work on latest Windows, 22H2 with latest version of node and npm. I even enlisted the help pf ChatGPT to no avail. If there is a way to inject the dns through the API method, it is unavailable to me at this time. Unsure if this is Windows/NPM issue or Nativefier. I came across a file somewhere indicating API was deprecated? If so, it would help if that was posted to github. I spent a demoralizing weekend attempting to get the API working with no success. Honestly, Nativefier documentation is lacking for implementing JSON scripts and the API method for non-programmers. While I am not a pro programmer, I dabble in Linux, Rpi, Visual Basic,  and more and have Visual Studio Code and Visual Studio 2022 running. Nativefier has been a good experience to refresh my CSS and Javascript knowlege. Thanks in advance for all you do to maintain Nativefier. 
ISSUE,  learning, how extract onnx file from pt file ? hi i want to use this project on a gpu server you says download a voice from [here](https://github.com/rhasspy/larynx2/releases/tag/v0.0.2)  and extract .onnx and .onnx.json files from it. i choose the _generator-en-us_blizzard_lessac.tar.gz_ file. but there is not any onnx file in it and its content is this: ``` blizzard_lessac-medium.pt blizzard_lessac-medium.pt.json ``` now how convert these files to onxx ? I asked chatgpt and it said use this python code: ``` import torch import onnx device = torch.device("cuda" if torch.cuda.is_available() else "cpu") print(device) model = torch.load('blizzard_lessac-medium.pt', map_location=device) input_shape = (1, 80, 1000)  # Change the shape to match your model's input shape input_names = ['input'] output_names = ['output'] # Export the model to ONNX format onnx_model = onnx.export(model, args=(torch.zeros(input_shape),), f='blizzard_lessac-medium.onnx',                          input_names=input_names, output_names=output_names) onnx.save_as_json(onnx_model, 'blizzard_lessac-medium.onnx.json') ``` and this code give me this error: `ModuleNotFoundError: No module named 'larynx_train'` finally i didn't understand how can use your program ! 
ISSUE,  documentation, Investigate and add additional metadata to map.tsv - Definitely for SEO we need keywords - We will certainly need to add descriptions  - OG tags (ahrefs explains them, they're for social media) Basically look at best practice and do it for our docs. Potentially utilize chatGPT to provide suggested values for some of these.
ISSUE,  software quality, Quick Gotchas and current solutions HI Cytospace team, Thank you all for providing this cool tool! I just give it a try on our samples and it look certainly promising on my dataset so thank you for developing this tool. During the test, I ran into some issues that was resolved later.  Thought I would share the experience here in case someone else also run into these errors ### Error1: ### **"Cell type TYPE_C0 in the ST dataset is not available in the scRNA-seq dataset."** During main cytospace run, raised by line 244 in the `cytospace.py` After inspected my data, I realize this is related to `Cell ID` in `scRNA_data.txt` v.s. `cell_type_labels.txt` file. 2 scenario would trigger this error for me: 1. Some Cell ID in `snRNA_data.txt` is missing in `cell_type_labels.txt` file. Usually caused by manually post filtering the unwanted cell type e.g. Doublet, after generating the file using provided `generate_cytospace_from_scRNA_seurat_object` function.  2. All Cell IDs in `snRNA_data.txt` are in `cell_type_labels.txt` file BUT some CellType are `NA` Somehow missing value„Ñã in the `cell_type_numbers_int` inside `main_cytospace` are filled with `TYPE_C0` Solution:  Need to prepare `scRNA_data.txt` v.s. `cell_type_labels.txt` file again with following adjustment: 1. Use Seurat's implementation of `subset` to first filter out the unwanted cell type e.g. Doublet 2. Set `NA` CellType to a string such as `Unknown`.  e.g. `scRNA_obj@meta.data[['cell_type']] = tidyr::replace_na(scRNA_obj@meta.data[['cell_type']], "Unknown") ` or `scRNA_obj@meta.data[['cell_type']][is.na(scRNA_obj@meta.data[['cell_type']])] = "Unknown"` Then run `generate_cytospace_from_scRNA_seurat_object`. This make sure meta.data and count_matrix have sample Cell ID and no missing value in the desire cell type column ### Error2: ### **"ModuleNotFoundError: No module named 'cytospace.common'; 'cytospace' is not a package."** Senario: Trying to run the cytospace.py script itself by calling `python cytospace/cytospace.py` as instructed in the readme as an alternative way to run the tool. I'm personally more familiar with R than Python, so this one was solved by help of ChatGPT : ] Seemingly it's caused by the conflicting file name and package name. Changing it to something like `cytospace_script.py` fixed this. I put the ChatGPT response here as a reference.  > This error occurs because you have named your Python script file as cytospace.py, which has the same name as the cytospace module that you are trying to import. When you run import cytospace, Python first looks for a package or module named cytospace in the current directory, and since it finds your script file first, it assumes that it is the cytospace module and tries to import cytospace.common from it, which results in the error you are seeing. > To fix this issue, you need to rename your script file to something else that does not conflict with the name of the cytospace module. For example, you could rename it to cytospace_script.py. Then, when you run python cytospace_script.py, Python will not mistake your script file for the cytospace module, and the import statement should work correctly. 
ISSUE,  feature implementation/enhancement, Initial proof of concept with ChatGPT I'm using ChatGPT to figure out enough AppleScript to get at my notes data.
ISSUE,  learning, Where to add codes for saving checkpoints and loading them (train.py)? I researched about this but I couldn't figure out where to add those codes in the train.py file. After manual research on the internet I asked ChatGPT to explain and it wasn't helpful. The suggestion for saving checkpoints from ChatGPT: _"To save and load checkpoints in PyTorch, you typically need to add the code within the training loop. (...)"_ ```         # save checkpoint         if i % 1000 == 999: # every 1000 mini-batches             checkpoint = {                 'epoch': epoch,                 'model_state_dict': model.state_dict(),                 'optimizer_state_dict': optimizer.state_dict(),                 'loss': loss             }             torch.save(checkpoint, 'checkpoint.pth') ``` **Where does the training loop exactly start and does it really not matter where I add that code within the training loop?** For loading checkpoints ("_To load a checkpoint and resume training from that point, you can add the following code at the beginning of your script_"): ``` # load checkpoint checkpoint = torch.load('checkpoint.pth') model.load_state_dict(checkpoint['model_state_dict']) optimizer.load_state_dict(checkpoint['optimizer_state_dict']) epoch_start = checkpoint['epoch'] + 1 loss_start = checkpoint['loss'] ``` **But where exactly at the beginning? To me, it sounds like I can put that code it into the first line but I know that's not the right way.** Please be aware that I'm still learning and trying to understand how programming works.
ISSUE,  software quality, Compiling errors due to multiple definitions for functions. I can't get the code to compile without errors, I ran into all the issues commonly reported here, like issues getting set up with the proper libraries, but I am now stuck at an error prompt I don't know how to solve, I have no experience coding in Arduino, my only experience is in python, so I've been using chatGPT to make sense of the error prompts, and it tells me there are multiple conflicting definitions of the begin(), play(), and stop() functions for the AudioPlaySdWavX. Has anyone ran into this issue and know how to solve it?  The following is a copy of the error output: Arduino: 1.8.19 (Mac OS X), TD: 1.57, Board: "Teensy 4.0, Serial + MTP Disk (Experimental), 600 MHz, Faster, US English" /Users/jasonz./Documents/Teensyduino/libraries/MTP_Teensy-main/src/MTP_Teensy.cpp: In member function 'uint32_t MTP_class::GetObjectInfo(MTP_class::MTPContainer&)': /Users/jasonz./Documents/Teensyduino/libraries/MTP_Teensy-main/src/MTP_Teensy.cpp:523:34: warning: unknown option after '#pragma GCC diagnostic' kind [-Wpragmas]    #pragma GCC diagnostic ignored "-Wformat-truncation" /* Or "-Wformat-truncation" */                                   ^ /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::begin()' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/tools/arm/bin/../lib/gcc/arm-none-eabi/5.4.1/../../../../arm-none-eabi/bin/ld: Disabling relaxation: it will not work with multiple definitions /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::stop()' Multiple libraries were found for "play_sd_wav.h" /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here Used: /Users/jasonz./Documents/Teensyduino/libraries/audio-guestbook-main  Not used: /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/libraries/Audio Multiple libraries were found for "SD.h"  Used: /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/libraries/SD  Not used: /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/libraries/SD /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::play(char const*)' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::togglePlayPause()' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::parse_format()' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::consume(unsigned long)' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::update()' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::isPlaying()' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::isPaused()' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::isStopped()' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::positionMillis()' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `AudioPlaySdWavX::lengthMillis()' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/libraries/audio-guestbook-main/play_sd_wav.cpp.o: In function `Print::Print()': /private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: multiple definition of `vtable for AudioPlaySdWavX' /var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/arduino_build_601194/sketch/play_sd_wav.cpp.o:/private/var/folders/9z/3q5xztxs0h71l4_8sy6bywgr0000gn/T/AppTranslocation/08026AAD-97AF-4487-9C19-2EECC525E909/d/Teensyduino.app/Contents/Java/hardware/teensy/avr/cores/teensy4/FS.h:158: first defined here collect2: error: ld returned 1 exit status Error compiling for board Teensy 4.0. 
ISSUE,  learning, [QUESTION] TypeError: __init__() got an unexpected keyword argument 'bot_token' ### Issue I am facing As creating a Telegram bot, I face this issue and I can't find solutions on internet. I tried to fix this with ChatGPT but it couldn't help. I tried with "token" instead of "bot_token" but the error's the same. Thank you for your time and your help :) ### Traceback to the issue ```python TypeError: __init__() got an unexpected keyword argument 'bot_token' ``` ### Related part of your code ```python updater = Updater(bot_token='MY_TOKEN', use_context=True) ``` ### Operating System Windows 10 ### Version of Python, python-telegram-bot & dependencies ```shell python-telegram-bot 20.1 Bot API 6.5 Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)] ``` 
ISSUE,  documentation, Write a tutorial with ChatGPT Somebody definitely need to try give some Reatom examples to ChatGPT and ask to generate an explanation article. Here is set of examples: ```ts import { createCtx, action, atom } from '@reatom/core' // create context in the app root const ctx = createCtx() // define your base mutable data references // by passing a primitive initial values const searchAtom = atom('') const isSearchingAtom = atom(false) const goodsAtom = atom<Array<Goods>>([]) // define computed atoms to infer data // with smart and optimized caching const tipAtom = atom((ctx) => {   // read and subscribe by `spy`   const goodsCount = ctx.spy(goodsAtom).length   if (goodsCount === 0) {     // read without subscribing by `get`     return ctx.get(searchAtom) ? 'Nothing found' : 'Try to search something'   }   if (goodsCount === 1) {     return `We found one treasure`   }   return `Found ${goodsCount} goods` }) // define your actions to handle any IO and work with atoms const onSearch = action((ctx, event) => {   // mutate base atoms by passing relative ctx and the new state   searchAtom(ctx, event.currentTarget.value) }) const fetchGoods = action((ctx) => {   const search = ctx.get(searchAtom)   // [OPTIONAL] get your services from the context   const api = ctx.get(apiAtom)   // all sync updates inside action automatically batched   // and dependent computations will call after the action callback return   isSearchingAtom(ctx, true)   // schedule side-effects   // which will be called after successful execution of all computations   const promise = ctx.schedule(async () => {     const goods = await api.getGoods(search)     // pass a callback to `get` to batch a few updates inside async resolve     ctx.get(() => {       isSearchingAtom(ctx, false)       goodsAtom(ctx, goods)     })   })   // returned promise could be handled in place of the action call   return promise }) ``` ```ts import {   Action,   action,   Atom,   atom,   AtomMut,   throwReatomError, } from '@reatom/core' import { withReducers } from '@reatom/primitives' import { noop, sleep } from '@reatom/utils' import { getRootCause, onUpdate } from '@reatom/hooks' export interface TimerAtom extends AtomMut<number> {   /** (delay - remains) / delay */   progressAtom: Atom<number>   /** interval in ms */   intervalAtom: AtomMut<number> & {     /** @deprecated extra thing */     setSeconds: Action<[seconds: number], number>   }   /** start timer by passed interval */   startTimer: Action<[delay: number], Promise<void>>   /** stop timer manually */   stopTimer: Action<[], void>   /** allow to pause timer */   pauseAtom: AtomMut<boolean>   /** switch pause state */   pause: Action<[], boolean>   /** track end of timer. Do not call manually! */   endTimer: Action<[], void> } export const reatomTimer = (   options:     | string     | {         name?: string         interval?: number         delayMultiplier?: number         progressPrecision?: number         resetProgress?: boolean       } = {}, ): TimerAtom => {   const {     name = 'timerAtom',     interval = 1000,     delayMultiplier = 1000,     progressPrecision = 2,     resetProgress = true,   } = typeof options === 'string' ? { name: options } : options   const progressMultiplier = Math.pow(10, progressPrecision)   const timerAtom = atom(0, `${name}Atom`)   const progressAtom /* : TimerAtom['progressAtom'] */ = atom(     0,     `${name}.progressAtom`,   )   const pauseAtom: TimerAtom['pauseAtom'] = atom(false, `${name}.pauseAtom`)   const intervalAtom: TimerAtom['intervalAtom'] = atom(     interval,     `${name}.intervalAtom`,   ).pipe(     withReducers({       setSeconds: (state, seconds: number) => seconds * 1000,     }),   )   const _versionAtom = atom(0, `${name}._versionAtom`)   const startTimer: TimerAtom['startTimer'] = action((ctx, delay: number) => {     delay *= delayMultiplier     throwReatomError(delay < ctx.get(intervalAtom), 'delay less than interval')     const version = _versionAtom(ctx, (s) => s + 1)     const start = Date.now()     let target = delay + start     let remains = delay     let pause = Promise.resolve()     let resolvePause = noop     timerAtom(ctx, remains)     progressAtom(ctx, 0)     pauseAtom(ctx, false)     const cleanupPause = onUpdate(pauseAtom, (pauseCtx, value) =>       getRootCause(ctx.cause) === getRootCause(pauseCtx.cause) &&       pauseCtx.schedule(() => {         if (value) {           const from = Date.now()           pause = new Promise((resolve) => {             resolvePause = () => {               target += Date.now() - from               resolve()             }           })         } else {           resolvePause()         }       }),     )     return ctx       .schedule(async () => {         while (remains > 0) {           await sleep(Math.min(remains, ctx.get(intervalAtom)))           await pause           if (version !== ctx.get(_versionAtom)) return           const batch = ctx.get.bind(ctx)           batch(() => {             remains = timerAtom(ctx, Math.max(0, target - Date.now()))             const interval = ctx.get(intervalAtom)             const steps = Math.ceil(delay / interval)             const stepsRemains = Math.ceil(remains / interval)             progressAtom(               ctx,               +(1 - stepsRemains / steps).toFixed(progressPrecision),             )           })         }         endTimer(ctx)       })       .finally(cleanupPause)   }, `${name}.startTimer`)   const stopTimer: TimerAtom['stopTimer'] = action((ctx) => {     _versionAtom(ctx, (s) => s + 1)     endTimer(ctx)     if (resetProgress) progressAtom(ctx, 0)   }, `${name}.stopTimer`)   const endTimer: TimerAtom['endTimer'] = action((ctx) => {     timerAtom(ctx, 0)   }, `${name}.endTimer`)   const pause: TimerAtom['pause'] = action(     (ctx) => pauseAtom(ctx, (s) => !s),     `${name}.pause`,   )   return Object.assign(timerAtom, {     progressAtom,     endTimer,     intervalAtom,     startTimer,     stopTimer,     pauseAtom,     pause,   }) } ``` ```ts import { test } from 'uvu' import * as assert from 'uvu/assert' import { createTestCtx } from '@reatom/testing' import { atom } from '@reatom/core' import { onConnect } from '@reatom/hooks' import { isDeepEqual, jsonClone, sleep } from '@reatom/utils' import { reatomAsync, withDataAtom, withAbort } from '@reatom/async' test('optimistic update without extra updates on invalidation', async () => {   //#region backend   let mock = [{ id: 1, value: 1 }]   const getData = async () => mock   const putData = async (id: number, value: number) => {     await sleep()     mock = jsonClone(mock)     mock.find((item) => item.id === id)!.value = value   }   //#endregion   // this is short for test purposes, use ~5000 in real code   const INTERVAL = 5   const fetchData = reatomAsync(getData, 'fetchData').pipe(     // add `dataAtom` and map the effect payload into it     // try to prevent new reference stream if nothing really changed     withDataAtom([], (ctx, payload, state) =>       isDeepEqual(payload, state) ? state : payload,     ),   )   const updateData = reatomAsync(     (ctx, id: number, value: number) => putData(id, value),     {       name: 'updateData',       onEffect: (ctx, [id, value]) =>         fetchData.dataAtom(ctx, (state) =>           state.map((item) => (item.id === id ? { ...item, value } : item)),         ),     },   )   onConnect(fetchData.dataAtom, async (ctx) => {     while (ctx.isConnected()) {       await fetchData(ctx)       await sleep(INTERVAL)     }   })   const ctx = createTestCtx()   const effectTrack = ctx.subscribeTrack(fetchData.onFulfill)   const dataTrack = ctx.subscribeTrack(fetchData.dataAtom)   // every subscription calls passed callback immediately   assert.is(effectTrack.calls.length, 1)   assert.is(dataTrack.calls.length, 1)   assert.equal(dataTrack.lastInput(), [])   // `onConnect` calls `fetchData`, wait it and check changes   await sleep()   assert.is(dataTrack.calls.length, 2)   assert.equal(dataTrack.lastInput(), [{ id: 1, value: 1 }])   // call `updateData` and check changes   updateData(ctx, 1, 2)   assert.is(dataTrack.calls.length, 3)   assert.equal(dataTrack.lastInput(), [{ id: 1, value: 2 }])   // wait for `fetchData` and check changes   assert.is(effectTrack.calls.length, 2)   await sleep(INTERVAL)   // the effect is called again, but dataAtom is not updated   assert.is(effectTrack.calls.length, 3)   assert.is(dataTrack.calls.length, 3)   // cleanup test   dataTrack.unsubscribe() }) test('safe pooling', async () => {   const createTask = reatomAsync(async () => Math.random())   const tasks = new Map<number, number>()   const poolTask = reatomAsync(async (ctx, taskId: number) => {     ctx.controller.signal.aborted     await sleep(5)     const progress = (tasks.get(taskId) ?? -10) + 10     tasks.set(taskId, progress)     return progress   })   const progressAtom = atom(0)   const search = reatomAsync(async (ctx) => {     const taskId = await createTask(ctx)     while (true) {       const progress = await poolTask(ctx, taskId)       progressAtom(ctx, progress)       if (progress === 100) return     }   }).pipe(withAbort())   const ctx = createTestCtx()   const track = ctx.subscribeTrack(progressAtom)   const promise1 = search(ctx)   await sleep(15)   const promise2 = search(ctx)   await Promise.allSettled([promise1, promise2])   assert.is(ctx.get(progressAtom), 100)   assert.equal(     track.inputs(),     [0, 10, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],   ) }) test.run() ``` ```tsx import { action, atom } from '@reatom/framework' import { useAction, useAtom } from '@reatom/npm-react' import './App.css' // base mutable atom const inputAtom = atom('', 'inputAtom') // computed readonly atom const greetingAtom = atom((ctx) => {   const input = ctx.spy(inputAtom)   return input ? `Hello, ${input}!` : '' }, 'greetingAtom') // a logic container const onSubmit = action((ctx) => {   const greeting = ctx.get(greetingAtom)   // side-effects should be scheduled   // you could do it anywhere with `ctx`   ctx.schedule(() => alert(greeting)) }, 'onSubmit') export default function App() {   const [input, setInput] = useAtom(inputAtom)   const [greeting] = useAtom(greetingAtom)   const handleSubmit = useAction(     (ctx, event: React.FormEvent<HTMLFormElement>) => {       event.preventDefault()       onSubmit(ctx)     },   )   return (     <form onSubmit={handleSubmit}>       <h1>Reatom</h1>       <p>         <input           value={input}           onChange={(e) => setInput(e.currentTarget.value)}           placeholder="Your name"         />         <button type="submit">Submit</button>       </p>       <p>{greeting}</p>     </form>   ) } ``` ```tsx import { Action, action, atom, AtomMut, onUpdate, ParseAtoms, parseAtoms, withInit, random } from "@reatom/framework"; import { useAction, useAtom } from "@reatom/npm-react"; /* You could store your data from the backend in atoms without any mappings, but it is a good practice to wrap some of your model slices to atoms for better control and to have access to more reactive features. The rule is simple: mutable properties should be an atom, readonly properties shout stay a primitive. */ type Field = {   id: number;   name: string;   value: AtomMut<string>;   remove: Action; }; const KEY = "FIELDS"; const fromLS = () => {   const snap = localStorage.getItem(KEY);   if (!snap) return [];   const json: ParseAtoms<Array<Field>> = JSON.parse(snap);   return json.map(({ id, name, value }) => getField(id, name, value)); }; const toLS = action((ctx) => {   const list = parseAtoms(ctx, listAtom);   localStorage.setItem(KEY, JSON.stringify(list)); }, "toLS"); const getField = (id: number, name: string, value: string): Field => {   const field: Field = {     id,     name,     value: atom(value, `${name}FieldAtom`),     remove: action(       (ctx) => listAtom(ctx, (state) => state.filter((el) => el !== field)),       `${name}Field.remove`     ),   };   onUpdate(field.value, toLS);   return field; }; const listAtom = atom(new Array<Field>(), "listAtom").pipe(withInit(fromLS)); onUpdate(listAtom, toLS); const newFieldAtom = atom("", "newFieldAtom"); const createField = action((ctx) => {   if (!ctx.get(newFieldAtom)) return   const field = getField(random(), ctx.get(newFieldAtom), "");   newFieldAtom(ctx, "");   listAtom(ctx, (state) => [...state, field]); }, "createField"); const NewFieldComponent = () => {   const [input, setInput] = useAtom(newFieldAtom);   const handleCreate = useAction(     (ctx, event: React.FormEvent<HTMLFormElement>) => {       event.preventDefault();       createField(ctx);     }   );   return (     <form onSubmit={handleCreate}>       <input placeholder="Name" value={input} onChange={(e) => setInput(e.currentTarget.value)} />       <button type="submit">Create</button>     </form>   ); }; const FieldComponent: React.FC<Field> = ({ name, value, remove }) => {   const [input, setInput] = useAtom(value);   const handleRemove = useAction(remove);   return (     <span>       <input value={input} onChange={(e) => setInput(e.currentTarget.value)} />       <button onClick={handleRemove}>del</button>       {` (${name}) `}     </span>   ); }; const ListComponent = () => {   const [list] = useAtom(listAtom);   return (     <ul>       {list.map((el) => (         <li key={el.id}>           <FieldComponent {...el} />         </li>       ))}     </ul>   ); }; export default function App() {   return (     <main>       <NewFieldComponent />       <ListComponent />     </main>   ); } ``` ```tsx import { useAction, useAtom } from "@reatom/npm-react"; import {   atom,   sample,   mapPayload,   onConnect,   onUpdate,   reatomAsync,   withDataAtom,   withReducers,   withAbort,   mapState } from "@reatom/framework"; import { Lens } from "./Lens"; // `@reatom/async` docs // https://reatom.dev/packages/async type ImageData = { image_id: string; title: string }; export const fetchImages = reatomAsync(   (ctx, page: number = 1) =>     fetch(       `https://api.artic.edu/api/v1/artworks?fields=image_id,title&page=${page}&limit=${10}`,       ctx.controller     ).then<{ data: Array<ImageData> }>((r) => r.json()),   "fetchImages" ).pipe(   withDataAtom([], (ctx, { data }) => data.filter((el) => el.image_id)),   withAbort({ strategy: "last-in-win" }) ); onConnect(fetchImages.dataAtom, fetchImages); export const pageAtom = atom(1, "pageAtom").pipe(   withReducers({     next: (state) => state + 1,     prev: (state) => Math.max(1, state - 1)   }) ); onUpdate(pageAtom, fetchImages); export const lastRequestTimeAtom = fetchImages.pipe(   mapPayload(0, () => Date.now(), "fetchStartAtom"),   sample(fetchImages.onSettle),   mapState((ctx, start) => start && Date.now() - start, "lastRequestTimeAtom") ); const Paging = () => {   const [page] = useAtom(pageAtom);   const prev = useAction((ctx) => pageAtom.prev(ctx));   const next = useAction((ctx) => pageAtom.next(ctx));   return (     <>       <button onClick={prev}>prev</button>       <span> page: {page} </span>       <button onClick={next}>next</button>     </>   ); }; export default function App() {   const [lastRequestTime] = useAtom(lastRequestTimeAtom);   const [data] = useAtom(fetchImages.dataAtom);   const [isLoading] = useAtom((ctx) => ctx.spy(fetchImages.pendingAtom) > 0);   return (     <div>       <h1>artic.edu</h1>       <Paging />       <span>{!!isLoading && ` (Loading)`}</span>       <p>         <small>Loaded by {lastRequestTime}ms</small>       </p>       <ul>         {data.map(({ image_id, title }) => (           <Lens             key={image_id}             src={`https://www.artic.edu/iiif/2/${image_id}/full/843,/0/default.jpg`}             alt={title}             width={"20rem"}             height={"20rem"}           />         ))}       </ul>     </div>   ); } ``` ```tsx import { atom, reatomAsync, withAbort, withDataAtom, withRetry, onUpdate, sleep } from "@reatom/framework"; import { useAtom } from '@reatom/npm-react' import * as api from './api' const searchAtom = atom('', 'searchAtom') const fetchIssues = reatomAsync(async (ctx, query: string) => {   await sleep(350)   const { items } = await fetch(     `https://api.github.com/search/issues?q=${query}&page=${1}&per_page=10`, ctx.controller   ).then<{ items: Array<{ title: string }> }>(async (r) => {     if (r.status !== 200) throw new Error(await r.text())     return r.json()   })   return items }, 'fetchIssues')   .pipe(     withDataAtom([]),     withAbort({ strategy: 'last-in-win' }),     withRetry({       onReject(ctx, error: any, retries) {         return error?.message.includes('rate limit')           ? 100 * Math.min(500, retries ** 2)           : -1       }     })   ) onUpdate(searchAtom, fetchIssues) export default function App() {   const [search, setSearch] = useAtom(searchAtom)   const [issues] = useAtom(fetchIssues.dataAtom)   const [isLoading] = useAtom(ctx => {     // check the console and inspect nested `cause`s!     console.log(ctx)     return ctx.spy(fetchIssues.pendingAtom) + ctx.spy(fetchIssues.retriesAtom) > 0   })   return (     <main>       <input         value={search}         onChange={e => setSearch(e.currentTarget.value)}         placeholder="Search"       />       {isLoading && 'Loading...'}       <ul>{issues.map(({ title }, i) => <li key={i}>{title}</li>)}</ul>     </main>   ) } ```
ISSUE,  software quality, '_Array' object has no attribute 'pct_change' ### Expected Behavior Hey,  Im trying to capitalize on price movements that are outliers.  My code below:  ``` import yfinance as yf import numpy as np from backtesting import Backtest, Strategy import backtesting backtesting.set_bokeh_output(notebook=False) def opportunity(df):     df["pct_change"] = abs(df['Close'].pct_change())     df["std_dev_change"] = df["pct_change"].expanding().std() * 2     df = df.fillna(0)     pct_change_np = df["pct_change"].to_numpy()     std_dev_change_np = df["std_dev_change"].to_numpy()     diff = std_dev_change_np - pct_change_np     return diff       class Divergence_Strategy_Long(Strategy):     def init(self):         self.signal = self.I(opportunity, self.data, plot=True, name="Outlier", color="blue")     def next(self):         if self.signal > 0:             self.buy(size=0.75)         elif self.position:             self.position.close() ticker = "^OMX" df = yf.download(ticker, period="max", interval = "1d", progress=False, rounding=False, ignore_tz=(True)) bt = Backtest(df, Divergence_Strategy_Long, exclusive_orders=False, cash=1_000_000, trade_on_close=(False),) stats = bt.run() print(stats) ``` So what I'm trying to acheive and backtest is when the self.signal value is higher than 0, the price change in percent is higher than the standard deviation times 2. The standard deviation is calculated on the percentage change of the closing price.  ### Actual Behavior ``` Traceback (most recent call last):   File "C:\Users\Andreas\anaconda3\lib\site-packages\backtesting\backtesting.py", line 127, in I     value = func(*args, **kwargs)   File "C:\Users\Andreas\Aktieanalys\untitled3.py", line 8, in opportunity     df["pct_change"] = abs(df['Close'].pct_change()) AttributeError: '_Array' object has no attribute 'pct_change' During handling of the above exception, another exception occurred: Traceback (most recent call last):   File "C:\Users\Andreas\Aktieanalys\untitled3.py", line 28, in <module>     stats = bt.run()   File "C:\Users\Andreas\anaconda3\lib\site-packages\backtesting\backtesting.py", line 1139, in run     strategy.init()   File "C:\Users\Andreas\Aktieanalys\untitled3.py", line 18, in init     self.signal = self.I(opportunity, self.data, plot=True, name="Outlier", color="blue")   File "C:\Users\Andreas\anaconda3\lib\site-packages\backtesting\backtesting.py", line 129, in I     raise RuntimeError(f'Indicator "{name}" errored with exception: {e}') RuntimeError: Indicator "Outlier" errored with exception: '_Array' object has no attribute 'pct_change' ``` I've tried solving it with ChatGPT but it just sends me in a loop with solutions that do not work. As far as I can understand, the problem stems from row 9, that numpy does not support the ".expanding".  I do not know how to resolve/change the code to make it fit into the self.I() argument, and would be thankful for any feedback.  Thanks a lot in advance. 
ISSUE,  software quality, Slow performance after docker upgrade ### Summary - **What is wrong?** <!-- When was it noted, what is the issue, what is the scope? --> ___tl;dr:___ `apt-get install apparmor` Site is extremely slow, solr auto-restarting frequently, 503s for merge queue (EDIT: some of these issues preceded docker upgrade) We checked on  <!-- Complete these sections once the problem is resolved. Be brief. --> - **What caused it?** <!-- What did the investigation reveal to be the cause? --> At ~11am PT @cclauss performed a docker upgrade on `ol-home0` re: https://github.com/internetarchive/openlibrary/pull/7626#discussion_r1133252975. The upgrade was completed successfully however no containers were running. Initially, we suspected ~performance issues with ol-www0? We ran a `sudo docker restart openlibrary_web_nginx_1 openlibrary_web_haproxy_1 on ol-www0` which seemed to work for a moment. - **What fixed it?** <!-- What fixed the problem? --> We `ssh`'ed to `ol-home0` and noticed no containers running via `docker ps`. Identified this was related to the recent docker upgrade. When we tried to manually restart containers: ``` cd /opt/openlibrary export COMPOSE_FILE="docker-compose.yml:docker-compose.production.yml" export    HOSTNAME="$HOSTNAME" docker compose --profile ol-home0 up -d ``` And failed with error: ``` Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: unable to apply AppArmor profile: AppArmor failed to apply profile: write /proc/self/attr/apparmor/exec: no such file or directory: unknown ``` We asked our newest team member ChatGPT for help and didn't get great suggestions however, we found a hint in https://github.com/docker/for-linux/issues/1199 which suggested `apt install AppArmor`. After a restart, and re-running of the commands above, the systems were back up! - **Meta Problems** * Notifications when big production changes to `#openlibrary` * Attempt install in dev environment prior to production * Upgrade - **Followup actions:** <!-- Note any resulting changes or decisions --> https://github.com/internetarchive/openlibrary/wiki/Production-Service-Architecture#performing-upgrades ### Steps to close <!-- For instructions, please see: https://github.com/internetarchive/openlibrary/wiki/Disaster-History-Log#how-to --> 1. [x] Assignment: Is someone assigned to this issue? (notetaker, responder) 2. [x] Labels: Is there an `Affects:` label applied? 3. [x] Diagnosis: Add a description and scope of the issue 4. [x] Updates: As events unfold, is notable **provenance** documented in issue comments? (i.e. useful debug commands / steps / learnings / reference links) 5. [x] "What caused it?" - please answer in summary 6. [x] "What fixed it?" - please answer in summary 7. [x] "Followup actions:" actions added to summary 
ISSUE,  process, Automate PR creation? (e.g. via GitHub+AskGPT integration) I see you said that you _manually_ created PR #17 from the output of ChatGPT. What if we could automate PR creation? The only manual step would be to create a GitHub account for ChatGPT. After creating that, I guess we would need a script that: - Gathers GitHub credentials from an account (e.g. ssh public key or key token). - Uses a tool that can feed the output of GptRepoLoader to ChatGPT (example: AskGPT): https://github.com/praeclarum/AskGPT - Tell ChatGPT to write his solution as a .patch file. - Apply it via `git am` locally. - Push branch to ChatGPT's fork in ChatGPT's Github account. - Hit GitHubActions API to create PR.
ISSUE,  learning, ImportError: DLL load failed while importing interpreter: The specified module was not found. Hi, I wanted to write a calculator using this library, but I ended up with this error: ![](https://cdn.discordapp.com/attachments/1014200166473023540/1087393975083729056/2023-03-20_221501.png) `(–ù–µ –Ω–∞–π–¥–µ–Ω —É–∫–∞–∑–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å - The specified module was not found.)` I naturally installed the library, even tried reinstalling it. But nothing helps. I'm not a very experienced programmer, so from what I've tried to solve this problem is: reinstall the library and ask ChatGPT how it's solved. Oh, well, also find information on Stackoverflow and don't understand anything. Lib version: 2.8.4 OS: Windows 11 (build 22621.1344) Tested with Python 3.11 and 3.10 Code: ```py import numexpr expr = input("–í–≤–µ–¥–∏—Ç–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ: ") result = numexpr.evaluate(expr) print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç {result}") ```
ISSUE,  feature implementation/enhancement, Transpile to cito This discussion is more of a brainstorming session than addressing a specific issue. I have been experimenting with ChatGPT to generate reliable code through a semi-automated TDD process that incorporates feedback loops. The system works quite well for Python, and now I'm exploring the possibility of adding more languages. However, the success of this approach depends heavily on how well the language model understands the programming language in question. It works well with Python, likely because the model has the most extensive knowledge of Python among all languages. Another challenge is that adding more languages necessitates language parsing, splitting tests into separate chunks, handling imports, running tests, and other complex tasks. I'm curious if there's a way to convert this code to Cito (from any "known" language) and then obtain multiple possible implementations "for free." How is the AST represented in Cito? How difficult would it be to load an external AST into Cito? Which language is the closest or has the most features similar to Cito? Does this idea even make sense?
ISSUE,  learning, bug on path rewrite on devProxy i think ### Environment node v18.14.0  nuxt 3.3.1 ### Reproduction create a nuxt 3 project with follow structure: / /web/ /web/create nuxt config: nitro: { 	devProxy: { 		'/api': { 			target: process.env.NUXT_PUBLIC_API_MANAGER_BASE, 			changeOrigin: true, 			pathRewrite: {'^/api': ''}, 		}, 	}, } when requests are made from /web/ and / works and on /web/create dont work on request url from /web/create stays like this: http://localhost:3000/web/manager/ but on /web/ and / stays like: http://localhost:3000/manager/ ### Describe the bug I cant made a single request on files on folders who isnt the index, the word "web" for example, dont get remove. talking with chatGPT he gave me the code: nitro: { 	devProxy: { 		'/manager': { 			target: process.env.NUXT_PUBLIC_API_MANAGER_BASE, 			changeOrigin: true, 			pathRewrite: {'^/manager': ''}, 		}, 		'/web/manager': { 			target: process.env.NUXT_PUBLIC_API_MANAGER_BASE, 			changeOrigin: true, 			pathRewrite: {'^/web/manager': '/manager'}, 		}, 	}, }, and this works. There's a way to improve it or make it simple, like, the devProxy scan all folders and get all files and apply the same rule for all? ### Additional context _No response_ ### Logs _No response_
ISSUE,  generating/manipulating data, Add more napkin math problems to https://sirupsen.com/ Hi @sirupsen - not sure if this is the right place to post this, but I've done most of your napkin math problems on your website, and would love to do more, if you could make them. Granted, I'm sure it takes a while to come up with and solve these problems. So, I noticed we could use ChatGPT to generate these problems and create a initial solution: These are not the best but you could make better ones, if you give it a topic and some more details. <img width="638" alt="Screen Shot 2023-03-21 at 10 28 28 PM" src="https://user-images.githubusercontent.com/12554095/226786286-4fbe6fa8-fd02-499f-8aee-af3cc2e1b6dd.png"> <img width="699" alt="Screen Shot 2023-03-21 at 10 30 50 PM" src="https://user-images.githubusercontent.com/12554095/226786619-29a6be7d-644d-4ccd-a7f1-d55546fb4157.png"> 
ISSUE,  learning, Definite guide to changing the default password single node deployment, container.  Good day Is there a 100% working guide on how to change the admin password from SecretPassword to something else? I've went through every single guide, even asked ChatGPT 4 with a doc dump into the prompt. Nothing works. My wazuh deployment is now in a worse of state than before. 
ISSUE,  documentation, Initial design Goal is to have a plugin that turns any Datasette instance into a ChatGPT plugin - providing instructions for ChatGPT to execute SQL queries against it.
ISSUE,  feature implementation/enhancement, ChatGPT says this exists in JavaScript (aka hurry up) ChatGPT just told me to use `RegExp.escape()` but it was throwing error in my console. ChatGPT said it was added in ECMAScript 2019 which I found no evidence for, so I asked ChatGPT and it apologized and said it was actually added in ECMAScript 2021. Upon Googling, I found this repo, so is it stagnant? This functionality appears to stem from Ruby or probably different languages too, and it would be most efficient solution for me to escape while doing `new RegExp(searchTerms, 'i').test(value)` when searchTerms is `|` or `\`. ChatGPT gave me a custom solution that looks reasonable: ``` javascript const escapedTerms = searchTerms.replace(/[.*+?^${}()|[\]\\]/g, '\\$&'); const regex = new RegExp(escapedTerms, 'i'); const isMatch = regex.test(value); ``` In my current application, I found this third party dependency (Oruga UI) that is using a custom solution that is the same but oddly different: ``` javascript /**  * Escape regex characters  * http://stackoverflow.com/a/6969486  */ export function escapeRegExpChars(value) {     if (!value) return value;     return value.replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, '\\$&'); // eslint-disable-line no-useless-escape } ``` After probing ChatGPT further, an issue there seems to stem from its knowledge cutoff of 2021, so my final takeaway is that this proposal looks simple but possibly stagnant and should be kicked. Given that it involves a final code as simple as `''.replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, '\\$&')`, I would say hurry up and deploy it, and if we're too scared, maybe start with an options object that features `include` and `exclude` to opt-in characters or opt-out characters beyond the reasonable default. I want to have the following code: ``` javascript new RegExp(RegExp.escape(searchTerms), 'i').test(value) ```
ISSUE,  learning, Android aarch64 not working Hello  I am conducting my thesis on android java process manipulation. I cant seem to get jattach working on a rooted XIAOMI Mi 9 Lite what has android 10 QKQ1.190828.002 with kernel 4.9.186-perf-G73C8056.  I downloaded the ARM and aarch versions found from this issue https://github.com/jattach/jattach/issues/62 but every time i get following error: ``` user$ ls -la total 4036 drwxrwx--x    6 shell    shell         4096 Mar 25 16:08 . drwxr-x--x    6 root     root          4096 Mar  5 13:31 .. drwxrwxr-x    5 shell    shell         4096 Mar  1 15:51 .studio drwxr-xr-x    3 root     root          4096 Mar  5 13:28 POC -rwxr-xr-x    1 root     everybod   3670016 Mar 16  2020 dumproid -rwxrwxrwx    1 root     everybod     20192 Sep 10  2022 jattach drwxrwxrwx    5 shell    shell         4096 Mar  1 15:22 perfd user$ file jattach jattach: ELF 64-bit LSB executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=51aba8fa1d1797e5fa8fbf4fac16a8121ed68972, for GNU/Linux 3.7.0, not stripped user$ file dumproid dumproid: ELF 64-bit LSB executable, ARM aarch64, version 1 (SYSV), statically linked, Go BuildID=aR_EWk9akGijNJXMclXq/BoW6QGGQVuKAOWWh_I7Q/kvj8QdVT_6h7HYmfAwUp/EUFFcZMXAPzN9ZIF4Fg2, stripped user$ ./jattach /system/bin/sh: ./jattach: No such file or directory user$ ``` I dont have experience with make or cc but I have managed to find following information: * https://stackoverflow.com/questions/53344338/cant-run-aarch64-compiled-binary-on-aarch64-android-rooted-device  It seems that the jattach  interpreter /lib/ld-linux-aarch64.so.1 is not available in my Android phone. I used find . -name "ld-linux-aarch64.so" on root dir and got nothing.  * With previous information I tried to use chatGPT to suggest me a solution. It sugested me to use android NDK when compiling. I crafted this make file but it fails. ``` JATTACH_VERSION=2.1 ifneq ($(findstring Windows,$(OS)),)   CL=cl.exe   CFLAGS=/O2 /D_CRT_SECURE_NO_WARNINGS   JATTACH_EXE=jattach.exe   JATTACH_DLL=jattach.dll else    JATTACH_EXE=jattach   UNAME_S:=$(shell uname -s)   ifeq ($(UNAME_S),Darwin)     CFLAGS ?= -O3 -arch x86_64 -arch arm64 -mmacos-version-min=10.12     JATTACH_DLL=libjattach.dylib   else     CFLAGS ?= -O3     JATTACH_DLL=libjattach.so   endif   ifeq ($(UNAME_S),Linux)     TOOLCHAIN=/mnt/c/Users/manderka/Desktop/3aastakevad/LT/android-ndk-r25c/toolchains/llvm/prebuilt/linux-x86_64     SYSROOT=$(TOOLCHAIN)/sysroot     CC=$(TOOLCHAIN)/bin/aarch64-linux-android21-clang   endif endif .PHONY: all dll clean all: build build/$(JATTACH_EXE) dll: build build/$(JATTACH_DLL) build: 	mkdir -p build build/jattach: src/posix/*.c src/posix/*.h 	$(CC) $(CPPFLAGS) $(CFLAGS) $(LDFLAGS) -DJATTACH_VERSION=\"$(JATTACH_VERSION)\" --sysroot=$(SYSROOT) -o $@ src/posix/*.c build/$(JATTACH_DLL): src/posix/*.c src/posix/*.h 	$(CC) $(CPPFLAGS) $(CFLAGS) $(LDFLAGS) -fPIC -shared -fvisibility=hidden --sysroot=$(SYSROOT) -o $@ src/posix/*.c build/jattach.exe: src/windows/jattach.c 	$(CL) $(CFLAGS) /DJATTACH_VERSION=\"$(JATTACH_VERSION)\" /Fobuild/jattach.obj /Fe$@ $^ advapi32.lib /link /SUBSYSTEM:CONSOLE,5.02 clean: 	rm -rf build ``` Error: ``` user@DESKTOP-F9687LK:/mnt/c/Users/manderka/Desktop/3aastakevad/LT/jattach-master$ make /mnt/c/Users/manderka/Desktop/3aastakevad/LT/android-ndk-r25c/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android21-clang  -O3 -static  -DJATTACH_VERSION=\"2.1\" --sysroot=/mnt/c/Users/manderka/Desktop/3aastakevad/LT/android-ndk-r25c/toolchains/llvm/prebuilt/linux-x86_64/sysroot -o build/jattach src/posix/*.c src/posix/jattach_openj9.c:292:15: warning: implicit declaration of function 'semget' is invalid in C99 [-Wimplicit-function-declaration]     int sem = semget(sem_key, 1, IPC_CREAT | 0666);               ^ src/posix/jattach_openj9.c:299:9: warning: implicit declaration of function 'semop' is invalid in C99 [-Wimplicit-function-declaration]         semop(sem, &op, 1);         ^ 2 warnings generated. clang: error: unable to execute command: posix_spawn failed: Exec format error clang: error: linker command failed with exit code 1 (use -v to see invocation) make: *** [Makefile:38: build/jattach] Error 1 ``` Sadly I am all out of ideas what to do due to my lack of knowledge in managed code. What would the next step be in finding the solution?  Plenty of sunshine Karl Erik
ISSUE,  process, feature request: Support `fetch` over libp2p I would like this to work: ``` fetch(https://QmPeerID.libp2p/some-http-api) ``` (Placeholder format, not sure what the url should look like. Suggestions welcome). We could have a service worker intercept the fetch call, and make the HTTP request over a libp2p stream (like WebTransport, WebSockets, and, I think with the main page‚Äôs cooperation, WebRTC.) This would allow a browser to use the exact same code to make requests to a normal https endpoint and a libp2p peer. No code changes required on the browser side. It effectively enables HTTP to be used in p2p environment. For example, a browser user may see that my laptop has the content it wants, but it doesn‚Äôt have a domain name it could reach. If my laptop exposes the HTTP gateway api, the browser user could simply `fetch` the content from my browser and the service worker would handle the request over libp2p. --- Alternative/Complementary ideas: 1. This could move even more logic to the service worker, such that the service worker can verify CIDs. Then browsers can simply `fetch` some cid and trust that it‚Äôs correct. (open question: can the service worker silently fail here?)   a. The service worker could also get more freedom to use bitswap to fulfill the request. 2. This could be a library that integrates with existing service workers in case a user already has an existing service worker. Or stand on its own in case a user doesn‚Äôt have one already. Related:  - https://github.com/ipfs/in-web-browsers/issues/207#issuecomment-1474108826 --- FAQ Q: Can‚Äôt we already do this with just a libp2p stream? Why do we need HTTP? A: Yes, it is possible to use libp2p streams directly for communication between peers. However, integrating HTTP with libp2p in a browser context offers several advantages: 1. Compatibility with existing web infrastructure: By utilizing HTTP, you can leverage the existing web technologies, APIs, and tools that are built around it. This makes it easier for developers to adopt and integrate with their current applications, as they don't have to learn a new protocol or change their existing codebase significantly. 2. Developer familiarity: Most web developers are already familiar with HTTP and how to work with it in the context of web browsers. Using HTTP over libp2p streams enables developers to use their existing knowledge and skills, lowering the barrier to entry for decentralized web development. 3. Reuse of HTTP semantics: HTTP is a well-defined and widely used protocol that provides features such as caching, content negotiation, and authentication. By using HTTP over libp2p, you can take advantage of these features and build on top of an established, reliable protocol. 4. Browser support: Browsers have built-in support for HTTP, which means that using HTTP over libp2p allows for more seamless integration with browser APIs like fetch. This makes it easier to build applications that can be easily accessed and used by users without the need for additional plugins or extensions. -FAQ answer by ChatGPT (proofread by me) 
ISSUE,  feature implementation/enhancement, [Feature request] File mode (e.g. aichat -r comment -f somefile.ts) It would be nice to have an inline file mode that allows using the content of a file as the aichat input and writing back the output to the file. It would allow this kind of use case for adding code comments to a file. ```console aichat -r comment -f somefile.ts  ``` Or fixing grammar: ```console aichat -r grammar -f README.md ``` etc. If you approve, I can try writing a PR. Temporary bash workaround (generated by ChatGPT :P): ```bash #!/bin/bash # Check if there are enough arguments if [ "$#" -lt 2 ]; then     echo "Usage: $0 <aichat_options> <filename>"     exit 1 fi # Store the last argument (filename) and remove it from the argument list FILENAME="${!#}" shopt -u nocasematch set -- "${@:1:$#-1}" # Check if the file exists if [ ! -f "$FILENAME" ]; then     echo "Error: File '$FILENAME' not found"     exit 1 fi # Create a temporary file in the OS temporary directory TMPFILE=$(mktemp) echo "$FILENAME" echo "${@}" # Pass all the arguments except the last one to the aichat command aichat "${@}" < "$FILENAME" > "$TMPFILE" # Move the temporary file to the original file mv "$TMPFILE" "$FILENAME" ```
ISSUE,  software quality, [WeeklyReport] Weekly report for lowcode-engine 3/20/2023 to 3/27/2023 # Weekly Report of lowcode-engine This is a weekly report of lowcode-engine. ### For more details about developer contirubtions, please check our [contribution leaderboard](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=lowcode-engine). It summarizes what have changed in the project during the passed week, including pr merged, new contributors, and more things in the future. ## Repo Overview ### Basic data Baisc data shows how the watch, star, fork and contributors count changed in the passed week. | Watch | Star | Fork | Contributors | |:-----:|:----:|:----:|:------------:| | 168 | 11003 (-) | 1831 (-) | 57 (-) | ### Issues & PRs Issues & PRs show the new/closed issues/pull requests count in the passed week. | New Issues | Closed Issues | New PR | Merged PR | |:----------:|:-------------:|:------:|:---------:| | 23 | 18 | 15 | 14 | ## PR Overview Thanks to contributions from community, **14** pull requests was merged in the repository last week. They are: | Contributor ID | Count | Pull Requests | |:--------------:|:-----:|:-------------| | @liujuping | 9 | #1753 fix: fix workspace api ts defined <br>#1754 feat: support PanelDock icon pointer cursor is always pointer <br>#1755 feat: update skeleton ts defined <br>#1764 feat: resourceList children support different resourceName <br>#1765 fix: fixed an issue where the outline tree was not displayed correctly when deleting a node <br>#1779 fix: fix cant change the panel tree item title with prop api <br>#1780 feat: update props ts defined <br>#1782 fix: fix the problem that the props value appears when the name is 0 <br>#1783 feat: update IPublicTypeComponentMetadata interface <br> | | @keuby | 1 | #1742 feat: add setConfig method for project <br> | | @LeoYuan | 1 | #1784 feat: add code review job by chatgpt <br> | | @Justin-lu | 1 | #1772 fix: ts interface error <br> | | @AndyJinSS | 1 | #1774 fix: optimize interface type export <br> | | @huxingyi1997 | 1 | #1741 fix: missing i18n param in simulator and renderer <br> | ## Code Review Statistics lowcode-engine encourages everyone to participant in code review, in order to improve software quality. This robot would automatically help to count pull request reviews of single github user as the following every week. So, try to help review code in this project. | Contributor ID | Pull Request Reviews | |:--------------:|:--------------------:| | @LeoYuan | 2 | | @ranyue | 1 | ## New Contributors We have no new contributors in this project this week. lowcode-engine team encourages everything about contribution from community. For more details, please refer to https://github.com/alibaba/lowcode-engine/blob/master/CONTRIBUTING.md . 
ISSUE,  feature implementation/enhancement, Notify_push without init system available I would like to use the files HPB but unfortunately my hosting provider does not allow me to create a daemon. I‚Äôm wondering if it‚Äôs possible to use a cron that checks if the push server is running and if it‚Äôs not restarts it ? It may not be as reliable as using an init system but it‚Äôs maybe the only way I can be able to run the push server on the same host as my nextcloud instance.  It seems like this would be much simpler than setting up a reverse proxy to a different machine on a different network. My provider does provide me with redis, so it seems all I am missing is a daemon. After a discussion with chatGPT I was thinking something like could work ? ``` #!/bin/bash # Set environment variables export SOCKET_PATH=/path/to/socket/nextcloud.socket # Check if push server is running if ! pgrep -x "notify_push" > /dev/null then     # Start the push server     nohup /path/to/push/binary/notify_push --config=/path/to/nextcloud/config/config.php --socket-path=$SOCKET_PATH & fi ``` Since I‚Äôm not a developer, I thought to validate my approach with the community before trying it. Any response would be much appreciated
ISSUE,  software quality, Parsing as className instead of class for React > First of all I'm sorry to open an issue in your GitHub repository! I really appreciate the tool you created and thank you for the work. ## Main question I was wondering if there is a way to parse markdown with this tool and give the attribute `className` instead of the regular `class` when using a comment like: ```markdown Some paragraph of my markdown file... <!--rehype:className=regularText--> ``` The problem with this is that if I render things as `.use(rehypeAttr, { properties: 'attr' })`, this gives a translation that adds `class` to the resulting `<p>` instead of `className`. I'm asking this in the hope that something already exists, but if not I'd be happy to collaborate on some code for the implementation. ## Another question I was using ChatGPT to help me fix this, and it suggested a syntax similar to this: ```js .use(rehypeAttr, {       p: { className: 'text-gray-700 my-10' },       h1: { className: 'text-3xl font-bold' }, }) ``` Is this valid? Because when I try to use it nothing happens.
ISSUE,  process, Folia support for ServerTap? Here is the repo link: https://github.com/PaperMC/Folia **__Here is a summary of everything made with ChatGPT to save some time:__** __For plugin developers, there are a few key takeaways from the given text:__ 1. Folia introduces regionised multithreading, which means that plugins will require modification to function properly. Plugin developers should expect compatibility with existing plugins to be at zero. 2. Folia introduces new API additions, such as the RegionisedScheduler and EntityScheduler, which can help plugins run on the correct thread context. 3. To ensure compatibility with Folia, plugin developers must mark their plugin as compatible by placing "folia-supported: true" in the plugin.yml file. Additionally, plugins must take advantage of the RegionisedScheduler and EntityScheduler API to ensure correct behavior. 4. Plugins should not assume that they can access or modify data in other regions, as Folia regions tick in parallel and do not share data. Plugin developers should take advantage of the upcoming thread-check API to ensure correct behavior. 5. There are currently broken API, including the scoreboard API, and planned API changes, such as super aggressive thread checks, which may impact plugin development. Overall, plugin developers should be prepared for significant changes in their plugins to ensure compatibility with Folia. They should take advantage of the new API additions and mark their plugin as compatible to ensure correct behavior. Additionally, they should be aware of the limitations and rules of Folia's regionised multithreading to avoid bugs and errors in their plugins.
ISSUE,  learning, Question: How to connect a sound source to the bluetooth output? This is not an issue but a question. I'm sorry, but I don't know were else to ask. I even tried chatGPT to help me on this:) So I use the rg353m as my [m8-Tracker](https://dirtywave.com/), a music creation device that's inspired by [lsdj](https://www.littlesounddj.com/lsd/index.php). There is a headless version that runs on a [Teensy 4.1](https://www.pjrc.com/store/teensy41.html), a powerful microcontroller board. This can be plugged in to the usb-c-otg port on the rg353m and used by an [interface software](https://github.com/jasonporritt/rg351_m8c) that is to be installed to the ports folder. It works really well when running it with the built in speakers but sadly not with bluetooth audio. My question is how to connect an external audio device (in this case the teensy-board) to the bluetooth output?  The script that starts the software uses this command to make the connection from the audio device to the speakers: `alsaloop -P hw:0,0 -C hw:2,0 -t 200000 -A 5 --rate 44100 --sync=0 -T -1 -d` when I use aplay -l to to list all audio outputs, it lists two loopback devices that are created when bluetooth is activated. They are numbered listed as card 2. So I tried to adapt the command above to match the playback device to hw 2: `alsaloop -P hw:2,0 -C hw:2,0 -t 200000 -A 5 --rate 44100 --sync=0 -T -1 -d` But this just leads to no sound at all. I've tried many different versions of this command with no luck. So I guess my approach is wrong somehow.  So a hint on how to connect a sound source to the bluetooth output would be very much appreciated!
ISSUE,  documentation, Document the new string functions ### Related code PR https://github.com/risingwavelabs/risingwave/pull/8767 ### Which part(s) of the docs might be affected or should be updated? And how? SQL - Functions - String Please experiment with creating docs for these functions using chatGPT Adds the following functions: chr(integer) -> varchar starts_with(varchar, varchar) -> boolean initcap(varchar) -> varchar lpad(varchar, integer) -> varchar lpad(varchar, integer, varchar) -> varchar rpad(varchar, integer) -> varchar rpad(varchar, integer, varchar) -> varchar reverse(varchar) -> varchar strpos(varchar, varchar) -> integer to_ascii(varchar) -> varchar to_hex(integer) -> varchar to_hex(bigint) -> varchar ### Reference Please reference PG's docs for these string functions.
ISSUE,  learning, Accept EULA remote doesn't work or isn't well documented ### Brief description of your issue I rolled out Winget to our domain machines and i'm facing an issue where i cant install a package due to the fact the msstore agreement terms aren't accepted. The exact error: ``` ...> winget upgrade "Adobe Acrobat (64-bit)" --silent --accept-source-agreements --accept-source-agreements The 'msstore' source requires that you view the following agreements before using. Terms of Transaction: https://aka.ms/microsoft-store-terms-of-transaction The source requires the current machine's 2-letter geographic region to be sent to the backend service to function properly (ex. "US"). Failed when searching source: winget An unexpected error occurred while executing the command: 0x8a15000f : Data required by the source is missing ``` ### Steps to reproduce Let me walk you through the steps i've taken. First step, install Winget on remote computers ``` Invoke-Command -ComputerName $Computername -ScriptBlock {     $Hostname = Hostname     #Install Winget and extra's     Invoke-WebRequest -Uri https://www.nuget.org/api/v2/package/Microsoft.UI.Xaml/2.7.3 -OutFile C:\temp\microsoft.ui.xaml.2.7.3.zip        Expand-Archive -Path "C:\temp\microsoft.ui.xaml.2.7.3.zip" -DestinationPath "C:\temp\microsoft.ui.xaml.2.7.3"     Add-AppxPackage C:\temp\microsoft.ui.xaml.2.7.3\tools\AppX\x64\Release\Microsoft.UI.Xaml.2.7.appx     Invoke-WebRequest -Uri https://github.com/microsoft/winget-cli/releases/download/v1.3.2691/Microsoft.DesktopAppInstaller_8wekyb3d8bbwe.msixbundle -OutFile C:\temp\Microsoft.DesktopAppInstaller_8wekyb3d8bbwe.msixbundle     Invoke-WebRequest -Uri https://aka.ms/Microsoft.VCLibs.x64.14.00.Desktop.appx -OutFile C:\temp\Microsoft.VCLibs.x64.14.00.Desktop.appx     Add-AppxPackage C:\temp\Microsoft.VCLibs.x64.14.00.Desktop.appx     Add-AppxPackage C:\temp\Microsoft.DesktopAppInstaller_8wekyb3d8bbwe.msixbundle     #Clean-up     Remove-Item "C:\temp\microsoft.ui.xaml.2.7.3" -Recurse     Remove-Item "C:\temp\microsoft.ui.xaml.2.7.3.zip"             Remove-Item "C:\temp\Microsoft.VCLibs.x64.14.00.Desktop.appx"     Remove-Item "C:\temp\Microsoft.DesktopAppInstaller_8wekyb3d8bbwe.msixbundle"     #Show version if succes     Write-Host "$Hostname - $(winget -v)" } ``` This installs version `v1.3.2691` on all computers. ### Expected behavior I excepted so see that is would upgrade adobe reader.  ### Actual behavior Next step was trying to `upgrade` something like `Adobe Reader`. I first searched using `Winget search adobe` for the right package name which led me to: ``` winget upgrade "Adobe Acrobat (64-bit)" --silent ``` But this resulted in the error: ``` ...> winget upgrade "Adobe Acrobat (64-bit)" --silent The `msstore` source requires that you view the following agreements before using. Terms of Transaction: https://aka.ms/microsoft-store-terms-of-transaction The source requires the current machine's 2-letter geographic region to be sent to the backend service to function properly (ex. "US"). Do you agree to all the source agreements terms? [Y] Yes  [N] No: An unexpected error occurred while executing the command: 0x8a150042 : Error reading input in prompt ``` So after some googling i found out i can add the `--accept-source-agreements` and `--accept-package-agreements` parameters to accept it remotely. Like this topic describes: #1536 but as you can see above in the first code block this still results in that error.  Next step was asking ChatGPT since i didn't find any other hint on google. ChatGPT suggested running: ``` Start-Process -FilePath "winget" -ArgumentList "features", "-a" -WindowStyle Hidden -Wait ``` But this didn't change anything. Back to google and i found out that there is also an ADMX file for GPO control. Imported that, enabled Winget AND set the trusted sources to `msstore` and `winget` but again without success. Finally I tried reinstalling everything and also `source reset` but all with no luck. Am i missing something here? Hopefully someone could pinpoint the problem. Thanks! ### Environment ```shell Windows Package Manager v1.4.10173 Copyright (c) Microsoft Corporation. All rights reserved. Windows: Windows.Desktop v10.0.19044.2728 System Architecture: X64 Package: Microsoft.DesktopAppInstaller v1.19.10173.0 Logs: %LOCALAPPDATA%\Packages\Microsoft.DesktopAppInstaller_8wekyb3d8bbwe\LocalState\DiagOutputDir User Settings: %LOCALAPPDATA%\Packages\Microsoft.DesktopAppInstaller_8wekyb3d8bbwe\LocalState\settings.json Links --------------------------------------------------------------------------- Privacy Statement   https://aka.ms/winget-privacy License Agreement   https://aka.ms/winget-license Third Party Notices https://aka.ms/winget-3rdPartyNotice Homepage            https://aka.ms/winget Windows Store Terms https://www.microsoft.com/en-us/storedocs/terms-of-sale ``` 
ISSUE,  software quality, Can't truncate tables with foreign keys I have two tables that have a foreign key, and I want to truncate both of them. When I do it, though, I get the following error.  ``` psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint DETAIL:  Table "tutor_bot_messages" references "tutor_bot_conversations". HINT:  Truncate table "tutor_bot_messages" at the same time, or use TRUNCATE ... CASCADE. ``` I asked ChatGPT for an answer:  * how do i truncate tables with foreign keys using pgantomizer?  * To truncate tables with foreign keys using pgantomizer, you can use the cascade_truncate function provided by the tool. The cascade_truncate function will automatically disable or drop the foreign key constraints, truncate the table, and then re-enable or recreate the foreign key constraints. It would be great if that were actually true
ISSUE,  learning, New to programming -- Keep getting AttributeError I'm on Windows 10. Running python 3.7 on a conda virtual env. Using VScode. I have tried to install swisseph and pyswisseph in every way possible to try and get it to work. I finally got swisseph working and recognized as a module, and I'm using chatgpt to help me write code and learn how to integrate the ephermis into the gpt api. We're going through the steps and it gave me the example code below to use to connect with postman/flask and run the calculate_planet_position function.  The problem is that when I run it, the attribute swe.MERCURY and swe.FLG_SPEED and swe.FLG_SWIEPH and the other attributes are just not being recognized for some reason by my terminal.  I keep getthing the error: _AttributeError: module 'swisseph' has no attribute 'FLG_SWIEPH_  I dont know what I'm doing wrong. Was hoping to get some help with this.  `      from flask import Flask, request      import swisseph as swe     app = Flask(__name__)     @app.route('/planet_position', methods=['GET'])     def planet_position():         # Get the parameters from the request         date = float(request.args.get('date'))         longitude = float(request.args.get('longitude'))         latitude = float(request.args.get('latitude'))         # Call the calculate_planet_position function to get the planet's position         planet_longitude, planet_latitude = calculate_planet_position(date, longitude, latitude)         # Return the planet's position as a JSON response         return {             'planet_longitude': planet_longitude,             'planet_latitude': planet_latitude         }     def calculate_planet_position(date, longitude, latitude):         """         Calculates the position of Mercury at a given date, longitude, and latitude.         Args:             date (float): The date and time in Julian days UTC.             longitude (float): The longitude of the observer in degrees.             latitude (float): The latitude of the observer in degrees.         Returns:             tuple: The longitude and latitude of Mercury in degrees.         """         # Set up the parameters for the calculation         planet = 1  # Use the integer planet number for Mercury         flags = swe.FLG_SWIEPH | swe.FLG_SPEED         position_format = swe.FLG_EQUATORIAL | swe.FLG_SPEED | swe.FLG_TRUEPOS         # Set the observer location         observer_location = [longitude, latitude, 0]         # Calculate the planet's position         planet_position = swe.calc_ut(date, planet, flags)         planet_longitude, planet_latitude, _ = swe.calc_ut(planet_position[0], planet, position_format)         # Convert the planet's position to the observer's location         swe.set_topo(observer_location[0], observer_location[1], observer_location[2])         swe.set_sid_mode(swe.SIDM_LAHIRI)         planet_longitude, planet_latitude, _ = swe.get_topo(planet_longitude)     if __name__ == '__main__':         app.run() `
ISSUE,  learning, [Docs]: Themes: more clear guidance needed on how to find dependencies <!-- Thanks for taking the time to open an issue and help us make Jekyll better! --> ## Motivation In [Themes](https://jekyllrb.com/docs/themes/#converting-gem-based-themes-to-regular-themes) documentation: > Then you must tell Jekyll about the plugins that were referenced by the theme. You can find these plugins in the theme‚Äôs gemspec file as runtime dependencies. It is not obvious where `gemspec` is located and i could not find any other way to list gems on which theme depends via bundle command, even with ChatGPT and Bing AI I was only able to find gemspec for minima theme here `/usr/gem/specifications/minima-2.5.1.gemspec` Which is not an obvious location ## Suggestion I suggest to improve the clarity of the docs page to make it easier to find theme's dependencies. <!-- Thanks for taking the time to open an issue and help us make Jekyll better! --> 
ISSUE,  software quality, [Bug]: Offline mode not working on page instances ### Bug expectation I navigate Twitter homepage, waited 3 seconds, and try to set the page offline, before navigating Twitter homepage again. The offline mode did not work. I got this error instead: Error: net::ERR_INTERNET_DISCONNECTED at https://twitter.com Am I missing some command line arguments. The same issue was also found on Windows (I have not tested a Linux distro so far) ### Bug behavior - [ ] Flaky - [ ] PDF ### Minimal, reproducible example ```TypeScript const puppeteer = require('puppeteer'); (async () => {     const browser = await puppeteer.launch({         headless: false     })     const page = await browser.newPage();     await page.goto('https://twitter.com')     await asyncTimeout(3000);     await page.setOfflineMode(true);     //await page.setRequestInterception(true);     await page.goto('https://twitter.com'); }) (); // Function provided by ChatGPT :-) async function asyncTimeout(timeout) { return new Promise(resolve => {     setTimeout(() => {     resolve(`Resolved after ${timeout} milliseconds`);     }, timeout); }); } ``` ### Error string Error: net::ERR_INTERNET_DISCONNECTED at https://twitter.com ### Puppeteer configuration _No response_ ### Puppeteer version 19.8.2 ### Node version 19.6.0 ### Package manager npm ### Package manager version 9.4.0 ### Operating system macOS
ISSUE,  learning, terminal on Mac should use user preferred shell. I always use `fish` instead of `bash` or `zsh`. I hope  terminal of cursor can detect the  default shell of the system. I asked ChatGPT 4, how to get the default shell of Mac system? It get it correctly! ``` dscl . -read /Users/$(whoami) UserShell ## the output UserShell: /usr/local/bin/fish ```
ISSUE,  learning, Serialize and Deserialize using reflection Hello im trying to create a network system to sync my components between server and client. To do this im thinking that i could use the reflection system of entt to know which component and member to modify.  After googling and asking chatgpt a lot for a solution i can't seem to find one that gets the result im looking for. I'd imagine that this would be the same if i wanted to save/load the data from a file, from the code snippet below is there a way to get/set the data? ```cpp entt::entity entity = registry.create(); std::string component_class_name = "my_type"; std::string component_member_name = "value"; std::string component_member_type = "float"; float component_member_data = 21.4f; ``` I've tried using a bunch of different ways but all of them require me to use template of the type at some point it seems like which i can't really do since i need it to be runtime. The code below is as far as i've been able get before im requied to use compile time stuff. ```cpp entt::meta<my_type>() 	.type(entt::hashed_string("my_type")) 	.data<&my_type::value>(entt::hashed_string("value")); auto type = entt::resolve(entt::hashed_string("my_type")); auto data = type.data(entt::hashed_string("value")); ``` Is there no way to set the data of my_type on entity using the 'type' and 'data' variables above? ChatGPT recommended something like this multiple times but there doesn't seem to exist something like this so im not sure where its getting that from but anyway, is there a way to do something like it? ```cpp auto component = registry.get(type, entity); data.set(component, component_member_data); ```
ISSUE,  learning, Support  @module-federation/client not found  ### Discussed in https://github.com/module-federation/module-federation-examples/discussions/577 <div type='discussions-op-text'> <sup>Originally posted by **Pravin** April 10, 2023</sup> I am having an issue with installing module-federation/client I am currently learning how to use webpack and module federation. I have build a simple calculator app and a container app with create-mf-app.It worked when i try to import the calculator component into the container app but now i am trying to import it into an existing react application but i am faced with an error saying that calculator falls outside the project directory. I have set up the relevent files such as webpack.config in my existing react app which i want to import the calculator component but i ask chatgpt and it says that i need to install module-federation/client for it to work. The existing react app is built with "create-react-app" and the calculator&container is built with create-mf-app. I am using react and nodejs . Bellow is the error i am getting :</div> <img> ![image](https://user-images.githubusercontent.com/64627871/230824401-7f88d664-5fb0-45c6-9446-d55c5fca0185.png) <img> <div> bellow is the webpack.config for the existing react app that i am trying to import my micro frontend component:<div> ![image](https://user-images.githubusercontent.com/64627871/230840557-22dc337a-aa92-442c-b453-071c3df0e570.png) <div> bellow is the webpack.config for the calculator micro frontend component : <div> ![image](https://user-images.githubusercontent.com/64627871/230840714-b97050e4-6d66-4de1-ac7c-032d68d4d2d2.png) <div>Please Help and thank you for your time.<div>
ISSUE,  feature implementation/enhancement, [Question/Improvement]Add Save/Load binding from llama.cpp First I want to say I really enjoy this binding, it's working as expected, useful and simple etc. etc.  But I would say I am missing one very crucial feature - save and load the model state to file. For example I want to ask the agent for improvement suggestions, and then use his suggestion as part of a prompt. All this means I want to save and load model state, and not run it all from the start again. I've checked the llama repository and they had this [issue](https://github.com/ggerganov/llama.cpp/issues/827) and they closed it, as if it was solved.  I used chatGPT to write me save/load function based on their comments. Now I want to add it to llama, and then to here.  The question here would be: how would I add this functionality from llama.cpp to this binding?  P.S. Another small suggestion regardless of llama binding is the option to have a call_back function that would stop the generation, for example when ### Human / ### Instructions appears in the response.
ISSUE,  development environment, A few feature requests Hi authors, thank you for writing this awesome tool!   JSON readability is very important but I think it's overlooked by the standard library. May I request the following features to make this library easier to use? 1. Could you add a `dump` method to directly dump the re-formatted JSON string to a JSON file? 2. Would it be possible to add a sanity checking method to check that the deserialized objects from the old string and the new string are identical? 3. Could you add a command line argument --output-file and let users directly create a new JSON file with formatted contents? 4. Could you register a command line entry point, so that users can use this tool in the command line with `compact-json ...` instead of `_compact_json.py ...`? -------------------- Additional notes for Request No. 4 above: I see that this package uses `poetry`.  I'm not so familiar with `poetry` so I asked ChatGPT how to register an entry point with `poetry`.  Below is my Q&A with ChatGPT: #### Question: How do I register a command line entry point for a Python package when I'm using poetry? #### Answer: To register a command line entry point for a Python package when using Poetry, you can use the `console_scripts` section in your `pyproject.toml` file. Here's an example `pyproject.toml` file: ``` [tool.poetry] name = "mypackage" version = "0.1.0" description = "My Python package" authors = ["Your Name <you@example.com>"] [tool.poetry.dependencies] python = "^3.9" [tool.poetry.scripts] mypackage-cli = "mypackage.cli:main" [build-system] requires = ["poetry-core>=1.1.0"] build-backend = "poetry.core.masonry.api" ``` In this example, we have specified a script called `mypackage-cli` which points to a function called `main` in a module called `cli` inside a package called `mypackage`. To use the command line entry point, you can install the package using `poetry install` and then run the command `mypackage-cli` in your terminal. ``` poetry install mypackage-cli ``` This will execute the `main` function defined in your `cli` module. You can replace `mypackage-cli` with any name you want for your command line entry point. 
ISSUE,  software quality, ActiveRecord::NotNullViolation in Devise::Api::TokensController#sign_in üëã I wanted to give a try to this gem but when I try to sign_in I get this error: ActiveRecord::NotNullViolation in Devise::Api::TokensController#sign_in PG::NotNullViolation: ERROR: null value in column "resource_owner_id" of relation "devise_api_tokens" violates not-null constraint  DETAIL: Failing row contains (21, User, null, 8rUvzXixhsNeyypZBWjsm3-aQszFgrBKm2EngYsy1S24PGsYSFrwD6Cxh3Hu, KuxVpFjzq8vopMMzztR3f7i47LgsST33XL44tAzCVA19xCnxZGLztgHwdTDk, 3600, null, null, 2023-04-12 09:50:42.582888, 2023-04-12 09:50:42.582888). I tried to debug with ChatGPT without success, I can't find where this ressource_owner_id got lost...
ISSUE,  documentation, I want to add a small nice thing to documentation for IntelliJ-Ultimate users My chat with ChatGPT (useless things left out): ME: in intellij for recognizing database schemas is it possible to hardcode a schema if the real db cannot be recognized? Do you know what I mean? GPT-4: I apologize for the confusion in my previous response. I made an error in my instructions, as [...]. In this case, you can still create an SQLite data source in IntelliJ to help it recognize the schema and provide better code assistance when working with SQL files and code blocks in your project. However, the process is slightly different: Go to View > Tool Windows > Database to open the Database tool window. Click the + button and select Data Source > SQLite. Instead of selecting the schema.sql file in the "File" field, create an empty SQLite database file. You can do this by clicking the "Create" button next to the "File" field and providing a name for the new SQLite database file (e.g., my_database.sqlite). This file will be created in your project directory. Click "OK" to save the data source. Now, you should have an SQLite data source in your Database tool window. To create the schema in the new SQLite database file, follow these steps: [IN THE DATABASE SIDEBAR TAB] Right-click on the SQLite data source you just created in the Database tool window. Select "Open SQL Console" to open a new SQL console connected to your SQLite data source. Copy and paste the SQL schema you prepared earlier into the SQL console. Run the SQL script by clicking the "Execute" button or pressing Ctrl+Enter (Cmd+Enter on macOS). This will create the schema in your SQLite database file, and IntelliJ should now recognize it and provide better code assistance when working with SQL files and code blocks in your project. Regarding my previous instructions, I apologize for any confusion I may have caused. I appreciate your patience and understanding as we work to find the correct solution for your needs. Links: (note: you have to be in the database tool window and click [QL]) https://www.jetbrains.com/help/idea/working-with-database-consoles.html#create_console if it is possible to even use SQL-wasm.wasm for this then apologize, then I would not include this into the docs. Otherwise I think this can help you spot bugs a lot easier in IntelliJ. Previously I had 500 errors in my db-service.ts files, now I have zero and see right away if I make a mistake Of course I would emphasize that you need to always keep your fake .sqlite database file schema up to date to get the correct IntelliJ-Errors
ISSUE,  learning, javacc plugin 1.10 not working with Idea 23.1 I don't know of any information, how to use the javacc plugin, but from my discussion with chatGPT I expect that after installation and right clicking on "new" I get a context menu saying "javacc File". And after editing a .jj file and right clicking I should get a context menu with "run javacc". Either one happens. The installation is successfull. An opened .jj file shows the three yellow line icon in its tab, but not in the project pane. After clearing the file cache, the icon appears for about 20 seconds and then changes back. Please clarify, how to use the plugin and to compile with javacc within Idea. Thank you, Wolfgang Gruber 
ISSUE,  learning, how to access postgres environemt in the asdf? I am having an issue where I can't create a database due to the following error: `createdb: error: connection to server on socket "/tmp/.s.PGSQL.5432" failed: FATAL:  role "luisgomes" does not exist` In order to solve that, I ran `sudo -i -u postgres` and then `createuser -s -P luisgomes`.  It gave me an `bash: createuser: command not found...` . I asked chatGPT what should I do and it explained that createuser is available only on the postgresql created by **asdf**. It also told me to run `asdf shell postgres 15.2` but nothing happened. (15.2 is the version I have installed) What should I do now?
ISSUE,  learning, How to close the flow when using the following code? package main import ( 	"context" 	"errors" 	"fmt" 	"io" 	openai "github.com/sashabaranov/go-openai" ) func main() { 	c := openai.NewClient("your token") 	ctx := context.Background() 	req := openai.ChatCompletionRequest{ 		Model:     openai.GPT3Dot5Turbo, 		MaxTokens: 20, 		Messages: []openai.ChatCompletionMessage{ 			{ 				Role:    openai.ChatMessageRoleUser, 				Content: "Lorem ipsum", 			}, 		}, 		Stream: true, 	} 	stream, err := c.CreateChatCompletionStream(ctx, req) 	if err != nil { 		fmt.Printf("ChatCompletionStream error: %v\n", err) 		return 	} 	defer stream.Close() 	fmt.Printf("Stream response: ") 	for { 		response, err := stream.Recv() 		if errors.Is(err, io.EOF) { 			fmt.Println("\nStream finished") 			return 		} 		if err != nil { 			fmt.Printf("\nStream error: %v\n", err) 			return 		} 		fmt.Printf(response.Choices[0].Delta.Content) 	} } ------ // The response provided by chatgpt seems to be that go openai does not have this method import (     "net/http"     "github.com/sashabaranov/go-openai"     "github.com/gin-gonic/gin" ) // Initialize OpenAI API client client := openai.NewCompletionAPI("YOUR_API_KEY") // Define HTTP endpoint to cancel a stream func cancelStream(c *gin.Context) {     // Extract stream ID from request data     var json struct {         StreamID string `json:"stream_id" binding:"required"`     }     if err := c.ShouldBindJSON(&json); err != nil {         c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})         return     }     // Call OpenAI API to cancel stream     _, err := client.CancelStream(json.StreamID)     if err != nil {         c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})         return     }     // Return success response     c.JSON(http.StatusOK, gin.H{"status": "success"}) } // Start Gin HTTP server and register endpoint r := gin.Default() r.POST("/cancel_stream", cancelStream) r.Run(":8080")
ISSUE,  software quality, Read body from NetworkResponseReceived event Rod Version: v0.112.8 ## The code to demonstrate your question ``` 	go page.EachEvent(func(e *proto.PageLoadEventFired) {                 //some stuff 		wg.Done() 	}, func(e *proto.NetworkResponseReceived) { 		if e.Response.URL == "https://thotsbay.ac/search/search" {                         // how can I get the response body here ? 			fmt.Println(e.Response) 		} 	})() ``` ## What you got The NetworkResponseReceived object, but I'm looking for the body of the response (which in this case is some json) ## What have you tried to solve the question Searching in the doc, the internet, ask chatGPT 
ISSUE,  learning, Installig pycurl on windows Things I have tried: pip install pycurl --global-option="--with-curl=c:/1/Curl/bin" libcurl location: c:\1\Curl\bin\libcurl.dll c:\1\Curl\bin\libcurl.def Error message: ``` Collecting pycurl   Using cached pycurl-7.45.2.tar.gz (234 kB)   Installing build dependencies ... done   Getting requirements to build wheel ... error   error: subprocess-exited-with-error   √ó Getting requirements to build wheel did not run successfully.   ‚îÇ exit code: 10   ‚ï∞‚îÄ> [1 lines of output]       Please specify --curl-dir=/path/to/built/libcurl       [end of output]   note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error √ó Getting requirements to build wheel did not run successfully. ‚îÇ exit code: 10 ‚ï∞‚îÄ> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. ``` I tried so many things, I'm not kiddding but also tried to use chatgpt to help install it. And still failed. I also read the "documentation" : http://pycurl.io/docs/latest/install.html#pip-and-cached-pycurl-package Simply not helpful with PIP. I still get the same problem. (I've cygwin installed too) 
ISSUE,  feature implementation/enhancement, I wrote some CSS for the settings page I was bored so I asked chatGPT to write some CSS for the settings page, and then slightly modified it. Feel free to use I (the bullet point before the first style block should be an * asterisk)t:  `<style>  * {      margin: 0;      padding: 0;      box-sizing: border-box;      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;  }  body {      background-color: #f5f5f5;      color: #333;      line-height: 1.6;  }  /* Container */  .container {      max-width: 960px;      margin: 0 auto;      padding: 30px;      background-color: #fff;      box-shadow: 0px 1px 3px rgba(0, 0, 0, 0.12), 0px 1px 2px rgba(0, 0, 0, 0.24);  }  /* Headings */  h1,  h2,  h3,  h4,  h5,  h6 {      margin-bottom: 20px;  }  /* Form */  form {      margin-top: 20px;  }  .form-group {      margin-bottom: 20px;      display: flex;      align-items: center;      justify-content: space-between;      flex-wrap: wrap;  }  label {      font-weight: bold;      display: inline-block;      margin-bottom: 5px;      width: 30%;      text-align: right;      padding-right: 15px;  }  select,  input[type="text"],  input[type="email"],  input[type="password"] {      width: calc(65% / 3 - 10px);      padding: 10px;      border: 1px solid #ddd;      border-radius: 5px;      outline: none;      margin-right: 5px;  }  input[type="checkbox"] {      width: auto;  }  button {      padding: 10px;      background-color: #007BFF;      color: #333;      border: none;      border-radius: 5px;      cursor: pointer;      font-size: 18px;      margin-top: 20px;  }  button:hover {      background-color: #0056b3;  }  </style>`
ISSUE,  software quality, Program not working after copying the exact same code as in Github. I copied every file that adrianhajdin posted here in Github.  When I try to use the program the AI bot stays on "loading" mode as the three dots keep on showing up. This is what console log looks like: ![image](https://user-images.githubusercontent.com/56239254/233686732-8197153d-4519-4039-8a0e-010ad02f5b81.png) I've been trying to fix this with ChatGpt for over 2 hours and I tried everything but I just can't get it working. Please helppppp :)
ISSUE,  process, Add Timestamp to Cart Instance ## Disclaimer This issue was written by ChatGPT lol ## Feature Proposal: Add Time Stamps to Cart Entries ### Overview Currently, there's no way to make changes or updates to the cart based on time. This proposal suggests adding time stamps to cart entries, enabling updates based on time. Time stamps should be created for every cart and turned on by default. ### Implementation The time stamp should be created in the `Entry.js` file located in the `core` folder. The `createEntry` function is the ideal location for creating the time stamp. Here's what the `createEntry` function currently looks like: ```javascript export function createEntry({   state,   id,   product,   count,   price_metadata,   product_metadata }) {   const entry = Entry({     id,     product,     quantity: count,     price_metadata,     product_metadata   })   state.cartDetails[id] = entry   updateFormattedValue(state, id)   updateFormattedPrice(state, id)   state.totalPrice += entry.value   state.cartCount += count   updateFormattedTotalPrice(state) } ``` To add the time stamp, we can simply modify the `createEntry` function to include the current time: ```javascript import { formatISO } from 'date-fns' export function createEntry({   state,   id,   product,   count,   price_metadata,   product_metadata }) {   const entry = Entry({     id,     product,     quantity: count,     price_metadata,     product_metadata,     timeStamp: formatISO(new Date()) // Add the time stamp here   })   state.cartDetails[id] = entry   updateFormattedValue(state, id)   updateFormattedPrice(state, id)   state.totalPrice += entry.value   state.cartCount += count   updateFormattedTotalPrice(state) } ``` ### Future Expansion This proposal focuses on adding time stamps to cart entries as an initial step. We can expand on time-oriented features in the future based on the requirements and use cases. Feel free to modify the content as needed, but this structure should give you a clear and concise proposal for adding time stamps to the cart entries in the `use-shopping-cart` library. Good luck!
ISSUE,  learning, How I Use Authentication  Can You Give Me The Example Code That Allow Me Use Authentication To Login. I Ask ChatGPT But ChatGPT Give Me Wrong Answer üòû 
ISSUE,  feature implementation/enhancement, FR: Detect or test for certain words I really want something that will listen for a particular word, like "um", and then run an action when the word is heard, like printf("\a"). I tried to add this myself but I learned C++ 20+ years ago and I haven't done anything with it since. I even tried to get ChatGPT to help me and it almost got me there, but I am going in circles. If this is a really easy add, could you add it? If it's not easy, then I might keep trying if I have the time. Edit: I was trying to add this to stream.cpp.
ISSUE,  feature implementation/enhancement, "Enter" to confirm key would it be possible to add the possibility to use the key "enter" to confirm the key we enter when launching the .exe ? i asked chatgpt how to and it gave me this :  ```cpp void InputWindow::AddTextBox(int id, const std::string& text, int x, int y, int width, int height) {     HWND textBox = CreateWindowEx(0, L"EDIT", L"", WS_CHILD | WS_VISIBLE | WS_BORDER | ES_AUTOHSCROLL, x, y, width, height, m_window, reinterpret_cast<HMENU>(id), m_instance, nullptr);     SendMessage(textBox, WM_SETFONT, reinterpret_cast<WPARAM>(m_textBoxFont), TRUE);     SetWindowText(textBox, make_wstring(text).c_str());     auto const handler = [](HWND hwnd, UINT message, WPARAM wParam, LPARAM lParam)     {         auto const ctrl = reinterpret_cast<InputWindow*>(GetWindowLongPtr(hwnd, GWLP_USERDATA));         switch (message)         {             case WM_KEYDOWN:             {                 if (wParam == VK_RETURN)                 {                     // Set focus to the button and simulate a click                     SetFocus(GetDlgItem(ctrl->m_window, 1));                     SendMessage(GetDlgItem(ctrl->m_window, 1), BM_CLICK, 0, 0);                 }                 break;             }         }         return CallWindowProc(ctrl->m_oldTextBoxProc, hwnd, message, wParam, lParam);     };     SetWindowLongPtr(textBox, GWLP_USERDATA, (LONG_PTR)this);     m_oldTextBoxProc = reinterpret_cast<WNDPROC>(SetWindowLongPtr(textBox, GWLP_WNDPROC, reinterpret_cast<LONG_PTR>(handler)));     m_textBoxes.emplace_back(textBox); } ```
ISSUE,  learning, [BUG] #Below is my code from pytube import YouTube url = "https://www.youtube.com/watch?v=rg6CiPI6h2g" video = YouTube(url) stream = video.streams.get_highest_resolution() print("downloading...") stream.download() #This is an error Traceback (most recent call last):   File "C:\Users\User\Desktop\yap guang shun\python\pytube\test.py", line 9, in <module>     stream = video.streams.get_highest_resolution()              ^^^^^^^^^^^^^   File "C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\site-packages\pytube\__main__.py", line 296, in streams     return StreamQuery(self.fmt_streams)                        ^^^^^^^^^^^^^^^^   File "C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\site-packages\pytube\__main__.py", line 176, in fmt_streams     stream_manifest = extract.apply_descrambler(self.streaming_data)                                                 ^^^^^^^^^^^^^^^^^^^   File "C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\site-packages\pytube\__main__.py", line 161, in streaming_data     return self.vid_info['streamingData']            ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^ KeyError: 'streamingData' #Version Python 3.11.3 pytube 12.1.3 pytube3 12.1.3 #I have modified and tried to ask chatGPT many times, but still get an error
ISSUE,  feature implementation/enhancement, Surface crate security information in the UI ## Background As part of the Rust Foundation's security initiative, we'd like to surface information related to crate security more prominently within crates.io. Our initial focus is on supply chain security, so surfacing information relevant to provenance is key, but we would also like to rapidly start surfacing information relevant to the security of individual crate versions as well. Items below that are related to open questions are linked and _italicised_. ## ~~tl;dr~~ executive summary We would like to add [_a new tab_](#what-does-this-tab-look-like) to the crate[^crate] page that surfaces security information relevant to the crate in an easy to digest form. In the initial version, this will include two things: the result of any [checks](#checks) run on the crate (the most recent version, in the case of the unversioned crate version), and ‚Äî if present ‚Äî the security policy in the `SECURITY.md`[^security-md] file [_in the repo_](#how-do-we-discover-the-security-policy). The initial check that would be added is almost certainly a provenance check[^provenance], with other checks[^other-checks] based on static analysis of the repository and code to be added in short order. ## Pictures These are static mockups for now, although I'm going to wire up a prototype PR Real Soon Now‚Ñ¢[^rsn] to better explore this space. (Obligatory disclaimer: I am not, nor do I pretend to be, a designer. I intend to iterate further on this; this is mostly just to show the shape of what I'm thinking, rather than the specifics of the design around typography, spacing, colours, etc.) ### Crate nav bar The styling is probably too dramatic, particularly in the failed check case, but you get the general idea. (And the "icons" are just Unicode right now, but would probably be replaced with a more appropriate SVG in due course.) ![pass-bar](https://user-images.githubusercontent.com/229984/234981669-cdc20bc1-5130-4a17-a065-90875d2752bf.png) ![fail-bar](https://user-images.githubusercontent.com/229984/234981690-5f47dffd-aeb2-4359-810e-5478cc4a03e2.png) ### Security tab I used ChatGPT to create a "plausible sounding security policy" for this screenshot. All errors are therefore‚Ä¶ uh, someone else's. Promise. (And, yes, the padding appears to be off just because of how Firefox chose to screenshot the relevant element.) ![success-security](https://user-images.githubusercontent.com/229984/234981718-017bda2e-4073-4dc5-8eeb-07b988c2bf6e.png) ![fail-security](https://user-images.githubusercontent.com/229984/234981734-389ccb71-9278-4763-b603-2902dea65386.png) ## Security policies We would discover the security policy using the same [general heuristics](https://docs.github.com/en/code-security/getting-started/adding-a-security-policy-to-your-repository) as GitHub uses, with one addition to handle repositories with multiple crates: 1. Is there a `SECURITY.md` file at the same level as `Cargo.toml`? 2. Is there a file at the repository root? 3. Is there a file in `docs` (relative to the repository root)? 4. Is there a file in `.github` (relative to the repository root)? This will obviously require Cargo assistance for anything beyond the first point, since theose files may not be in the `.crate` tarball. [_We may also want to add a new `Cargo.toml` field_](#how-do-we-discover-the-security-policy), similar to [the existing `readme` field](https://doc.rust-lang.org/cargo/reference/manifest.html#the-readme-field). ## Checks The idea here is much like general CI checks in GitHub, GitLab, or your favourite code host. Each check will represent a single pass/fail/skipped check for a specific crate version, with some level of content shown in the UI to indicate the result in more detail and contextualise what the check is actually showing. As mentioned above, the initial check this would roll out with would be a provenance check for the source code in the crate tarball. It is anticipated that the initial work here would only involve checks run on a global basis. These checks would be facilitated (and, initially, developed) by the Rust Foundation in conjunction with the crates.io team. Over time, I expect this would open up to allow other collaborators within the Rust ecosystem to propose and implement other checks that make sense within the ecosystem. It is also possible in the future that these checks may feed into a quarantine system where crates that fail key, high fidelity security checks require human review before being published. That is not in the scope of this proposal, however. An open question here is whether [_crate authors should be able to define their own, crate-specific checks_](#should-we-allow-crate-authors-to-include-their-own-checks). ### Operation Without going into great detail just yet on the schema or API calls, here's how I anticipate this would work: 1. crates.io can be configured with a webhook[^webhook] that is invoked when a crate version is published. 1. On publish, the webhook would be invoked with the crate version metadata and a URL including a one time shared secret that can be used to report check results back. 1. Each check started would then:    1. Report that it has started.    1. Report success, failure, or skipped with a structured blob[^blob] that also includes relevant data for the UI to display, describing the result and what the check actually checked. ## Open questions ### Should we allow crate authors to include their own checks? As discussed in the [checks section](#checks), an additional possibility here would be to allow crate authors to define crate-specific checks they want to run when their crate is published, and then surface those results alongside the security checks that are run over all published crates. ### What does this tab look like? "Security" would be the obvious label, but if we allow non-security checks as well, then either a more generic name or two tabs might make more sense. ### How do we discover the security policy? I think the heuristic in the [security policies section](#security-policies) is probably uncontroversial. Whether we should add a new `Cargo.toml` field is a different question.[^epage] Because this is a relatively new "standard" from GitHub, there isn't really the diversity of possible locations that there is with READMEs ‚Äî we can reasonably assume the file will be called `SECURITY.md`, and that it will be in Markdown format. However, I do wonder if there are organisations that would prefer to link out to a single security policy, rather than adding it to each repository. My gut feeling is that a new field _may_ make sense, but only if it also allows for URLs to be specified, with some sort of handling in the crates.io UI to then provide a useful looking link (presumably augmented with Open Graph metadata where available). Whether this is better than simply having a `SECURITY.md` in the repository that points to the external policy is unclear to me. ## We need you! Nothing in here is set in stone, or closed to discussion. I genuinely want your feedback! Ideally, I'd like to have feedback in within the next couple of weeks (so by early-to-mid May) ‚Äî I intend to do some prototyping work in parallel with the feedback session to continue to explore this space, but obviously no concrete decisions will be made until after that. I'm also more focused on the admin console work in #6353, since that's a higher priority for me right now, so there isn't a tonne of urgency on this just at the moment. Nevertheless, I would love to hear from you! [^blob]: There's complexity that we definitely don't need to reproduce here, but a cut down version of [the `output` field in GitHub check results](https://docs.github.com/en/rest/checks/runs?apiVersion=2022-11-28#create-a-check-run) is probably pretty close to what I'm thinking. [^crate]: For simplicity, I'm mostly talking in terms of crates in the UI, but I'm aware that this will also extend to crate version specific routes. Consider "crate" shorthand for both unless there's an explicit callout that something's different on a version route. [^epage]: My thanks to @epage for pointing this out. [^other-checks]: This is a significant focus area for @walterhpearce in the near term, so I would expect the initial batch of follow up checks would likely come from the Foundation. [^provenance]: Or, put in plain English, does the content of the `.crate` file match what was tagged in the source repository? [^rsn]: I'm also working on #6353 simultaneously, and that's higher priority, so take this more as an expression of intention than a rock solid commitment. [^security-md]: This is a [GitHub initiative](https://docs.github.com/en/code-security/getting-started/adding-a-security-policy-to-your-repository), announced [back in 2019](https://github.blog/changelog/2019-05-23-security-policy/), to make it easier for authors to provide security policies for their projects in a standard place. [^webhook]: Initially, I expect this will invoke a service or lambda in Rust Foundation managed infrastructure than can then fan out to whatever checks are available.
ISSUE,  learning, Code 952 - Invalid FileMaker Data API token (*) Edit: I guess after analyzing the fmrest.js in /lib there doesn't seem to be any functionality for automatic refreshing of the access token. Was this in a previous version?  I have an app that sets up a new instance of fmrest and provides an `onTokenRefresh` option that is supposed to refresh the token automatically when necessary.  So I wanted to test this, so I opened up the web page and loaded a customer's file, and let it sit there for about 30 minutes or so before trying to refresh the page. When I refreshed the page, I received this error in the console: ``` {     "messages": [         {             "code":  "952",             "message": "Invalid FileMaker Data API token (*)"         }     ]     "response": {} } ``` The app is very simple, here is the code for it: ``` require('dotenv').config(); const express = require('express'); const fmrest = require('fmrest'); const app = express(); const port = 3000; let token = null; // Create a single fmrest instance const fm = new fmrest({     host: process.env.FM_HOST,     database: process.env.FM_DATABASE,     layout: process.env.FM_LAYOUT,     user: process.env.FM_USERNAME,     password: process.env.FM_PASSWORD,     auth: 'basic',     onTokenRefresh: (newToken) => {         console.log(`Token refreshed, newToken value: ${newToken}`);         token = newToken;     }, }); // Login to FileMaker fm.login()     .then((data) => {         console.log('fm:', JSON.stringify(fm, null, 3));         token = data?.response?.token;         console.log(`Token value: ${token}`);         app.get('/customer/:customerId', async (req, res) => {             try {                 const customerId = req.params.customerId;                 if (!customerId) {                     return res.send('No customer ID provided');                 }                 let request = fm.createRequest().where('ID').is(customerId);                 fm.find({ requests: [request], options: { token: token } }).then(                     (data) => {                         console.log(JSON.stringify(data, null, 3));                     },                 );                 return res.send('Customer ID: ' + customerId);             } catch (err) {                 res.status(500).send(err.message);             }         });         app.listen(port, () => {             console.log(`Listening on port ${port}`);         });     })     .catch((err) => {         console.error(`Error logging in to FileMaker: ${err}`);     }); ``` Am I doing something wrong, or does fmrest not refresh the token, or are my expectations of how it refreshes the token incorrect? I've been using ChatGPT to help me figure it out, but so far I haven't had any luck and ChatGPT says my code is correct and should refresh the token automatically.  
ISSUE,  learning, Error on bazel build to use mediapipe  pose_detection ### OS Platform and Distribution Windows 10, 64 bit ### Compiler version _No response_ ### Programming Language and version Java ### Installed using virtualenv? pip? Conda?(if python) _No response_ ### MediaPipe version mediapipe-master folder maybe 0.9.3.0 ### Bazel version 3.22.1 ### XCode and Tulsi versions (if iOS) _No response_ ### Android SDK and NDK versions (if android) SDK=33 and NDK=25.2.9519653 ### Android AAR (if android) No ### OpenCV version (if running on desktop) _No response_ ### Describe the problem ```shell I am using mediapipe-master downloaded from github and I want to use pose detection in android studio so I am trying to create aar file but on running command I am getting error  I got many errors previously too but I solved it using chatGPT on this error I asked ChatGPT it is saying to add 'declare toolchain info' in Build file by adding ("@bazel_tools//tools/build_defs/config:toolchains.bzl", "declare_local_toolchain_info") declare_local_toolchain_info(     name = "py_toolchain",     toolchain = "@local_config_python//:toolchain", )  but In my bazel folder bazel_tools//tools/build_defs/ does not contain config folder I hope you understand the problem. ``` ### Complete Logs ```shell Command Run  build -c opt --config=android_arm64 mediapipe/modules/pose_detection:*, Got Error ```ERROR: C:/users/asrock/downloads/mediapipe-master/mediapipe/framework/tool/BUILD:109:10: While resolving toolchains for target //mediapipe/framework/tool:encode_as_c_string: invalid registered toolchain '@local_execution_config_python//:py_toolchain': target does not provide the DeclaredToolchainInfo provider ERROR: Analysis of target '//mediapipe/modules/pose_detection:pose_detection_cpu.inc' failed; build aborted: INFO: Elapsed time: 0.441s INFO: 0 processes. FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)``` ``` 
ISSUE,  documentation, Create a doc page for migrating a parent Create a new markdown https://github.com/netdata/netdata/tree/master/streaming/MIGRATE.md based on https://community.netdata.cloud/t/how-to-migrate-a-parent-to-a-new-server-and-keep-the-previously-collected-data/3967 It will need some cleanup. Pass it through chatgpt to improve it. Publish it under https://learn.netdata.cloud/docs/deployment-in-production/streaming-and-replication/
ISSUE,  learning, Please help me, I have tried a variety of methods to compile, all failed, including using the method provided by chatGPT 4.0. also failed. Please help me to compile the third-party library of this link with cmake gui under Windows. https://github.com/mfontanini/cppkafka, please give the most detailed compilation process, including the compilation process of dependent libraries, to Debug x64 version ![image](https://user-images.githubusercontent.com/45205351/236397516-8d24f68b-c6ef-42a7-a83e-0a62e9afa3f8.png) 
ISSUE,  feature implementation/enhancement, Best practice: How to phrase traitlets into js classes? I asked chatGPT to convert this code into a widget ```js %%react import React, { PureComponent } from 'react'; import ReactDiffViewer from 'react-diff-viewer'; const oldCode = `Hello World`; const newCode = `Hello New World`; export default class App extends PureComponent {   render = () => {     return (        <div         style={{             position: "relative",             width: "600px",             height: "100px",         }}         >       <ReactDiffViewer oldValue={oldCode} newValue={newCode} splitView={true} />       </div>     );   }; } ```  and after some prompting and telling the GPT how an widget is structured, it came up with this js string literal ``oldCode = `${this.props.my_old_code}`;`` syntax, which is working. My question: Is this best practice? ```python import ipyreact from traitlets import Unicode, Bool class DiffViewerWidget(ipyreact.ReactWidget):     my_old_code = Unicode('Hello World').tag(sync=True)     my_new_code = Unicode('Hello New World').tag(sync=True)     split_view = Bool(True).tag(sync=True)          _esm = """     import React, { PureComponent } from 'react';     import ReactDiffViewer from 'react-diff-viewer';     export default class App extends PureComponent {         render = () => {             const oldCode = `${this.props.my_old_code}`;             const newCode = `${this.props.my_new_code}`;             return (                 <div                     style={{                         position: "relative",                         width: "600px",                         height: "100px",                     }}                 >                     <ReactDiffViewer oldValue={oldCode} newValue={newCode} splitView={this.props.split_view} />                 </div>             );         };     }     """      d = DiffViewerWidget(my_old_code='Hello World', my_new_code='Hello New World') d ``` with the GPT explaination: > In this code, my_old_code and my_new_code are traits defined in the Jupyter widget DiffViewerWidget. When the widget is rendered, their values will be available as properties of the props object in the React component.  The template literals use ${} to embed the values of my_old_code and my_new_code inside a string.
ISSUE,  process, [SUGGESTION] Add a feature to redirect ROBLOX to use the UWP version of it. becus yes yes yes (explained by the title) the paragraph below is a generated using chatgpt on why you should add this feature > Adding UWP redirection of ROBLOX can greatly enhance the user experience, increase user engagement and retention, differentiate an app from competitors, and future-proof an app. With so many benefits to adding UWP redirection, it's a smart business decision for anyone looking to improve their ROBLOX app and stand out in a crowded market.
ISSUE,  learning, How to source information from a password manager or fallback to prompt user to type it in if not present ## What exactly are you trying to do? While installing dotfiles I would like to source configuration settings like User Name, User Email and Git Signing Key from a password manager i.e. Bitwarden. However, if Bitwarden is not installed a user needs to be prompted to type these in. Some of my code snippets that work independently ``` [data]   dotfiles.name = "{{ (bitwarden "item" "dotfiles").identity.firstName }} {{ (bitwarden "item" "dotfiles").identity.lastName }}"   dotfiles.email = "{{ (bitwarden "item" "dotfiles").identity.email }}"   dotfiles.git_signingkey = "{{ (bitwardenFields "item" "dotfiles").git_signingkey.value }}" ``` ``` #   {{- $name := promptString "Git user name for the author/committer" }} #   {{- $email := promptString "Git user email for the author/committer" }} #   {{- $signingkey := promptString "Git user signing key for the author/committer" }} ``` But I wasn't able to construct an if statement as no matter how it is implemented it throws an error. ## What have you tried so far? I have tried a number of things within the `.chezmoi.toml.tmpl` file but nothing seems to work. - `if` statement to check presence of `.chezmoi.config.bitwarden.command` using `index`, `with`, `lookup`, `exec` - non of that worked The main problem is that there seems to be no way to implement an `if` statement to make a decision depending on the presence of a key in the  `.chezmoi.` data configuration. The pseudo-code I'm after is ``` # {{ if .chezmoi.config.bitwarden.command is installed }} #   get name using "{{ (bitwarden "item" "dotfiles").identity.firstName }} {{ (bitwarden "item" "dotfiles").identity.lastName }}" # {{ end }} #   {{- $name := promptString "Git user name for the author/committer" }} # {{ end }} ``` ## Where else have you checked for solutions? * [x] I have read [chezmoi's user guide](https://chezmoi.io/user-guide/command-overview/), and not found the answer. * [x] I have searched [chezmoi's reference guide](https://chezmoi.io/reference/), and not found the answer. * [x] Other, please give details. Had a good but not so helpful chat with ChatGPT ## Output of any commands you've tried ```console ‚ûú  chezmoi git:(main) ‚úó echo '{{ .chezmoi.config.bitwarden | toJson }}' | chezmoi execute-template | jq chezmoi: template: stdin:1:17: executing "stdin" at <.chezmoi.config.bitwarden>: map has no entry for key "bitwardend" ``` 
ISSUE,  feature implementation/enhancement, Mask Secrets Callback Plugin ### Summary I would like the ability to define what constitutes as a secret in the stdout and stderr and have the values of those keys masked. A proper implementation of this feature would allow a user to define keys that should have their values encrypted. This would be handy on some modules that are unaware of secret data being passed through the module, such as `ansible.builtin.uri`. I believe that `community.general` would be a prime location for this callback plugin. ### Issue Type Feature Idea ### Component Name new callback plugin ### Additional Information I asked ChatGPT what this potential implementation could look like. This is what was returned. <!--- Paste example playbooks or commands between quotes below --> For this task, we will create a callback plugin called `mask_secrets.py`. ```python from ansible.plugins.callback import CallbackBase import os import re import json class CallbackModule(CallbackBase):     CALLBACK_VERSION = 2.0     CALLBACK_TYPE = 'stdout'     CALLBACK_NAME = 'mask_secrets'     def __init__(self, display=None):         super(CallbackModule, self).__init__(display)         self.masked_keys = os.getenv('MASKED_KEYS', '').split(',')         self.mask = '********'     def _mask_secrets(self, result):         if isinstance(result, (dict, list)):             return self._mask_secrets_in_structure(result)         return result     def _mask_secrets_in_structure(self, struct):         if isinstance(struct, dict):             return {k: self._mask_secrets(v) if k not in self.masked_keys else self.mask for k, v in struct.items()}         elif isinstance(struct, list):             return [self._mask_secrets(v) for v in struct]         return struct     def v2_runner_on_ok(self, result):         result._result = self._mask_secrets(result._result)         super(CallbackModule, self).v2_runner_on_ok(result)     def v2_runner_on_failed(self, result, ignore_errors=False):         result._result = self._mask_secrets(result._result)         super(CallbackModule, self).v2_runner_on_failed(result, ignore_errors)     def v2_playbook_on_notify(self, result, handler):         result._result = self._mask_secrets(result._result)         super(CallbackModule, self).v2_playbook_on_notify(result, handler) ``` Modify the `ansible.cfg`. ``` [defaults] stdout_callback = mask_secrets ``` Test the plugin. ```yaml (paste below) - hosts: localhost   environment:     MASKED_KEYS: "key1,key2,key3"   tasks:     - debug:         msg: "This is a test message with secrets: {{ key1 }}, {{ key2 }}, {{ key3 }}"       vars:         key1: "secret1"         key2: "secret2"         key3: "secret3" ``` I've tested this implementation enough to know that some of the details need to be fleshed out. However, I believe that the bones of the request are in place. I'd love some feedback from the community. If it's suitable, I can flesh out the details and contribute the code to the community. ### Code of Conduct - [X] I agree to follow the Ansible Code of Conduct
ISSUE,  feature implementation/enhancement, Sad to only discover it now I didn't know your project existed so I have started to implement with ChatGPT a client side template engine based on thymeleaf syntax https://github.com/vricosti/thymeleafjs...
ISSUE,  learning, Landingpage not reachable with https:// **Thanks for reaching out! We're happy to help resolve issues as quickly as possible.** **Please use this template when creating a new issue. If you do not follow this template format, your issue may be closed without comment.** Before filing a new issue, please use the search bar at the top of the browser to search for similar issues. Also, please make sure you have read our [documentation](https://getgophish.com/documentation/), which covers how to use Gophish. **What version of Gophish are you using?:**  v0.12.1-linux-64bit **Brief description of the issue:**  I have installed gophish on an Ubuntu Server 20.04 at ionos. I installed it with the help of ChatGPT :-), because it was not running smoothly before. That worked wonderfully. But now I have a problem that I cannot solve. It is possible to access the admin page via https://gophish.xxx.de, this works perfectly. However, I would also like to open the landing page, which opens after clicking on the link in the mail, via https://. But this does not work and I always get the following error message: Error: Secure Connection Failed - Error code: SSL_ERROR_RX_RECORD_TOO_LONG Opening via http://XX.XXX.XXX.142:8080 is not a problem. Is there a way to open the landing page via http**s**://XX.XXX.XXX.142:8080?  I have already tried for hours, even with the help of ChatGPT, but no matter what I try, it does not work. **What are you expecting to see happen? :** I would like to open the landing page via https:// and not http://, because otherwise, for example, I get a message in the browser when I want to enter a password that it is an insecure connection. This could deter users from entering their data. **What are you seeing happen?** I can't open the landing page via https://XX.XXX.XXX.142:8080 and I receive an error message. - Here is the excerpt from the Nginx configuration file that ChatGPT suggested and which works well, except for the https:// connection of the landing page. ``` server {     server_name gophish.xxx.de;     location / {         proxy_pass https://XX.XXX.XXX.142:3343;         proxy_set_header Host $host;         proxy_set_header X-Real-IP $remote_addr;         proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 		     }     listen 443 ssl; # managed by Certbot 	server_name gophish.xxx.de;     ssl_certificate /etc/letsencrypt/live/gophish.xxx.de/fullchain.pem; # managed by Certbot     ssl_certificate_key /etc/letsencrypt/live/gophish.xxx.de/privkey.pem; # managed by Certbot     include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot     ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot } server {     if ($host = gophish.xxx.de) {         return 301 https://$host$request_uri;     } # managed by Certbot     listen 80;     server_name gophish.xxx.de; 	location / {     return 301 https://$host$request_uri; 	return 404; # managed by Certbot     } } ``` During the installation, there were some obstacles to opening the admin interface via an https:// connection and the registered domain gophish.xxx.de. But in the end, everything worked out, except for one remaining problem. I had to change the ports of Gophish to 3343 and 8080 because there were conflicts with Nginx, as Nginx was already occupying those ports somehow. I have allowed all ports in the firewall policy: 80, 443, 3343, 8080. I have obscured the IP address and the domain, I hope that's OK. Thank you for the possible help! 
ISSUE,  learning, windows.svcscan plugin not working  i have my kali linux on aws cloud when i try to run windows.svcscan on cridex.vmem(which is a well known memory dump)  using the  command: vol.py -f cridex.vmem windows.svcscan iam getting the following error: Volatility 3 Framework 2.4.2 usage: volatility [-h] [-c CONFIG] [--parallelism [{processes,threads,off}]] [-e EXTEND] [-p PLUGIN_DIRS] [-s SYMBOL_DIRS] [-v]                   [-l LOG] [-o OUTPUT_DIR] [-q] [-r RENDERER] [-f FILE] [--write-config] [--save-config SAVE_CONFIG]                   [--clear-cache] [--cache-path CACHE_PATH] [--offline] [--single-location SINGLE_LOCATION]                   [--stackers [STACKERS ...]] [--single-swap-locations [SINGLE_SWAP_LOCATIONS ...]]                   plugin ... volatility: error: argument plugin: invalid choice windows.svcscan (choose from banners.Banners, configwriter.ConfigWriter, frameworkinfo.FrameworkInfo, isfinfo.IsfInfo, layerwriter.LayerWriter, linux.bash.Bash, linux.check_afinfo.Check_afinfo, linux.check_creds.Check_creds, linux.check_idt.Check_idt, linux.check_modules.Check_modules, linux.check_syscall.Check_syscall, linux.elfs.Elfs, linux.envars.Envars, linux.envvars.Envvars, linux.iomem.IOMem, linux.keyboard_notifiers.Keyboard_notifiers, linux.kmsg.Kmsg, linux.lsmod.Lsmod, linux.lsof.Lsof, linux.malfind.Malfind, linux.mountinfo.MountInfo, linux.proc.Maps, linux.psaux.PsAux, linux.pslist.PsList, linux.psscan.PsScan, linux.pstree.PsTree, linux.sockstat.Sockstat, linux.tty_check.tty_check, mac.bash.Bash, mac.check_syscall.Check_syscall, mac.check_sysctl.Check_sysctl, mac.check_trap_table.Check_trap_table, mac.ifconfig.Ifconfig, mac.kauth_listeners.Kauth_listeners, mac.kauth_scopes.Kauth_scopes, mac.kevents.Kevents, mac.list_files.List_Files, mac.lsmod.Lsmod, mac.lsof.Lsof, mac.malfind.Malfind, mac.mount.Mount, mac.netstat.Netstat, mac.proc_maps.Maps, mac.psaux.Psaux, mac.pslist.PsList, mac.pstree.PsTree, mac.socket_filters.Socket_filters, mac.timers.Timers, mac.trustedbsd.Trustedbsd, mac.vfsevents.VFSevents, timeliner.Timeliner, windows.bigpools.BigPools, windows.cachedump.Cachedump, windows.callbacks.Callbacks, windows.cmdline.CmdLine, windows.crashinfo.Crashinfo, windows.devicetree.DeviceTree, windows.dlllist.DllList, windows.driverirp.DriverIrp, windows.drivermodule.DriverModule, windows.driverscan.DriverScan, windows.dumpfiles.DumpFiles, windows.envars.Envars, windows.filescan.FileScan, windows.getservicesids.GetServiceSIDs, windows.getsids.GetSIDs, windows.handles.Handles, windows.hashdump.Hashdump, windows.info.Info, windows.joblinks.JobLinks, windows.ldrmodules.LdrModules, windows.lsadump.Lsadump, windows.malfind.Malfind, windows.mbrscan.MBRScan, windows.memmap.Memmap, windows.modscan.ModScan, windows.modules.Modules, windows.mutantscan.MutantScan, windows.netscan.NetScan, windows.netstat.NetStat, windows.poolscanner.PoolScanner, windows.privileges.Privs, windows.pslist.PsList, windows.psscan.PsScan, windows.pstree.PsTree, windows.registry.certificates.Certificates, windows.registry.hivelist.HiveList, windows.registry.hivescan.HiveScan, windows.registry.printkey.PrintKey, windows.registry.userassist.UserAssist, windows.sessions.Sessions, windows.skeleton_key_check.Skeleton_Key_Check, windows.ssdt.SSDT, windows.statistics.Statistics, windows.strings.Strings, windows.symlinkscan.SymlinkScan, windows.vadinfo.VadInfo, windows.vadwalk.VadWalk, windows.verinfo.VerInfo, windows.virtmap.VirtMap) so i tried this command suggested by chatgpt  to know if any plugins failed to load.. command: vol.py -vvv --help i got the following plugins which could not be loaded: The following plugins could not be loaded (use -vv to see why): volatility3.plugins.windows.mftscan, volatility3.plugins.windows.svcscan, volatility3.plugins.windows.vadyarascan, volatility3.plugins.yarascan to get a more verbose output on error i used -vv command: vol.py -f cridex.vmem -vv and got the following error: Volatility 3 Framework 2.4.2 INFO     volatility3.cli: Volatility plugins path: ['/home/kali/volatility3/volatility3/plugins', '/home/kali/volatility3/volatility3/framework/plugins'] INFO     volatility3.cli: Volatility symbols path: ['/home/kali/volatility3/volatility3/symbols', '/home/kali/volatility3/volatility3/framework/symbols'] INFO     volatility3.plugins.yarascan: Python Yara (>3.8.0) module not found, plugin (and dependent plugins) not available DEBUG    volatility3.framework: Traceback (most recent call last):   File "/home/kali/volatility3/volatility3/framework/__init__.py", line 185, in import_file     importlib.import_module(module)   File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module     return _bootstrap._gcd_import(name[level:], package, level)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "<frozen importlib._bootstrap>", line 1206, in _gcd_import   File "<frozen importlib._bootstrap>", line 1178, in _find_and_load   File "<frozen importlib._bootstrap>", line 1149, in _find_and_load_unlocked   File "<frozen importlib._bootstrap>", line 690, in _load_unlocked   File "<frozen importlib._bootstrap_external>", line 940, in exec_module   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed   File "/home/kali/volatility3/volatility3/framework/plugins/yarascan.py", line 20, in <module>     raise ImportError ImportError DEBUG    volatility3.framework: Failed to import module volatility3.plugins.yarascan based on file: /home/kali/volatility3/volatility3/framework/plugins/yarascan.py INFO     volatility3.plugins.yarascan: Python Yara (>3.8.0) module not found, plugin (and dependent plugins) not available DEBUG    volatility3.framework: Traceback (most recent call last):   File "/home/kali/volatility3/volatility3/framework/__init__.py", line 185, in import_file     importlib.import_module(module)   File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module     return _bootstrap._gcd_import(name[level:], package, level)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "<frozen importlib._bootstrap>", line 1206, in _gcd_import   File "<frozen importlib._bootstrap>", line 1178, in _find_and_load   File "<frozen importlib._bootstrap>", line 1149, in _find_and_load_unlocked   File "<frozen importlib._bootstrap>", line 690, in _load_unlocked   File "<frozen importlib._bootstrap_external>", line 940, in exec_module   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed   File "/home/kali/volatility3/volatility3/framework/plugins/windows/vadyarascan.py", line 11, in <module>     from volatility3.plugins import yarascan   File "/home/kali/volatility3/volatility3/framework/plugins/yarascan.py", line 20, in <module>     raise ImportError ImportError DEBUG    volatility3.framework: Failed to import module volatility3.plugins.windows.vadyarascan based on file: /home/kali/volatility3/volatility3/framework/plugins/windows/vadyarascan.py INFO     volatility3.plugins.yarascan: Python Yara (>3.8.0) module not found, plugin (and dependent plugins) not available DEBUG    volatility3.framework: Traceback (most recent call last):   File "/home/kali/volatility3/volatility3/framework/__init__.py", line 185, in import_file     importlib.import_module(module)   File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module     return _bootstrap._gcd_import(name[level:], package, level)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "<frozen importlib._bootstrap>", line 1206, in _gcd_import   File "<frozen importlib._bootstrap>", line 1178, in _find_and_load   File "<frozen importlib._bootstrap>", line 1149, in _find_and_load_unlocked   File "<frozen importlib._bootstrap>", line 690, in _load_unlocked   File "<frozen importlib._bootstrap_external>", line 940, in exec_module   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed   File "/home/kali/volatility3/volatility3/framework/plugins/windows/mftscan.py", line 13, in <module>     from volatility3.plugins import timeliner, yarascan   File "/home/kali/volatility3/volatility3/framework/plugins/yarascan.py", line 20, in <module>     raise ImportError ImportError DEBUG    volatility3.framework: Failed to import module volatility3.plugins.windows.mftscan based on file: /home/kali/volatility3/volatility3/framework/plugins/windows/mftscan.py INFO     volatility3.plugins.yarascan: Python Yara (>3.8.0) module not found, plugin (and dependent plugins) not available DEBUG    volatility3.framework: Traceback (most recent call last):   File "/home/kali/volatility3/volatility3/framework/__init__.py", line 185, in import_file     importlib.import_module(module)   File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module     return _bootstrap._gcd_import(name[level:], package, level)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "<frozen importlib._bootstrap>", line 1206, in _gcd_import   File "<frozen importlib._bootstrap>", line 1178, in _find_and_load   File "<frozen importlib._bootstrap>", line 1149, in _find_and_load_unlocked   File "<frozen importlib._bootstrap>", line 690, in _load_unlocked   File "<frozen importlib._bootstrap_external>", line 940, in exec_module   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed   File "/home/kali/volatility3/volatility3/framework/plugins/windows/svcscan.py", line 16, in <module>     from volatility3.plugins.windows import poolscanner, vadyarascan, pslist   File "/home/kali/volatility3/volatility3/framework/plugins/windows/vadyarascan.py", line 11, in <module>     from volatility3.plugins import yarascan   File "/home/kali/volatility3/volatility3/framework/plugins/yarascan.py", line 20, in <module>     raise ImportError ImportError DEBUG    volatility3.framework: Failed to import module volatility3.plugins.windows.svcscan based on file: /home/kali/volatility3/volatility3/framework/plugins/windows/svcscan.py INFO     volatility3.cli: The following plugins could not be loaded (use -vv to see why): volatility3.plugins.windows.mftscan, volatility3.plugins.windows.svcscan, volatility3.plugins.windows.vadyarascan, volatility3.plugins.yarascan usage: volatility [-h] [-c CONFIG] [--parallelism [{processes,threads,off}]] [-e EXTEND] [-p PLUGIN_DIRS] [-s SYMBOL_DIRS] [-v]                   [-l LOG] [-o OUTPUT_DIR] [-q] [-r RENDERER] [-f FILE] [--write-config] [--save-config SAVE_CONFIG]                   [--clear-cache] [--cache-path CACHE_PATH] [--offline] [--single-location SINGLE_LOCATION]                   [--stackers [STACKERS ...]] [--single-swap-locations [SINGLE_SWAP_LOCATIONS ...]]                   plugin ... volatility: error: argument plugin: invalid choice windows.svcscan (choose from banners.Banners, configwriter.ConfigWriter, frameworkinfo.FrameworkInfo, isfinfo.IsfInfo, layerwriter.LayerWriter, linux.bash.Bash, linux.check_afinfo.Check_afinfo, linux.check_creds.Check_creds, linux.check_idt.Check_idt, linux.check_modules.Check_modules, linux.check_syscall.Check_syscall, linux.elfs.Elfs, linux.envars.Envars, linux.envvars.Envvars, linux.iomem.IOMem, linux.keyboard_notifiers.Keyboard_notifiers, linux.kmsg.Kmsg, linux.lsmod.Lsmod, linux.lsof.Lsof, linux.malfind.Malfind, linux.mountinfo.MountInfo, linux.proc.Maps, linux.psaux.PsAux, linux.pslist.PsList, linux.psscan.PsScan, linux.pstree.PsTree, linux.sockstat.Sockstat, linux.tty_check.tty_check, mac.bash.Bash, mac.check_syscall.Check_syscall, mac.check_sysctl.Check_sysctl, mac.check_trap_table.Check_trap_table, mac.ifconfig.Ifconfig, mac.kauth_listeners.Kauth_listeners, mac.kauth_scopes.Kauth_scopes, mac.kevents.Kevents, mac.list_files.List_Files, mac.lsmod.Lsmod, mac.lsof.Lsof, mac.malfind.Malfind, mac.mount.Mount, mac.netstat.Netstat, mac.proc_maps.Maps, mac.psaux.Psaux, mac.pslist.PsList, mac.pstree.PsTree, mac.socket_filters.Socket_filters, mac.timers.Timers, mac.trustedbsd.Trustedbsd, mac.vfsevents.VFSevents, timeliner.Timeliner, windows.bigpools.BigPools, windows.cachedump.Cachedump, windows.callbacks.Callbacks, windows.cmdline.CmdLine, windows.crashinfo.Crashinfo, windows.devicetree.DeviceTree, windows.dlllist.DllList, windows.driverirp.DriverIrp, windows.drivermodule.DriverModule, windows.driverscan.DriverScan, windows.dumpfiles.DumpFiles, windows.envars.Envars, windows.filescan.FileScan, windows.getservicesids.GetServiceSIDs, windows.getsids.GetSIDs, windows.handles.Handles, windows.hashdump.Hashdump, windows.info.Info, windows.joblinks.JobLinks, windows.ldrmodules.LdrModules, windows.lsadump.Lsadump, windows.malfind.Malfind, windows.mbrscan.MBRScan, windows.memmap.Memmap, windows.modscan.ModScan, windows.modules.Modules, windows.mutantscan.MutantScan, windows.netscan.NetScan, windows.netstat.NetStat, windows.poolscanner.PoolScanner, windows.privileges.Privs, windows.pslist.PsList, windows.psscan.PsScan, windows.pstree.PsTree, windows.registry.certificates.Certificates, windows.registry.hivelist.HiveList, windows.registry.hivescan.HiveScan, windows.registry.printkey.PrintKey, windows.registry.userassist.UserAssist, windows.sessions.Sessions, windows.skeleton_key_check.Skeleton_Key_Check, windows.ssdt.SSDT, windows.statistics.Statistics, windows.strings.Strings, windows.symlinkscan.SymlinkScan, windows.vadinfo.VadInfo, windows.vadwalk.VadWalk, windows.verinfo.VerInfo, windows.virtmap.VirtMap) mostly mentioned about python Yara (>3.8.0) module not found but yara-python is installed in my kali as iam getting the ffollwoing output while i run "pip show yara-python" outptut: Name: yara-python Version: 4.3.0 Summary: Python interface for YARA Home-page: https://github.com/VirusTotal/yara-python Author: Victor M. Alvarez Author-email: plusvic@gmail.com, vmalvarez@virustotal.com License: Apache 2.0 Location: /usr/local/lib/python3.11/dist-packages/yara_python-4.3.0-py3.11-linux-x86_64.egg Requires: Required-by: i even installed the requirements.txt using pip install -r reuirements.txt, but still could not resolve the error and found all that all the requirements are installed using "pip list" output: Package             Version ------------------- ---------- attrs               22.2.0 awscli              2.9.19 awscrt              1.0.0.dev0 blinker             1.5 boto                2.49.0 capstone            4.0.2 certifi             2022.9.24 chardet             5.1.0 charset-normalizer  3.0.1 cloud-init          22.4.2 colorama            0.4.6 configobj           5.0.8 cryptography        38.0.4 cupshelpers         1.0 dbus-python         1.3.2 distro              1.8.0 distro-info         1.5 docutils            0.19 httplib2            0.20.4 idna                3.3 Jinja2              3.0.3 jmespath            1.0.1 jsonpatch           1.32 jsonpointer         2.3 jsonschema          4.10.3 kali-tweaks         2023.1.5 lazr.restfulclient  0.14.5 lazr.uri            1.0.6 leechcorepyc        2.14.4 MarkupSafe          2.1.2 netifaces           0.11.0 oauthlib            3.2.2 pefile              2023.2.7 pip                 23.0.1 prompt-toolkit      3.0.36 pyasn1              0.4.8 pycairo             1.20.1 pycryptodome        3.17 pycups              2.0.1 PyGObject           3.42.2 PyJWT               2.6.0 pyparsing           3.0.9 pyrsistent          0.18.1 pyserial            3.5 pysmbc              1.0.23 python-apt          2.5.3 python-dateutil     2.8.2 pyxdg               0.28 PyYAML              6.0 requests            2.28.1 roman               3.3 ruamel.yaml         0.17.21 ruamel.yaml.clib    0.2.7 setuptools          66.1.1 six                 1.16.0 unattended-upgrades 0.1 urllib3             1.26.12 volatility3         2.4.1 wadllib             1.3.6 wcwidth             0.2.5 wheel               0.38.4 xdg                 5 yara                1.7.7 yara-python         4.3.0 yara-python         4.3.0 
ISSUE,  feature implementation/enhancement, feature request: build endpoint from sockaddr* I've been converting UNIX code to ASIO C++, and one of the most significant friction point is that there seem to be no simple way to build `asio::ip::udp::endpoint` or `asio::ip::tcp::endpoint` from the standard `struct sockaddr*.` This is a pain-point when interfacing ASIO code with C libraries or other parts of the code that use struct sockaddr*. I've been generating the following conversion routine with ChatGPT (see bellow), but it seems bug-prone and inefficient. It seems like the standard struct sockaddr* is supported on every platform supported by ASIO ? So it might be possible to have conversion routines from struct sockaddr* part of ASIO ? Or is it something that already exists and that I've been missing ? Many thanks. ```c++ asio::ip::udp::endpoint sockaddr_to_endpoint(struct sockaddr* addr) {     if (addr->sa_family == AF_INET) {         auto addr_in = reinterpret_cast<struct sockaddr_in*>(addr);         asio::ip::address_v4 address(ntohl(addr_in->sin_addr.s_addr));         unsigned short port = ntohs(addr_in->sin_port);         return asio::ip::udp::endpoint(address, port);     } else if (addr->sa_family == AF_INET6) {         auto addr_in6 = reinterpret_cast<struct sockaddr_in6*>(addr);         asio::ip::address_v6::bytes_type bytes;         std::memcpy(bytes.data(), &addr_in6->sin6_addr, 16);         asio::ip::address_v6 address(bytes, addr_in6->sin6_scope_id);         unsigned short port = ntohs(addr_in6->sin6_port);         return asio::ip::udp::endpoint(address, port);     } else {         throw std::runtime_error("Invalid sockaddr family");     } } ``` 
ISSUE,  software quality, as a developer I want to split the code into v2 and v3 in different directories as proposed by chatgpt so I can serve multiple versions simultaneously form the same AWS instance ![image](https://github.com/internetarchive/iari/assets/68460690/a46a2456-798d-49f5-a22d-0102511178a0) 
ISSUE,  learning, Transmit Text File using libhackrf in C/C++ ### What would you like to know? All the sample code provided by ChatGPT says to use hackrf_send but this method does not exist in hackrf.h. What is the minimum C or C++ code necessary to transmit and receive a small text file? Could you provide an example, perhaps named as transmit.cpp and receive.cpp?
ISSUE,  learning, Cannot send media ### Is there an existing issue for this? - [X] I have searched the existing issues ### Describe the bug I want to send a local image (converted to base64) through the whatsapp-web.js library (version 1.19.5) with node version v19.5.0 . I have searched on the Internet (including with chatGPT assistance) for example and tried all possible combinations but it still failed. My script is working fine for sending plain text message. Can anybody please lead me to the right way to do it? Thanks. ### Expected behavior Expecting script to send an image of the base64 converted text, but nothing happens. It seems that it all treat it as a chat message and never send anything out to the recipient. The result is the same when, as suggested from Internet, I change from: var media=new MessageMedia('image/png', b64img,'table.png'); to: var media = await MessageMedia.fromUrl('https://colouredlab.com/assets/coloured-lab-symbol-white.png'); ### Steps to Reproduce the Bug or Issue 1. On whatsapp client connection ready, call sendImage() function 2. nothing is sent out and on console.log(res) dump, will see:     Message {   _data: {     id: {       fromMe: true,       remote: [Object],       id: '3EB0F756D3E7DCFD26B20F',       _serialized: 'true_123456789@c.us_3EB0F756D3E7DCFD26B20F'     },     body: {       mimetype: 'image/png',       data: ....           filename: 'table.png',     },     **type: 'chat',**    .... mediaKey: undefined,   id: {     fromMe: true,     remote: {       server: 'c.us',       user: '123456789',       _serialized: '123456789@c.us'     },     id: '3EB042D81B655BCE33B110',     _serialized: 'true_123456789@c.us_3EB042D81B655BCE33B110'   },   ack: 0,   **hasMedia: false,**   body: {     mimetype: 'image/png',     data:       ### Relevant Code const fs=require('fs'); const { MessageMedia } = require('whatsapp-web.js'); const { Client, Location, List, Buttons, LocalAuth } = require('./whatsapp-web.js/index'); whatsappClient = new Client({ authStrategy: new LocalAuth(), puppeteer: { headless: false } }); whatsappClient.initialize(); async function sendImage(client, phone, ack ="") { const b64img=fs.readFileSync('tablepng-base64.txt','utf8'); await client.sendMessage(phone,'plain text message is working fine'); var media=new MessageMedia('image/png', b64img,'table.png'); return await client.sendMessage(phone,media,{caption:"Please send it!"}); } whatsappClient.on('ready', () => {     console.log('Client is ready!');     const toPhone = "123456789@c.us" // to phone number     const ack = "can you see it?"    // dunno, why console.log of next statement (on then case) always says of type: 'chat' and not type: 'image' and never send an image out??     sendImage(whatsappClient, toPhone,'part 2').then((res)=> { console.log('done!'); console.log(res); }).catch((error) => console.log("Can not send message.", error)); }); ### Browser Type Chromium ### WhatsApp Account Type Standard ### Does your WhatsApp account have multidevice enabled? No, I am not using Multi Device ### Environment OS: Macos Ventura 13.3.1(a) Phone OS: Android Whatsapp-web.js version 1.19.5 WhatApp web version: 2.2320.10 node.js version: v19.5.0 ### Additional context _No response_
ISSUE,  feature implementation/enhancement, Streaming from my cell phone camera to a rtmp server Hi I am new to stream development. I ask chatGPT to provide me with some code in python for streaming from my cell phone camera to a rtmp server. The RTMP server is working fine. I have tested with OBS and VLC Media Player. When I executed this code on Jupyter Notebook I got this error below Can someone help figure out what is wrong? Regards A Sokone. ```python ------------------------- PYTHON CODE START HERE import cv2 import gi # ---- I added these lines import ctypes import numpy as np import cairo # ---- End of added lines gi.require_version('Gst', '1.0') from gi.repository import Gst # Initialize GStreamer Gst.init(None) # Define the RTMP server URL rtmp_server_url = 'rtmp://192.168.0.30/live' # Set up the GStreamer pipeline pipeline = Gst.Pipeline.new() # Create the elements for video capture and encoding src = Gst.ElementFactory.make("appsrc", "source") caps = Gst.Caps.from_string("video/x-raw,format=BGR") filter = Gst.ElementFactory.make("capsfilter", "filter") filter.set_property("caps", caps) video_convert = Gst.ElementFactory.make("videoconvert", "video_convert") x264enc = Gst.ElementFactory.make("x264enc", "x264enc") mux = Gst.ElementFactory.make("flvmux", "mux") sink = Gst.ElementFactory.make("rtmpsink", "sink") sink.set_property("location", rtmp_server_url) # Add elements to the pipeline for element in [src, filter, video_convert, x264enc, mux, sink]:     pipeline.add(element) # Link elements in the pipeline src.link(filter) filter.link(video_convert) video_convert.link(x264enc) x264enc.link(mux) mux.link(sink) # Start the pipeline pipeline.set_state(Gst.State.PLAYING) # Open the video capture device cap = cv2.VideoCapture(0) while True:     ret, frame = cap.read()     if not ret:         break          # Convert frame to Cairo surface     height, width, _ = frame.shape     stride = width * 1  # Assuming 3 channels (RGB)          surface = cairo.ImageSurface.create_for_data(frame.data, cairo.FORMAT_RGB24, width, height)          print("surface.get_data", surface.get_data())          # Push frame to the GStreamer pipeline     buf = Gst.Buffer.new_wrapped(surface.get_data())          print("buf", buf)     buf.pts = buf.dts = Gst.CLOCK_TIME_NONE     buf.duration = 1     src.emit("push-buffer", buf) # Clean up cap.release() pipeline.set_state(Gst.State.NULL) ``` --------------------------- End of Python Code Error below --------------------------------------------------------------------------- TypeError                                 Traceback (most recent call last) Cell In[1], line 65      61 print ("height", height, "width", width, "stride", stride)      63 #surface = cairo.ImageSurface.create_for_data(frame.data, cairo.FORMAT_RGB24, width, height, stride) ---> 65 surface = cairo.ImageSurface.create_for_data(frame.data, cairo.FORMAT_RGB24, width, height)      67 print("surface.get_data", surface.get_data())      69 # Push frame to the GStreamer pipeline TypeError: buffer is not long enough 
ISSUE,  software quality, IfEquals Unexpected token, expected "?" <!-- Thanks for reporting an issue! Please fill out the blanks below. --> ## What are the steps to reproduce this issue? 1. Generate your client 2. When using the client with Typescript 5.x the error occurs when building ## What happens? <img width="726" alt="Screenshot 2023-05-21 at 13 05 57" src="https://github.com/anymaniax/orval/assets/5333875/0780fe7f-8fed-460b-a082-70615ec607f8"> This pops up when building my Next.js project that uses an Orval generated client.  ## What were you expecting to happen? There shouldn't be an error.  ## Any other comments? I asked ChatGPT about this and here's the response ` The error you're encountering in TypeScript 5 is related to a syntax change introduced in TypeScript 4.1. The error message suggests that the "extends" keyword is not being recognized in the position where it appears. This issue is caused by a limitation in TypeScript 4.0 and earlier versions, where conditional types (the construct with the extends keyword) could not be used in arrow function return types. ` Furthermore, it suggests the following replacement ```ts type IfEquals<X, Y, A = X, B = never> = [X] extends [Y] ? ([Y] extends [X] ? A : B) : B; ``` When I manually change the generated output to match that, the error disappears. I'm not a Typescript wizard myself so I'm not quite sure if this code is equivalent. If it is and you guys want me to open a PR, let me know.  ## What versions are you using? **Operating System:** macOS Ventura 13.0 **Package Version:** 6.15.0 **Browser Version:** Not running on browser yet.  
ISSUE,  generating/manipulating data, Use large language models (LLM) to perform ingredient list spellcheck ### Problem Numerous quality errors about ingredient lists come from spelling errors. It's mostly due to errors during the OCR process, because the image is blurry or because of OCR model limitation (we use Google Cloud Vision). As a result, we have: 1. ingredients with spelling mistakes 2. ingredients not separated with comma (or other ingredient list separator), resulting in "unknown ingredient" warning 3. incorrect line continuation (the way Google Cloud Vision joins words to makes paragraphs): the ingredient list has unrelated words inside it. I think (1) and (2) can be corrected using language models for spelling correction, (3) is more tricky. We implemented a spellcheck module in Robotoff using Elasticsearch, but it's currently not good enough to be used without human supervision: it's currently unused and will be removed soon from the codebase. ### Proposed solution Explore the use of large language models for performing ingredient spellcheck. We must ensure the model does not hallucinate new ingredients or modify ingredients that were already valid. ChatGPT (GPT-3.5) seems a good starter. If it works correctly, we can try to generate a high quality spellcheck dataset using chatGPT (a dataset mapping text to correct into corrected text), and fine-tune an open source large language model we can host on our servers to replicate this feature. ### Where to get the data? The best way to get a list of products with ingredient list with error is to get the [Open Food Facts JSONL dataset](https://world.openfoodfacts.org/data), and look for products with ingredient quality warnings. The data quality warnings tags are available in the `data_quality_warnings_tags` field. Relevant tags for spotting ingredient lists with errors: - `en:ingredients-unknown-score-above-0` - `en:ingredients-50-percent-unknown` - `en:ingredients-60-percent-unknown` - `en:ingredients-70-percent-unknown` - `en:ingredients-80-percent-unknown` - `en:ingredients-90-percent-unknown` - ... ### Additional resources Wiki page about [ingredient data quality](https://wiki.openfoodfacts.org/Ingredients_Analysis_Quality). You can test if the corrected text is well-recognized by Open Food Facts server by using this link: https://world.openfoodfacts.org/cgi/test_ingredients_analysis.pl?lc=it Note that the `lc=fr` parameter is used to provide the language of the ingredient list, that is used to parse the ingredient list. If there are some unknown ingredients, it does not necessarily mean there is a spelling error, as some ingredients are not recognized, as they are not in our ingredient taxonomy. Ingredient coverage depends on the language (good for English and French, bad for low-resources languages). ### Part of - https://github.com/openfoodfacts/openfoodfacts-ai/issues/289
ISSUE,  learning, Unable to download from link XML/ Access Denied oops
ISSUE,  development environment, Okta Provider expected 200 got 401 ### Question üí¨ Hey guys, thanks for reviewing my question. In short, when I use an Okta developer account, NextAuth works fine. When I switch to my production org, however, I am getting this error back. `[next-auth][error][SIGNIN_OAUTH_ERROR]  https://next-auth.js.org/errors#signin_oauth_error expected 200 OK, got: 401 Unauthorized {   error: {     message: 'expected 200 OK, got: 401 Unauthorized',     stack: 'OPError: expected 200 OK, got: 401 Unauthorized\n' +       '    at processResponse (/var/task/node_modules/openid-client/lib/helpers/process_response.js:41:11)\n' +       '    at Issuer.discover (/var/task/node_modules/openid-client/lib/issuer.js:152:20)\n' +       '    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n' +       '    at async openidClient (/var/task/node_modules/next-auth/core/lib/oauth/client.js:16:14)\n' +       '    at async getAuthorizationUrl (/var/task/node_modules/next-auth/core/lib/oauth/authorization-url.js:70:18)\n' +       '    at async Object.signin (/var/task/node_modules/next-auth/core/routes/signin.js:38:24)\n' +       '    at async AuthHandler (/var/task/node_modules/next-auth/core/index.js:260:26)\n' +       '    at async NextAuthApiHandler (/var/task/node_modules/next-auth/next/index.js:22:19)\n' +       '    at async NextAuth._args$ (/var/task/node_modules/next-auth/next/index.js:106:14)',     name: 'OPError'   },   providerId: 'okta',   message: 'expected 200 OK, got: 401 Unauthorized' }` About the code: I used the create t3 app to initialize the code. I modified it for Okta and tested using a developer org. All of that was gravy baby, yet I'm having issues when using a production org's credentials. I'm thinking some kind of access issue but not sure what could cause this. I had a lengthy discussion with ChatGPT in which i double checked my Client Id, Client Secret, Okta Domain, scopes, trusted origin, and CORS settings. Things appear to be configured correctly, however I am still getting this error. I consulted the docs and saw that this error is usually a misconfiguration (https://next-auth.js.org/errors). Are there any considerations for Production Okta orgs compared to developer instances? Is any additional configuration needed within NextAuth in order to work with a production Okta Provider? Thanks, Paul ### How to reproduce ‚òïÔ∏è Here is my auth.ts file ```import { type GetServerSidePropsContext } from "next"; import {   getServerSession,   type NextAuthOptions,   type DefaultSession, } from "next-auth"; import Okta from "next-auth/providers/okta"; import { env } from "~/env.mjs"; /**  * Module augmentation for `next-auth` types. Allows us to add custom properties to the `session`  * object and keep type safety.  *  * @see https://next-auth.js.org/getting-started/typescript#module-augmentation  */ declare module "next-auth" {   interface Session extends DefaultSession {     user: {       id: string;       // ...other properties       // role: UserRole;     } & DefaultSession["user"];   }   // interface User {   //   // ...other properties   //   // role: UserRole;   // } } /**  * Options for NextAuth.js used to configure adapters, providers, callbacks, etc.  *  * @see https://next-auth.js.org/configuration/options  */ export const authOptions: NextAuthOptions = {   debug: true,   callbacks: {     session: ({ session, token }) => ({       ...session,       user: {         ...session.user,         id: token.sub,       },     }),   },   providers: [     Okta({       clientId: env.OKTA_CLIENT_ID,       clientSecret: env.OKTA_CLIENT_SECRET,       issuer: env.OKTA_DOMAIN,     }),     /**      * @see https://next-auth.js.org/providers/okta      */   ], }; /**  * Wrapper for `getServerSession` so that you don't need to import the `authOptions` in every file.  *  * @see https://next-auth.js.org/configuration/nextjs  */ export const getServerAuthSession = (ctx: {   req: GetServerSidePropsContext["req"];   res: GetServerSidePropsContext["res"]; }) => {   return getServerSession(ctx.req, ctx.res, authOptions); }; ``` ### Contributing üôåüèΩ Yes, I am willing to help answer this question in a PR
ISSUE,  feature implementation/enhancement, Enable analyzers that work on "available" resources: in other words those not already reserved **Describe the rationale for the suggested feature.** I'd like to be able to include logic around "available" resources on a node in when writing analyzers that deal with node resources. This will help me close in on whether my Kubernetes will be able to schedule my pod before I attempt my install, assuming I align my check with my resource requests. **Describe the feature** With this feature implemented, I'd be able to write a preflight that looks like this: ```         - nodeResources:             checkName: Are sufficient CPU resources available in the cluster             outcomes:               - fail:                   when: "min(cpuAvailable) < 250m"                   message: Your cluster currently has too few CPU resources available to install Gitea               - pass:                   message: Your cluster has sufficient CPU resources available to install Gitea         - nodeResources:             checkName: Is sufficient memory available in the cluster             outcomes:               - fail:                   when: "min(memoryAvailable) < 256Mi"                    message: Your cluster currently has too little memory available to install Gitea               - pass:                   message: Your cluster has sufficient memory available to install Gitea  ```     and fail the install if my resource requests could not be fulfilled on any node (or any node that I've filtered into my analyzer). `kubectl describe node` provides insight into these values, but they are not available as part of the `status` of the node so just getting the node doesn't show them. **Describe alternatives you've considered** Describe alternative solutions here. Include any workarounds you've considered. **Additional context** It seems like the best way to handle this is to collect all the resource requests for the pods running on the node and subtract that from the allocatable resources on that node. Based on the order of the `kubectl describe node` output I'd bet that's what it is doing, though I haven't read through the code to check.  I also asked ChatGPT to write me a `kubectl` plugin to calculate this to see what the code might look like, I'm attaching it for fun and reference. [kubectl-available-plugin.tar.gz](https://github.com/replicatedhq/troubleshoot/files/11578168/kubectl-available-plugin.tar.gz) @chris-sanders, @diamonwiggins, and I chatted about this on Slack.
ISSUE,  feature implementation/enhancement, Documentation: Example for pure Home Assistant MQTT Zeroexport / Nulleinspeisung with Battery & AHOY-DTU Automation Hello, I tried to find a pure and simple Home Assistant only solution for a Zero Export / Nulleinspeisung automation without NodeRed and python scripts, additional RPIs, tools etc to simply work directly with HA & ahoy-du via MQTT and didn't find any matching my requirements. So I created my own with some support from ChatGPT to enhance it further and would like to share it here so others can use it as a starting point. You need: - A HA sensor state which holds your current total household/grid power usage where negative values mean export to grid and positive import from grid. I have an extra sensor for each (positive) - you can replace/calculate household power to your needs in the automation _curGridUsage_. (I have Shelly 3EM and also Tibber Pulse) - Power switch(_switch.switchonoff_inverter_AC_battery_to_house_) with Sensor (_sensor.power_battery_to_house_) for your Homilies Inverter AC output that you are using for unloading your battery. Automation can work without and would use/can use previous battery power setting if actual power not available alternatively. Might need to disable condition if no switch available.(I have Shelly Plug flashed and power calibrated with Tasmota) - a new input_number and input_boolean for control of the zero export in your _configuration.yaml_. Not using any initial value means last value is saved between HA restarts. Can adjust min/max/step on slider which is holding/showing the calculated automation value or can be manually set when _battery_nulleinspeisung_ is off: ``` input_number:   battery_limit:     name: Battery Nulleinspeisung Limit     min: 0     max: 120     step: 1     mode: slider     # initial: 0 input_boolean:   battery_nulleinspeisung:     name: Battery Nulleinspeisung     icon: mdi:battery-heart-variant     # initial: false ``` - Automation. You need to adjust the MQTT Topics to match your ahoy-dtu inverter target. Can set min, max battery power, hysteresis. If _input_boolean.battery_nulleinspeisung_  is flipped off, you can use the slider for _input_number.battery_limit_ to manually set the battery limit and skip the calculation. Inverter target will be turned off/on dynamically with the first MQTT publish when lower than 7W which you can adjust there of course in the template. My HM-400 does not follow any limit values below 8W so switching off is better for my battery for "idle" power saving (Bluetti EB70 only). Also HM400 stays in "hot-standby" and is turned on and power-set  directly when MQTT limit is calculated for higher again or manually set  ``` alias: AHOY-DTU Battery Nulleinspeisung mit Hysterese description: >-   H√§lt den Battery-Power-Limit f√ºr die Nulleinspeisung durch die Batterie   aufrecht. trigger:   - platform: time_pattern     seconds: /20 condition:   - condition: state     entity_id: switch.switchonoff_inverter_AC_battery_to_house     state: "on"     enabled: true action:   - service: input_number.set_value     target:       entity_id: input_number.battery_limit     data_template:       value: >-         {% set minBatteryPower = 0 %}         {% set maxBatteryPower = 75 %}         {% set hysteresis = 5 %}         {% set oldBatteryLimit = states('input_number.battery_limit') | float %}         {% set newBatteryLimit = oldBatteryLimit %}         {% set curGridUsage = states('sensor.power_import') | int - states('sensor.power_export') | int %}         {% set curBatteryPower = states('sensor.power_battery_to_house') | int(oldBatteryLimit)  %}         {% set powerFlow = curGridUsage + curBatteryPower %}         {% if is_number(powerFlow) and is_state('input_boolean.battery_nulleinspeisung','on') %}           {% if powerFlow >= maxBatteryPower and oldBatteryLimit != maxBatteryPower %}             {% set newBatteryLimit = maxBatteryPower %}           {% elif powerFlow <= maxBatteryPower and powerFlow + hysteresis < oldBatteryLimit %}             {% set newBatteryLimit = powerFlow + hysteresis %}           {% elif powerFlow <= maxBatteryPower and powerFlow - hysteresis > oldBatteryLimit %}             {% set newBatteryLimit = powerFlow - hysteresis %}           {% endif %}         {% endif %}         {% if newBatteryLimit <= minBatteryPower %}           {% set newBatteryLimit = minBatteryPower %}         {% endif %}         {{ newBatteryLimit }}   - service: mqtt.publish     data:       topic: inverter/ctrl/power/0       payload_template: |-         {% if states('input_number.battery_limit') | int < 7 %}           0         {% else %}           1         {% endif %}       retain: true   - service: mqtt.publish     data:       topic: inverter/ctrl/limit/0       payload_template: "{{ states('input_number.battery_limit') |round }}W" mode: single ``` Hints: -  ahoy-dtu does not seem to read the retained power on/off state when subscribing to the MQTT broker/topic after a reboot from my experience. I suggest you set some lower persistent limit on your battery inverter directly via ahoy GUI. - You can switch off the input_boolean and set battery_limit to 0 (so this automation will send an inverter off) before powering off the Inverter AC smart plug after a delay is a great other automation idea if your RCD/FI doesn't like full-load cutting ;) - I turn of/on this automation here with others based on my virtual battery SOC
ISSUE,  software quality, Wrong credentials when adding integration **Was not able to add the integration due to bad credentials** I added using HACS, tried to add "Aqua Temp" integration, got the popup with username and password, entered email and password and got an error stating wrong credentials.  Checked the log files and found the following lines: _2023-05-27 23:04:22.982 WARNING (MainThread) [custom_components.aqua_temp.managers.aqua_temp_api] Failed to initialize session, Error: Cannot serialize non-str key None, Line: 77 2023-05-27 23:04:22.988 WARNING (MainThread) [custom_components.aqua_temp.config_flow] Failed to create integration, Error: invalid_credentials_ So changed line 52 in custom_components.aqua_temp.managers.aqua_temp_api: self._headers = HEADERS to: self._headers = {str(key): value for key, value in HEADERS.items()} Which resulted in a successful message where the integration was added. I'm able to control the heat pump and get correct temperature readings. Thanks to ChatGPT since I've never programmed in python. Thought I should share this if someone else experience the same issue. **To Reproduce** Steps to reproduce the behavior: 1. Go to Settings - Devices & Services 2. Click on '+ ADD INTEGRATION' 3. Find Aqua Temp 4. Enter email & password 5. Check error **Expected behavior** I expected that the credentials would work, double checked my credentials used in the app. **Aqua Temp Custom Component** - Version: v.3.0.1 **Desktop** - OS: Win 10 Pro - Browser: Chrome 113.0.5672.127 (Official Build) (64-bit)
ISSUE,  learning, "count" part of the  masks codes in the JSON generated by  demo/image_demo.py program Hello, how is coded the "count" part of the  masks codes in the JSON generated by demo/image_demo.py program please ? I asked chatGPT and it said to me that it was RLE ASCII but it can't explain me correctly how it works Please help me ! Best regards
ISSUE,  feature implementation/enhancement, [FR] Allow to upload APNs auth keys to Firebase **Is your feature request related to a problem? Please describe.** We are trying to automate Firebase app creation since we manage many apps and most of them need push notifications. I played around with the Firebase admin SDK and got the most stuff working somehow, but I can't figure out how to upload the APNs auth key to Firebase. Is there and official way to do this via this SDK or via Rest API? **Describe the solution you'd like** I would like to have an SDK method or rest API endpoint where I can upload an Apple Push Notification service authentication key to Firebase so we can send push notifications to iOS apps managed in Firebase. **Describe alternatives you've considered** The alternative to having a method in the SDK would be at least a rest API endpoint. **Additional context** Funnily in my desperation I asked ChatGPT how it would solve this and it generated this code: ```js const messaging = admin.messaging(app); const apnsKey = {   authKey: '/path/to/AuthKey_XXXXXXXX.p8',   keyId: 'XXXXXXXX',   teamId: 'XXXXXXXX', }; messaging.setApnsCredential(credential, apnsKey); ``` But I looked around the repo (even in older revisions) and could not find any method like this. Was there any and if yes, when and why was it removed? 
ISSUE,  software quality, Subscriber recieve hangs infinitely ### What pyzmq version? infinitely ### What libzmq version? no ### Python version (and how it was installed) 3.10.5 ### OS Windows 11 ### What happened? A bug happened! ### Code to reproduce bug ```python That's the server code: # -*- coding: utf-8 -*- #from modules.telegram_client import telegram_client #from settings import Settings # tg_api_cred = Settings.get_telegram_api_credentials() # tg_client = telegram_client.TelegramClientCustom('session_name', tg_api_cred['api_id'], tg_api_cred['api_hash'], #                                                  raise_last_call_error=True) # # tg_client.start(Settings.get_telegram_phone_number()) import zmq import threading import json import os import time # Connect the publisher socket context = zmq.Context() publisher = context.socket(zmq.PUB) publisher.bind("tcp://127.0.0.1:5556") # Connect the router socket for receiving acknowledgements router = context.socket(zmq.ROUTER) router.bind("tcp://127.0.0.1:5557") # Save message info to a dictionary (a stand-in for your database) messages = {} # Send messages consumers = ['consumer1',  ] for i in range(10):     message_id = str(i)     file_path = f'./content/{i}'     # Save message info to the database     messages[message_id] = {         'file_path': file_path,         'consumers': consumers.copy(),  # copy the list because we're going to modify it         'processed_by': [],     }     # Send the message to all consumers     publisher.send_json({         'message_id': message_id,         'text': f'This is message {i}',         'media_path': file_path,     }) # Cleanup process def cleanup():     while True:         for message_id, message in messages.items():             print(message.items())             print(message['processed_by'])             if set(message['consumers']) == set(message['processed_by']):                 print(f"Deleting file {message['file_path']}")                 # os.remove(message['file_path'])  # uncomment this to actually delete the file                 del messages[message_id]         time.sleep(5)  # pause between cleanup runs cleanup_thread = threading.Thread(target=cleanup, daemon=True) cleanup_thread.start() # Receive acknowledgements while True:     # Wait for next request from client     message = router.recv_json()     print(f"Received request: {message}")     # Process the acknowledgement     if message['message_id'] in messages:         messages[message['message_id']]['processed_by'].append(message['consumer'])     time.sleep(5) That's the client code: import zmq import time # Prepare context and sockets context = zmq.Context() consumer_id = 'consumer1'  # change this for each consumer # Connect the subscriber socket subscriber = context.socket(zmq.SUB) subscriber.connect("tcp://127.0.0.1:5556") subscriber.setsockopt_string(zmq.SUBSCRIBE, '') # Connect the dealer socket for sending acknowledgements dealer = context.socket(zmq.DEALER) dealer.identity = consumer_id.encode() dealer.connect("tcp://127.0.0.1:5557") # Process messages while True:       message = subscriber.recv_json()     print(f"Received message: {message}")     # Send acknowledgement     dealer.send_json({         'message_id': message['message_id'],         'consumer': consumer_id,     })     time.sleep(2)  # pause between processing messages ``` ### Traceback, if applicable ```shell Consumer just hangs and does nothing, reason unknown. This example was generated by chatGPT v4 and looks fine, but for some reason it won't work ``` ### More info _No response_
ISSUE,  development environment, Improve AWS Load Balancer Controller documentation /kind bug **1. What `kops` version are you running? The command `kops version`, will display  this information.** 1.26.3 **2. What Kubernetes version are you running? `kubectl version` will print the  version if a cluster is running or provide the Kubernetes version specified as  a `kops` flag.** 1.24.14 **3. What cloud provider are you using?** AWS **4. What commands did you run?  What is the simplest way to reproduce this issue?** - `kops edit cluster` - added load balancer spec: ```yaml spec:   awsLoadBalancerController:     enabled: true ``` - closed the file **5. What happened after the commands executed?** Get the following error: `Error: Edit cancelled: no valid changes were saved.` **6. What did you expect to happen?** A message informing that I need to enable the Cert-manager addon. **7. Please provide your cluster manifest. Execute   `kops get --name my.example.com -o yaml` to display your cluster manifest.   You may want to remove your cluster name and other sensitive information.** ```yaml apiVersion: kops.k8s.io/v1alpha2 kind: Cluster metadata:   creationTimestamp: "2023-06-03T21:37:51Z"   generation: 5   name: k8s-cluster.example.com spec:   api:     dns: {}   authorization:     rbac: {}   channel: stable   cloudProvider: aws   clusterAutoscaler:     enabled: true     image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.24.2   configBase: s3://kops-state-store/k8s-cluster.example.com   etcdClusters:   - cpuRequest: 200m     etcdMembers:     - encryptedVolume: true       instanceGroup: control-plane-us-east-1a       name: a     memoryRequest: 100Mi     name: main   - cpuRequest: 100m     etcdMembers:     - encryptedVolume: true       instanceGroup: control-plane-us-east-1a       name: a     memoryRequest: 100Mi     name: events   iam:     allowContainerRegistry: true     legacy: false   kubeProxy:     enabled: false   kubelet:     anonymousAuth: false   kubernetesApiAccess:   - 0.0.0.0/0   - ::/0   kubernetesVersion: 1.24.14   masterPublicName: api.k8s-cluster.example.com   networkCIDR: 172.20.0.0/16   networking:     cilium:       enableNodePort: true   nonMasqueradeCIDR: 100.64.0.0/10   sshAccess:   - 0.0.0.0/0   - ::/0   subnets:   - cidr: 172.20.32.0/19     name: us-east-1a     type: Public     zone: us-east-1a   topology:     dns:       type: Public     masters: public     nodes: public --- apiVersion: kops.k8s.io/v1alpha2 kind: InstanceGroup metadata:   creationTimestamp: "2023-06-03T21:37:52Z"   labels:     kops.k8s.io/cluster: k8s-cluster.example.com   name: control-plane-us-east-1a spec:   image: 099720109477/ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-20230502   instanceMetadata:     httpPutResponseHopLimit: 3     httpTokens: required   machineType: t3.small   maxSize: 1   minSize: 1   role: Master   subnets:   - us-east-1a --- apiVersion: kops.k8s.io/v1alpha2 kind: InstanceGroup metadata:   creationTimestamp: "2023-06-03T21:37:52Z"   generation: 1   labels:     kops.k8s.io/cluster: k8s-cluster.example.com   name: nodes-us-east-1a spec:   image: 099720109477/ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-20230502   instanceMetadata:     httpPutResponseHopLimit: 1     httpTokens: required   machineType: t3.small   maxSize: 5   minSize: 1   role: Node   subnets:   - us-east-1a ``` **8. Please run the commands with most verbose logging by adding the `-v 10` flag.   Paste the logs into this report, or in a gist and provide the gist link here.** ``` I0603 20:11:10.396534  187391 factory.go:83] state store s3://kops-state-store I0603 20:11:10.396576  187391 s3context.go:329] unable to read /sys/devices/virtual/dmi/id/product_uuid, assuming not running on EC2: open /sys/devices/virtual/dmi/id/product_uuid: permission denied I0603 20:11:11.838065  187391 s3context.go:165] unable to get region from metadata:unable to get region from metadata: EC2MetadataRequestError: failed to get EC2 instance identity document caused by: RequestError: send request failed caused by: Get "http://169.254.169.254/latest/dynamic/instance-identity/document": context deadline exceeded (Client.Timeout exceeded while awaiting headers) I0603 20:11:11.838101  187391 s3context.go:175] defaulting region to "us-east-1" I0603 20:11:12.557386  187391 s3context.go:211] found bucket in region "us-east-1" I0603 20:11:12.557408  187391 s3fs.go:338] Reading file "s3://kops-state-store/k8s-cluster.example.com/config" I0603 20:11:13.297108  187391 s3fs.go:376] Listing objects in S3 bucket "kops-state-store" with prefix "k8s-cluster.example.com/instancegroup/" I0603 20:11:13.486579  187391 s3fs.go:404] Listed files in s3://kops-state-store/k8s-cluster.example.com/instancegroup: [s3://kops-state-store/k8s-cluster.example.com/instancegroup/control-plane-us-east-1a s3://kops-state-store/k8s-cluster.example.com/instancegroup/nodes-us-east-1a] I0603 20:11:13.486621  187391 s3fs.go:338] Reading file "s3://kops-state-store/k8s-cluster.example.com/instancegroup/control-plane-us-east-1a" I0603 20:11:13.667748  187391 s3fs.go:338] Reading file "s3://kops-state-store/k8s-cluster.example.com/instancegroup/nodes-us-east-1a" I0603 20:11:13.845466  187391 editor.go:127] Opening file with editor [code -w /tmp/kops-edit-3093854194yaml] I0603 20:11:30.510875  187391 aws_cloud.go:2114] Querying EC2 for all valid zones in region "us-east-1" I0603 20:11:30.511065  187391 request_logger.go:45] AWS request: ec2/DescribeAvailabilityZones I0603 20:11:31.532042  187391 subnets.go:51] All subnets have CIDRs; skipping assignment logic I0603 20:11:31.532073  187391 defaults.go:232] Not setting up Proxy Excludes I0603 20:11:31.532323  187391 editor.go:127] Opening file with editor [code -w /tmp/kops-edit-531991407yaml] A copy of your changes has been stored to "/tmp/kops-edit-531991407yaml" Error: Edit cancelled: no valid changes were saved. ``` **9. Anything else do we need to know?** I discovered that I needed Cert-manager with a suggestion given by ChatGPT to manually change the kOps config on S3, adding the AWS LoadBalancer Controller, and running `kops update cluster`. The output for this was `Error: spec.cloudProvider.aws.awsLoadBalanceController: Forbidden: AWS Load Balancer Controller requires that cert manager is enabled`
ISSUE,  feature implementation/enhancement, Apple M1 building instructions **Is your feature request related to a problem? Please describe.** <!-- A clear and concise description of what the problem is, e.g. "I'm always frustrated when [...]" --> I'd like to build myself darktable from the sources, but all the instructions I've found are not working. **Describe the solution you'd like** <!-- A clear and concise description of what you want to happen. --> A wiki page with all the steps to build the package using homwbrew or macports **Alternatives** <!-- A clear and concise description of any alternative solutions or features you've considered. --> I tried to build the  source using the current instructions, and trying to troubleshooting also with ChatGPT help. But I'm not a developer and it's very hard to build without some knowledge. 
ISSUE,  software quality, Tensorflow freezes during training on Mac Studio <details><summary>Click to expand!</summary>     ### Issue Type Bug ### Have you reproduced the bug with TF nightly? Yes ### Source binary ### Tensorflow Version tf2.9 ### Custom Code No ### OS Platform and Distribution MacOS Ventura 13.4/13.1 ### Mobile device Mac Studio ### Python version 3.10 ### Bazel version _No response_ ### GCC/Compiler version _No response_ ### CUDA/cuDNN version _No response_ ### GPU model and memory Apple M1 ### Current Behaviour? When training a model on my Mac Studio, from time to time it seems that the training freezes, and an epoch takes much more time than usual, from x10 to x200. I have tried contacting Apple's support, and they are clueless about this phenomena. I also went through recent Mac related issues in this repository and found nothing relevant. Here is an example of the phenomena as seen in the training's output: ``` 28/28 [==============================] - 5s 195ms/step - loss: 6.2743e-04 - accuracy: 0.0113 - val_loss: 3.4481e-04 - val_accuracy: 0.0147 Epoch 597/1000 28/28 [==============================] - 61s 2s/step - loss: 8.0337e-04 - accuracy: 0.0055 - val_loss: 3.4126e-04 - val_accuracy: 0.0244 Epoch 598/1000 28/28 [==============================] - 5s 195ms/step - loss: 7.9272e-04 - accuracy: 0.0101 - val_loss: 3.3659e-04 - val_accuracy: 0.0208 ``` ### Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np # Generate random training data np.random.seed(0) x_train = np.random.rand(100, 1) y_train = 3 * x_train + 2 + np.random.randn(100, 1) * 0.1 # Define the neural network architecture model = tf.keras.models.Sequential([     tf.keras.layers.Dense(1, input_shape=(1,)) ]) # Compile the model model.compile(optimizer='sgd', loss='mean_squared_error') # Train the model model.fit(x_train, y_train, epochs=1000, batch_size=10) ``` (Thank ChatGPT for the minimal working example) ``` ### Relevant log output ```shell Metal device set to: Apple M1 Ultra systemMemory: 128.00 GB maxCacheSize: 48.00 GB 2023-06-07 13:46:01.901374: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 2023-06-07 13:46:01.901665: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>) 2023-06-07 13:46:01.992101: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz Epoch 1/1000 2023-06-07 13:46:02.108687: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled. 10/10 [==============================] - 0s 3ms/step - loss: 14.7499 Epoch 2/1000 10/10 [==============================] - 0s 10ms/step - loss: 9.1414 ... Epoch 33/1000 10/10 [==============================] - 0s 13ms/step - loss: 0.0382 Epoch 34/1000 10/10 [==============================] - 6s 716ms/step - loss: 0.0374 Epoch 35/1000 10/10 [==============================] - 0s 5ms/step - loss: 0.0367 ... ``` </details>
ISSUE,  learning, how to connect to microsoft365 with oauth2 on powershell? I tried a lot things in powershell to connect to my exchange 365 mailbox, even ask ChatGPT AI. All examples give me: $authority = "https://login.microsoftonline.com/$tenantId/oauth2/v2.0/token" $scope = "https://outlook.office365.com/.default" $body = @{     client_id     = $clientId     client_secret = $clientSecret     scope         = $scope     grant_type    = "client_credentials" } $tokenResponse = Invoke-RestMethod -Uri $authority -Method Post -Body $body $imapClient = New-Object ImapX.ImapClient("outlook.office365.com", 993, $true) $imapClient.AuthenticateWithOAuth2($tokenResponse) or $imapClient.OAuth2Token = $tokenResponse.access_token But both (.AuthenticateWithOAuth2 and .OAuth2Token) are no valid methods of imapx. I don't get it. Are there any examples available which are working? Or a documentations hwo to do this in powershell. Thanks in advance
ISSUE,  feature implementation/enhancement, [Feature Request] Async setLookAt ### Is your feature request related to a problem? Please describe. Yes, I'm trying to run a callback after the rotation has finished. I tried using the `addEventHandler("controlend", ...)` as suggested by chatgpt and it wasn't firing. ### Describe the solution you'd like It would be awesome if we could make the setLookAt method async which fires once the rotation is completed. Not sure exactly how we would trigger the resolve. ### Describe alternatives you've considered the "controlend" event which isn't firing. Also chatgpt suggested maybe using timeouts and just passing a number of milliseconds before resolving (not a fan). ### Additional context _No response_
ISSUE,  software quality, Help > "About" > App Crashes!!!!!!!!!! ## Issue Summary Application crashes when attempting to open the "About" section from "Help" in the menu bar. ## System Information - **Operating System**: Windows 11 Pro Version 23H2, build	25387.1 [Windows Feature Experience Pack 1000.25350.1000.0] - **Notable Version**: v1.9.0-beta.10  ## Current Behavior When I run the app, click on "Help > About" from the menu bar, the application crashes. I briefly see the logo of the application, which I believe is part of the "About" section, before the crash occurs. ## Expected Behavior The "About" section should open without causing the application to crash. ## Additional Information Based on an analysis with ChatGPT and the data from DevTools/Console, it was suggested that this issue could be related to a conflict with the Squirrel updater that is used by many Electron apps for managing updates. The application might be trying to call the Squirrel updater to check for updates when "Help > About" is selected, causing the crash. I've tried the following troubleshooting steps: 1. Running the application as an administrator. 2. Reinstalling the application with the latest stable version. 3. Checked for antivirus or firewall interference. Later found the following. ## Error: 2renderer.eb130228ba9a9a47fd0d.js:37 Error: Can not find Squirrel     at Object.m [as castError] (renderer.eb130228ba9a9a47fd0d.js:3:1491)     at Object.make (renderer.eb130228ba9a9a47fd0d.js:37:26790)     at Object.silent (renderer.eb130228ba9a9a47fd0d.js:37:26538)     at onError (renderer.eb130228ba9a9a47fd0d.js:69:51706)     at i (renderer.eb130228ba9a9a47fd0d.js:1007:125975)     at execute (renderer.eb130228ba9a9a47fd0d.js:1007:124247)     at o.apply (renderer.eb130228ba9a9a47fd0d.js:1007:123948)     at Object.apply (renderer.eb130228ba9a9a47fd0d.js:1007:121575)     at Object.trigger (renderer.eb130228ba9a9a47fd0d.js:69:21811)     at Object.trigger (renderer.eb130228ba9a9a47fd0d.js:20:12599)     at Object.call (renderer.eb130228ba9a9a47fd0d.js:20:12104)     at o (renderer.eb130228ba9a9a47fd0d.js:1013:28374)     at EventEmitter.onCall (renderer.eb130228ba9a9a47fd0d.js:178:49151)     at EventEmitter.emit (node:events:390:28)     at Object.onMessage (node:electron/js2c/renderer_init:69:746) silent @ renderer.eb130228ba9a9a47fd0d.js:37 
ISSUE,  software quality, how much memory is used per item / false positive percentage? 1. lower false positive rates = higher memory used. is there a vague guide on the number of bytes? ```     filter := bloom.NewWithEstimates(1000000, 0.01)  ``` 2. for a cdn deployment with 100 million items, what's a good number for the bloom item count and false positive rate? also how much memory is needed for this bloom filter? 3. i was thinking if it's using a lot of memory, would it be advisable to use roaring bitmaps as "replacement" to bloom filter as suggested by chatgpt here: https://github.com/RoaringBitmap/roaring/issues/388
PULL-REQUEST,  software quality, Use reflection + Generics to delete most CLI boilerplate for queries <!-- < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < ‚ò∫ v                               ‚ú∞  Thanks for creating a PR! ‚ú∞     v    Before smashing the submit button please review the checkboxes. v    If a checkbox is n/a - please still include it but + a little note why v    If your PR doesn't close an issue, that's OK!  Just remove the Closes: #XXX line! ‚ò∫ > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > >  --> ## What is the purpose of the change Cleans up some CLI boilerplate. Mostly was using it as an exercise to play around with ChatGPT ## Brief Changelog - Less lines of code to create an index command - Make a generic to create our query commands - Make a generic to create our tx commands - Make a generics + reflection approach to automatically get code for "simple" queries - Index commands have more succinct outputs: Before: ![image](https://user-images.githubusercontent.com/6440154/205454032-4d8f1a67-81b3-4f10-a759-af80da7c0481.png) After: ![image](https://user-images.githubusercontent.com/6440154/205454043-90431b43-dc02-47da-96a6-cc2021f17b32.png) ## Testing and Verifying - Manually verified index commands still work, just more succinctly. - Manually verified all query commands altered work as before. Two modules not changed (Superlfuid and one other), because they do a non-standard unwrapping of the proto response. - Other TWAP commands just had code moves. ## Documentation and Release Note   - Does this pull request introduce a new feature or user-facing behavior changes? (yes / no)   - Is a relevant changelog entry added to the `Unreleased` section in `CHANGELOG.md`? (yes / no)   - How is the feature or change documented? (not applicable   /   specification (`x/<module>/spec/`)  /  [Osmosis docs repo](https://github.com/osmosis-labs/docs)   /   not documented)
PULL-REQUEST,  documentation, feat: added logs with chatgpt Added logs to cert util file using chat gpt @Evanfeenstra what do you think about this? I still haven't read over the results too much but they look decent
PULL-REQUEST,  software quality, update: redirects to use next.config instead of useEffect The previous implementation of the redirect features relied on the useEffect hook to handle redirects. However, this caused some issues with the component's rendering and led to unexpected behavior. This commit updates the redirects feature to use the next.config instead, which fixes the issues and improves the reliability of the feature. PS: Commit written by chatgpt :)
PULL-REQUEST,  documentation, Update readme.md with an updated version of an extension Hi there! I propose adding a link to [my fork](https://github.com/timkmecl/chatgpt-vscode) of the first vscode extension on the list of projects after it as an update, since the original hasn't been updated since `v0.4` of the api. I have added multiple improvements to the original: - uses `api v1.4` with session token (and am planning on upgrading it to `v2.0` today) - dispalys response word by word by using `onProgress` option - multiple UX improvements over the original - includes `.vsix` file of the extension in releases for quick installation - extended readme (co-written by chatGPT as a bonus;) )
PULL-REQUEST,  development environment, using chatgpt write a Dockerfile for chatgpt-telegrma-bot #1  This is just an experimental behavior, to test feasibility, isn't it fun?
PULL-REQUEST,  software quality, Fixing Packaging workflow using chatGPT  **What does this PR do?** The encoding change did not cause the issue, but using `with` to fix the linters incorrectly was the root cause. I used ChatGPT in order to confirm it, very impressive: <img width="717" alt="image" src="https://user-images.githubusercontent.com/85433724/206320642-5bf66039-6925-4682-a519-8f39db91d37d.png"> <img width="733" alt="image" src="https://user-images.githubusercontent.com/85433724/206320537-5e0654f1-ade0-4177-9cab-c5bb837ed50e.png"> 
PULL-REQUEST,  documentation, Improve wording / format of issue template and feature request templates ### Reasons for making this change Make wording / format better. I was playing around with ChatGPT and it made these changes, but I like them...
PULL-REQUEST,  software quality, Swap position of (t+b)/(t-b) in frustum matrix Not completely sure, but it seems there is typo here, where `(top + bottom) / (top - bottom)` is at position M[2,1], but should be at M[1,2]. Checked it with the implementation of GLM (C++ version at https://github.com/g-truc/glm) and your own examples at https://www.labri.fr/perso/nrougier/python-opengl/ Does not matter much in this library though, because, when `top == - button` then the value is always zero, and since there are no direct calls to frustum, but instead `perspective` is used, that issue will probably not be noticed. However, the reason is why I wanted to make this PR is also to note that the same implementation of `frustum` is used at https://matplotlib.org/matplotblog/posts/custom-3d-engine/. If you agree with the fix, probably you would like to fix the article as well. Having this chance once more, I would like to say again that appreciate your work a lot! P.S.: Yes, I also checked the implementation of frustum with ChatGPT üòÖ 
PULL-REQUEST,  feature implementation/enhancement, Use Get-ChildItem instead of a hardcoded path for php_relay.dll @yatsukhnenko the future is now.  This line of code was generated by chatGPT ![Screenshot 2022-12-10 at 2 39 27 PM](https://user-images.githubusercontent.com/468149/206878334-68cedacd-a2ce-4a5f-8424-013fc57c40a1.png) mind=blown
PULL-REQUEST,  feature implementation/enhancement, Scammers getting ratioed by chatGPT Written by chatGPT not Tested
PULL-REQUEST,  documentation, Update README with chatgpt help ## Changes Made - Improved the README by adding helpful information. - Created with chatGPT, curated by Me. ## How chatGPT helped The changes to the README were made with the help of an AI language model trained by OpenAI. The model was given the name and description of the plugin, as well as the current README and documentation for the plugin. Based on this information, the model generated a revised README file. ### Inputs to chatGPT #### Input 1 Do you know the app obsidian? #### Input 2 I will give you informations about an obsidian plugin and I want you to return a proper readme for its repository.  Plugin name: QuickAdd; Current readme: {{ paste from https://github.com/chhoumann/quickadd/blob/53e4f4cfbc1dfdbe83a6e4560d225487dadbb352/README.md }} Current documentation home: {{ paste from https://quickadd.obsidian.guide/docs/ }} #### Input 3 Include the demo video in the readme; #### Input 4 Instead of using troubleshooting, the people with questions or problems should use the [community discussions](https://github.com/chhoumann/quickadd/discussions) #### Input 5 Remove the features from the readme #### Input 6 In the Getting Started session just link to the documentation. #### Input 7 print as raw markdown 
PULL-REQUEST,  process, It's a new year Time flies when you're coding! This commit updates the copyright year in every file in the repository to the current year, because the old one is so last year. We've made the following changes: - [x] Replaced the outdated copyright year with the fresh and current one in the header of each file - [x] Gave the LICENSE file a much needed copyright year makeover - [x] Double checked that every file in the repository is now sporting the current copyright year This commit is essential for keeping our code looking sharp and staying ahead of the times. Plus, it's the responsible thing to do. --- _PR description gently donated by ChatGPT because humans are so 2009_
PULL-REQUEST,  software quality, chore: add a couple unit tests Signed-off-by: Charles-Edouard Br√©t√©ch√© <charles.edouard@nirmata.com> ## Explanation This PR adds a few unit tests. Those tests were generated by chatgpt and while it was not perfect it did a decent job at creating the unit tests code. 
PULL-REQUEST,  process, Add EPSS Ingestor command to ingest-worker Detailed Summary: - This commit adds a new epss command to the ingest-worker. - The epss command includes an "ingest" subcommand which allows the user to update EPSS scores. - The epss command is implemented using the clifx library and includes a NewCommand function to create the cli.Command struct. - The epss command has a dependency on an EPSSIngester interface which is provided through the fx dependency injection library. - The epss command logs informative messages when starting and completing the ingestion process. (Thank ChatGPT for the git commit message lol) The SQL I wrote is the fastest that I could figure out. The first implementation took >10 minutes and this one takes about 30 seconds now. It turns out that query for the IDs ahead of time and then running an "UPDATE" for that is the fastest way. (worth noting) 
PULL-REQUEST,  feature implementation/enhancement, Add OpenAI Textcat recipes ## Description This Pull Request does two things: - Add two recipes for text classification with the help of ChatGPT - Refactor the `OpenAISuggester` as a base class so that it's more reusable ## Add recipes for text classification This PR adds two recipes, `textcat.openai.correct` and `textcat.openai.fetch`, to perform text classification in a streaming and batch manner. I also removed the  `textcat.openai.suggest` because the `fetch` command already fulfills the same purpose. Unlike the previous state of this PR, I have now combined binary and multiclass textcat into a single recipe, where the logic is based on the number of labels (i.e., len(labels)>1 then multiclass). The template also follows a similar logic using if-else statements found in Jinja2.  I've also made the `prompt_path` a positional argument because I think it reads better, i.e.: > Help me annotate <input_path> using this <prompt_path> Below are a few examples: ### Example 1: Binary classification In this example, we perform binary classification by passing a single label to `--labels`. The `view_id` also changes with respect to this value. ```sh python3 -m prodigy textcat.openai.correct textcat_test \     data/reddit_r_cooking_sample.jsonl \ # input path     templates/textcat_prompt.jinja2 \ # template path     --labels recipe     -F recipes/openai_textcat.py     --verbose ``` <img src="https://user-images.githubusercontent.com/12949683/213096513-eb4bda7b-a5cb-4e7f-a870-52670ade687d.png" width=400> ### Example 2: Multiclass / Multilabel classification For multiclass (exclusive) classification, we pass the parameter `--exclusive-classes` or `-E` to state that we only want a single value. The `config.choice` parameter changes accordingly: ```sh python3 -m prodigy textcat.openai.correct textcat_test \     data/reddit_r_cooking_sample.jsonl \ # input path     templates/textcat_prompt.jinja2 \ # template path     --labels recipe,feedback,question     -F recipes/openai_textcat.py     --exclusive-classes     --verbose ``` <img src="https://user-images.githubusercontent.com/12949683/213096307-eb656885-f4e4-47b2-8efd-74b97b4c8b9b.png" width=400> For multilabel classification, we don't set the `-E` flag. Notice that it's now a checkbox instead of a choice button: ```sh python3 -m prodigy textcat.openai.correct textcat_test \     data/reddit_r_cooking_sample.jsonl \ # input path     templates/textcat_prompt.jinja2 \ # template path     --labels recipe,feedback,question     -F recipes/openai_textcat.py     --exclusive-classes     --verbose ``` <img src="https://user-images.githubusercontent.com/12949683/213096054-07579491-6b07-47eb-8546-e9eb8e4062a0.png" width=400> ## Refactor OpenAISuggester I went ahead and attempted to generalize the `OpenAISuggester` class. It is now located in `recipes/openai.py`. It is now an abstract base class (`abc.ABC`) and has an abstractmethod `parse_response`. This means that we need to inherit `OpenAISuggester` and implement the `parse_response(example: Dict, response: str)` function. The role of `parse_response` is to convert whatever ChatGPT returns (`response`) and assign the parsed values into appropriate keys in a copy of `example`. You can see an example implementation of this in `recipes:openai_textcat.py:TextCatOpenAISuggester`.  ## Changelog - January 18: did a few refactors after initial review.  
PULL-REQUEST,  software quality, Add more link parsing unit tests Uncovered test case ideas came with minimal thinking thanks to ChatGPT üëÄ 
PULL-REQUEST,  documentation, Update README Grammar nit suggested by ChatGPT
PULL-REQUEST,  learning, minio: disable file index This prevents unauthorized users from browsing other files in the bucket Special thanks to ChatGPT for the code: ![image](https://user-images.githubusercontent.com/16504501/212819817-e98bd80a-68bc-42f6-91a7-bf9a063a89f2.png) 
PULL-REQUEST,  documentation, Feature generation overhaul & better docs # CANCELLED in favor of smaller individual PRs <details> <div align=center> ![image](https://user-images.githubusercontent.com/61068799/215210381-a4141df7-6b0f-4847-8069-94a531663b29.png) THIS IS ONLY "CLOSED" TO NOT SPAM GITHUB ACTIONS ON THIS REPO. I'm still working on it! </div> ## Progress - [ ] Use ChatGPT or GitHub Copilot to add a description header to each of the Actions in the .github/workflows directory - [x] Make sure all GitHub Actions work Answer... - [ ] How do I create a feature? - [ ] What's the bash_modules/ folder for? - [ ] How does a feature work? - [ ] Why are some things features, and other things not? - [ ] How do I test my feature? - [ ] What's a good way to install $X in my feature? - [ ] What are the implicit dependencies of all features? - [ ] How do M1 macs vs x86 work? ## Executive summary of how the new version works Let's say you want to make a new feature. Let's say it's for Deno. 1. Read CONTRIBUTING.md 2. Fork 3. Open MAIN in CODESPACES 4. Run `tools/create-feature.js` 5. Add `source bash_modules/ensure_curl.sh` to `install.sh` 6. Add _some magic commands_ to make it install a specific version (or LATEST if no `$VERSION`) 7. Add a link to the Deno install instructions 8. Add a README.md with a brief description saying that this adds the Deno binary to `usr/local/bin/` or whatever 9. Run tests (starter test command is prompted in `tools/create-feature.js`) 10. Add a scenario to test supplying version 11. Run tests again 12. Commit 13. Push 14. Open PR 15. Tests run in CI 16. Maintainer merges 17. DONE! The highlights are that: 1. `bash_modules/` is back 2. Deployed IMMEDIATELY from main to ghcr.io 3. GitHub Pages is used for docs. Using PHP template SSG-ish thing. Can be changed to Next.js or some other SSG later if you want. 4. Scope of features is SIGNIFICANTLY reduced 5. Added guidelines to detail what makes a feature and what should be a postCreateCommand.
PULL-REQUEST,  documentation, Added detectors README descriptions For the purpose of code walkthrough of the slither/detector repo and to make a first-hand understanding of how the detectors work in the first place, it is vital to know how all of the detectors are classified (following bug classes i.e. the detector classes).  To find the answers I looked up the code pattern of each detector after which I was partially able to understand where inside the detector directory the files of a new detector are to be placed. I felt the need that all detectors in their naming are not mostly self-explanatory hence, used ChatGPT to feed detectors' attributes to describe each directory inside the detector folder. 
PULL-REQUEST,  software quality, [EZY KARMA] [GENERATED BY CHATGPT] NONE: retry database connection errors on backfill **What's in this PR?** Retrying yet another error that could stop backfilling Kudos to ChatGPT for help! <img width="507" alt="image" src="https://user-images.githubusercontent.com/20631664/216189190-196c50b6-a3bd-4a50-8daf-956249875315.png"> **Why** Customer is complaining **Added feature flags** **Affected issues**   **Whats Next?** The end of the world 
PULL-REQUEST,  generating/manipulating data, ‚ú® Add support for new languages for DeepL translation Language pairs proudly generated with ChatGPT ‚Äôcause lazy.
PULL-REQUEST,  software quality, test: add chatGPT code review github action <!--  Thanks for sending a pull request!  Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here: https://git.k8s.io/community/contributors/devel/release.md#issue-kind-label 3. Ensure you have added or ran the appropriate tests for your PR: https://git.k8s.io/community/contributors/devel/testing.md 4. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews 5. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/release-notes.md 6. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests --> **What type of PR is this?** /kind test **What this PR does / why we need it**: test: add chatGPT code review **Which issue(s) this PR fixes**: <!--  *Automatically closes linked issue when PR is merged. Usage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`. _If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_* --> Fixes # **Requirements**: - [ ] uses [conventional commit messages](https://www.conventionalcommits.org/)   <!-- Common commit types:         build: Build üè≠         chore: Maintenance üîß         ci: Continuous Integration üíú         docs: Documentation üìò         feat: Features üåà         fix: Bug Fixes üêû         perf: Performance Improvements üöÄ         refactor: Code Refactoring üíé         revert: Revert Change ‚óÄÔ∏è         style: Code Style üé∂         security: Security Fix üõ°Ô∏è         test: Testing üíö --> - [ ] includes documentation - [ ] adds unit tests - [ ] tested upgrade from previous version **Special notes for your reviewer**: **Release note**: ``` none ``` 
PULL-REQUEST,  documentation, Fixes #2212 Adding a README for our add-permissions drush command  ## Description I asked ChatGPT to create this README file, so please review thoroughly. ## Related issues #2212  ### Arizona Quickstart (install profile, custom modules, custom theme) - **Patch release changes**    - [ ] Bug fix    - [ ] Accessibility, performance, or security improvement    - [ ] Critical institutional link or brand change    - [ ] Adding experimental module    - [ ] Update experimental module - **Minor release changes**    - [ ] New feature    - [ ] Breaking or visual change to existing behavior    - [ ] Upgrade experimental module to stable    - [ ] Enable existing module by default or database update    - [ ] Non-critical brand change    - [ ] New internal API or API improvement with backwards compatibility    - [ ] Risky or disruptive cleanup to comply with coding standards    - [ ] High-risk or disruptive change (requires upgrade path, risks regression, etc.) - **Other or unknown**    - [x] Other or unknown ### Drupal core - **Patch release changes**    - [ ] Security update    - [ ] Patch level release (non-security bug-fix release)    - [ ] Patch removal that's no longer necessary - **Minor release changes**    - [ ] Major or minor level update - **Other or unknown**    - [ ] Other or unknown ### Drupal contrib projects - **Patch release changes**    - [ ] Security update    - [ ] Patch or minor level update    - [ ] Add new module    - [ ] Patch removal that's no longer necessary - **Minor release changes**    - [ ] Major level update - **Other or unknown**    - [ ] Other or unknown ## Checklist <!--- Go over all the following points, and put an `x` in all the boxes that apply. --> <!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! --> - [ ] My code follows the code style of this project. - [ ] My change requires a change to the documentation. - [x] I have updated the documentation accordingly. - [x] I have read the **CONTRIBUTING** document. - [ ] I have added tests to cover my changes. - [ ] All new and existing tests passed. 
PULL-REQUEST,  feature implementation/enhancement, fix: hot reload server ### Issue / feature description The hot reload server was using `debounce` from `lodash`. `lodash` is banned and `lodash-es` doesn't work with `ts-node` atm, leaving hot reload server broken. ### Changes - have ChatGPT write a debounce function - remove lodash / lodash-es dependency ### Checklist - [x] Rebased to the last commit of the target branch (or merged) - [x] Code self-reviewed - [x] Code self-tested - [x] Tests updated (if needed) - [x] All tests are passing locally Please keep your pull request as small as possible. If you need to make multiple changes, please create separate pull requests for each change. Create a draft pull request if you need early feedback. This will mark the pull request as a work in progress and prevent it from being reviewed or merged until you are ready. Please move drafts to the ready for review (regular PR) when you are ready to start the review process and other developers will pick it up from there. 
PULL-REQUEST,  software quality, feat(android): android optimizations I've used ChatGPT and put in three methods and asked for optimizations :smile:  Here are the changes: <b>ListView:</b> * Instead of using a LinkedList to store the recyclable items, you can use a LinkedHashSet to store the recyclable items in a more efficient way. A LinkedHashSet provides fast lookups and preserves the order of insertion. This code uses a LinkedHashSet to store the recyclable items for each template, and it iterates over the recyclable items using a for-each loop to find a recyclable item with child views that can be reused by the current item. When a recyclable item is found, it removes it from the set and transfers its child views to the current item. <b>removeAllChildren</b> * One small optimization you could make is to avoid creating a new list if there are no children to remove. This would save a little bit of memory and processing time <b>remove</b> * The check for peekView() != null could be moved to the beginning of the function to avoid unnecessary computations if the parent view has not yet been attached to a window. * The check for child.parent != null && child.parent.get() == this can be replaced with a simpler child.parent = null, since this statement removes the child view from the current parent, if any. <b>toImage</b> * The unnecessary blob variable was removed, since it's only used to hold a default result value Android tests run fine, App runs fine.  I like the `toImage` and `removeAllChildren` changes! Makes the code a bit easier to read. Don't see any performance improvements. We can ignore the ListView part since it's a bit out of my knowledge if its better or not.
PULL-REQUEST,  documentation, chore: add comments using chatGPT <!--  Thanks for sending a pull request! Read contributing.md before commit pull request. --> **What this PR does**: Add comments using chatGPT. This is an experiment. **Which issue(s) this PR fixes**: <!-- *Automatically closes linked issue when PR is merged. Usage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`. _If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_* --> Fixes # **Special notes for your reviewer**: **Does this PR introduce a user-facing change?**: <!-- If no, just write "NONE" in the release-note block below. If yes, a release note is required: Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string "action required". --> ```release-note ```
PULL-REQUEST,  software quality, Baseline tests for `fs.go` and `file.go` Tests generated by ChatGPT and just slightly adjusted. The tests are not perfect, but a nice baseline.
PULL-REQUEST,  feature implementation/enhancement, test: add UT and fix a typo using chatGPT <!--  Thanks for sending a pull request! Read contributing.md before commit pull request. --> **What this PR does**: It's an experiment to generate UT using chatGPT.  And the generated UT find out a typo: <img width="886" alt="image" src="https://user-images.githubusercontent.com/26001097/219986967-050aa19c-355c-412c-a9fa-9825d60787e5.png"> <img width="745" alt="image" src="https://user-images.githubusercontent.com/26001097/219983430-c34b465c-be3e-4a43-a744-b07e4290e0ed.png"> **Which issue(s) this PR fixes**: <!-- *Automatically closes linked issue when PR is merged. Usage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`. _If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_* --> Fixes # **Special notes for your reviewer**: **Does this PR introduce a user-facing change?**: <!-- If no, just write "NONE" in the release-note block below. If yes, a release note is required: Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string "action required". --> ```release-note ```
PULL-REQUEST,  feature implementation/enhancement, clang-tidy: Enable more checks, take 2 This supersedes #149. We haven't had any checks (except for naming) enabled in our clang-tidy config for the past 6 months or so, which not just means nothing gets checked during CI, but also that some IDEs (e.g. VSCode + clangd) don't show any diagnostics during programming. This is a second stab at enabling more checks, this time however without `misc-const-correctness` and `misc-unused-parameters`. Those two were responsible for 95% of the changes in the previous patch, and I don't feel like going over everything again (since the auto-fix only works half of the time). I've also snuck in a small change to the clang-format config to prescribe CV-qualifier positions to be west const. The previous PR took quite a long time during CI (40 minutes); honestly I don't understand why (edit: I just remembered - the CI script doesn't run across TUs in parallel, something we could look at). A full run on all files takes 5 minutes on gpuc1, and CI should only run on changed files (the previous PR did touch a lot more files, though). Anyway, lets see how this pans out. For future reference, I measured the time required for each of the checks (shoutout to ChatGPT for co-authoring that script): ``` bugprone-* added 51.56 seconds (total 51.56) -bugprone-easily-swappable-parameters added -1.67 seconds (total 49.89) -bugprone-lambda-function-name added 0.14 seconds (total 50.03) -bugprone-macro-parentheses added 0.48 seconds (total 50.51) misc-* added 7.37 seconds (total 57.88) -misc-no-recursion added -1.09 seconds (total 56.79) -misc-non-private-member-variables-in-classes added 0.50 seconds (total 57.29) clang-analyzer-* added 187.68 seconds (total 244.96) clang-diagnostic-* added -0.60 seconds (total 244.36) cppcoreguidelines-* added 19.73 seconds (total 264.09) -cppcoreguidelines-avoid-c-arrays added -2.23 seconds (total 261.86) -cppcoreguidelines-avoid-magic-numbers added -4.90 seconds (total 256.96) -cppcoreguidelines-macro-usage added 4.19 seconds (total 261.15) -cppcoreguidelines-pro-bounds-pointer-arithmetic added -0.04 seconds (total 261.11) mpi-* added -0.99 seconds (total 260.12) performance-* added 19.02 seconds (total 279.14) readability-* added 22.42 seconds (total 301.57) -readability-avoid-const-params-in-decls added -5.62 seconds (total 295.95) -readability-identifier-length added 0.41 seconds (total 296.35) -readability-magic-numbers added 2.31 seconds (total 298.66) -readability-uppercase-literal-suffix added -4.19 seconds (total 294.47) ```
PULL-REQUEST,  feature implementation/enhancement, chore: add demo code for delay queue API, using chatGPT <!--  Thanks for sending a pull request! Read contributing.md before commit pull request. --> **What this PR does**: It's an experiment to generate gRPC demo code using chatGPT **Which issue(s) this PR fixes**: <!-- *Automatically closes linked issue when PR is merged. Usage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`. _If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_* --> Fixes # **Special notes for your reviewer**: **Does this PR introduce a user-facing change?**: <!-- If no, just write "NONE" in the release-note block below. If yes, a release note is required: Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string "action required". --> ```release-note ```
PULL-REQUEST,  software quality, add environment variable for ffmpeg threads This addresses https://github.com/clipable/clipable/issues/52 After having a conversation with ChatGPT I've come to the conclusion that the best way for us to limit memory usage is to restrict threads. The x primary factors for memory usage are:  - Input resolution  - Input bit depth  - Output resolution(s)  - Output bit depth(s)  - Output bit rate buffers  - Parallelism Parallelism being threading here is the only way we can affect memory usage without also adversely affecting quality. I plan to put this all into a wiki page at some point but here are some numbers for memory reduction based on thread usage: at 1440p/60fps: ``` 1 Thread: 2.74Gb 8 Threads: 4.01Gb 16 Threads: 5.26Gb 32 Thread: 8.66Gb ```
PULL-REQUEST,  software quality, optimize performance by replacing modulus with bitwise operator <!--- Provide a general summary of your changes in the Title above --> ## Description <!--- Describe your changes in detail --> Replaces all usage of modulus "% 2" with the bitwise AND operator "& 1" ## Motivation and Context <!--- Why is this change required? What problem does it solve? --> <!--- If it fixes an open issue, please link to the issue here. --> Inspired by https://github.com/opentypejs/opentype.js/pull/569#discussion_r1114926698 ## How Has This Been Tested? <!--- Please describe in detail how you tested your changes. --> <!--- Include details of your testing environment, and the tests you ran to --> <!--- see how your change affects other areas of the code, etc. --> Ran all tests (and asked ChatGPT if those statements are really interchangeable in JavaScript üòÖ) ## Screenshots (if appropriate): ## Types of changes <!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: --> - [ ] Bug fix (non-breaking change which fixes an issue) - [ ] New feature (non-breaking change which adds functionality) - [ ] Breaking change (fix or feature that would cause existing functionality to change) - [X] Performance improvement ## Checklist: <!--- Go over all the following points, and put an `x` in all the boxes that apply. --> <!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! --> - [X] I did `npm run test` and all tests passed green (including code styling checks). - [ ] I have added tests to cover my changes. - [ ] My change requires a change to the documentation. - [ ] I have updated the **README** accordingly. - [X] I have read the **CONTRIBUTING** document. 
PULL-REQUEST,  feature implementation/enhancement, [Fixes #148] Write script for quick branch / PR creation. Fixes #148 I used ChatGPT to help me crafting the shell scripts as a kind of experiment (I have not used it before). It was impressive. <img width="821" alt="Screen Shot 2023-02-24 at 1 55 08 PM" src="https://user-images.githubusercontent.com/52168/221276577-136df018-a6bc-47ed-a1c0-f9bfa2e7b940.png"> <img width="835" alt="Screen Shot 2023-02-24 at 2 45 33 PM" src="https://user-images.githubusercontent.com/52168/221276578-bc97cde9-9bd0-40c6-bd10-79b3a6fca07f.png"> <img width="828" alt="Screen Shot 2023-02-24 at 2 33 42 PM" src="https://user-images.githubusercontent.com/52168/221276579-20470201-f57b-45af-a3e1-adb0cfdc2744.png"> <img width="838" alt="Screen Shot 2023-02-24 at 1 58 43 PM" src="https://user-images.githubusercontent.com/52168/221276582-eae6b2e4-a93d-4fde-8a96-86f91455ea69.png"> 
PULL-REQUEST,  feature implementation/enhancement, feat: Improve text labels based on suggestions from chatGPT I asked chatGPT to help me brainstorm improvements to some of the labels and hint text based on the Apple Human Interface Guidelines. I then edited them as human to improve them further. Review and let me know if you think any are worse or weird.
PULL-REQUEST,  feature implementation/enhancement, docs: add NextJS chatgpt to README.md Hiüëã I use chatgpt-api to make a ChatGPT website
PULL-REQUEST,  feature implementation/enhancement, Improvement: Add shortcuts PAGE_UP and PAGE_DOWN in the image editor ‚Ä¶ ‚Ä¶to switch to the next or previous image. Created using ChatGPT in Bing. I thought using ChatGPT for this is fun and somewhat meta, I hope it is also ok. The keys are currently hardcoded to PAGE_UP and PAGE_DOWN. I don't think init.py is the right place for this but rather somewhere else in the code. No need to put the keys into the addon preferences, though because the user is able to change the keys using the Blender keymap editor once the addon is registered: ![image](https://user-images.githubusercontent.com/2581621/222390564-7d1fbf92-3c80-4e54-900f-1460aa5850ce.png) 
PULL-REQUEST,  development environment, [ci] put primary condition to rector github action run @samsonasik I had this in mind :) suggested by ChatGPT Could you test it on the use case that was failing?
PULL-REQUEST,  generating/manipulating data, add ChatGPT suggestions to bully messages more better more bullying
PULL-REQUEST,  software quality, [GITHUB] Add a GitHub Action for ChatGPT code review ## Purpose Make ChatGPT do code review for each newly-opened PR. JIRA issue: N/A ## Proposed changes - Add `.github/workflow/cr.yml` for GitHub Actions. ## TODO - [x] Check the cost and the budget. - [ ] Reach consensus. - [ ] Add `OPENAI_API_KEY` repository secret. - [ ] Confirm whether this PR will work. ## Knowledge base - An ChatGPT API key from OpenAI is necessary. - The trigger of the GitHub Action is to open or reopen a PR after commit of this PR. - ChatGPT will read PR changes. - ChatGPT doesn't reply a PR's comment (yet). - ChatGPT won't read more than 5000 bytes. - The `Ada` plan (fastest): $0.0004 / 1K tokens - The `Davinci` plan (most powerful): $0.0200 / 1K tokens - For text in English, 1 token is approximately 4 characters or 0.75 words.
PULL-REQUEST,  software quality, Update chat.go Update ChatCompletionResponse struct and ChatChoice struct by removing the Usage field from ChatCompletionResponse and moving it to ChatChoice. by ChatGPT
PULL-REQUEST,  feature implementation/enhancement, Spanish translation By ChatGpt
PULL-REQUEST,  process, Java Demo App first commit ### Description of change ##### Checklist - [ ] Tested in playground or other setup - [ ] Screenshot (Grafana) from playground added to PR for 15+ minute run - [ ] Documentation is changed or added - [ ] Tests and/or benchmarks are included - [ ] Breaking changes <!-- This is an auto-generated comment: release notes by chatgpt --> ### Summary by ChatGPT **Release Notes** This pull request adds new functionality to the Java demo app, including classes for handling requests, subrequests, and subrequest chains. The changes include: - New Feature: Added classes for handling requests, subrequests, and subrequest chains. - Documentation: Added a command to build and run the app, and a URL to check the results. - Chore: Updated several files, including the gitignore file, build.gradle, gradle wrapper, and Gradle scripts. <!-- end of auto-generated comment: release notes by chatgpt -->
PULL-REQUEST,  process, Fix tiltfile to allow specific resources to run ### Description of change This fix allows to run tilt in three ways, with the default scenario and specific scenario , and lastly with specific resources in cli. tilt up tilt up -- agent controller tilt up -- --scenario=./scenarios/workload_prioritization ##### Checklist - [ ] Tested in playground or other setup - [ ] Screenshot (Grafana) from playground added to PR for 15+ minute run - [ ] Documentation is changed or added - [ ] Tests and/or benchmarks are included - [ ] Breaking changes <!-- This is an auto-generated comment: release notes by chatgpt --> ### Summary by ChatGPT **Release Notes:** - Bug fix: Fixed Tiltfile to allow running tilt in three ways: with the default scenario, with a specific scenario, and with specific resources in CLI. The diff for `playground/Tiltfile` adds an if-else block to check if no resources are specified, run the default ones. It also adds a check to load k6 configuration and process it for inclusion into a YAML file. If no load generator config is found, it prints a message. <!-- end of auto-generated comment: release notes by chatgpt -->
PULL-REQUEST,  process, Reorder SDK usage docs to better show supported libraries/frameworks ### Description of change ##### Checklist - [ ] Tested in playground or other setup - [ ] Screenshot (Grafana) from playground added to PR for 15+ minute run - [x] Documentation is changed or added - [ ] Tests and/or benchmarks are included - [ ] Breaking changes <!-- This is an auto-generated comment: release notes by chatgpt --> ### Summary by ChatGPT Documentation: Updated Aperture Java SDK documentation to include instructions on how to use the SDK with various libraries and frameworks, including Aperture Java Instrumentation Agent, Netty Handler, Servlet package, Spring Boot Filter, and Tomcat Filter. <!-- end of auto-generated comment: release notes by chatgpt -->
PULL-REQUEST,  process, Add ability to list discovery entities in aperturectl ### Description of change - Add `discovery entities` command that list all entities by IP addresses - Add `--find-by="[name|ip]=<string>" flag to get by name or ip ##### Checklist - [x] Tested in playground or other setup - [ ] Screenshot (Grafana) from playground added to PR for 15+ minute run - [ ] Documentation is changed or added - [ ] Tests and/or benchmarks are included - [ ] Breaking changes <!-- This is an auto-generated comment: release notes by chatgpt --> ### Summary by ChatGPT **New Feature:** Added new functions to list discovery entities and a single discovery entity by IP address or name. These functions were added to `pkg/agentfunctions/agents/agents.go` and `pkg/discovery/entities/entities-service.go`. Additionally, two new RPCs were added to `api/aperture/cmd/v1/cmd.proto`. The changes also include minor updates to documentation and CircleCI configuration. <!-- end of auto-generated comment: release notes by chatgpt -->
PULL-REQUEST,  process, chatgpt review: rename repository <!-- This is an auto-generated comment: release notes by chatgpt --> ### Summary by ChatGPT **Chore:** Renamed repository used in a workflow job. <!-- end of auto-generated comment: release notes by chatgpt -->
PULL-REQUEST,  process, Fixed only flag not working properly in tilt ### Description of change Fixed the issue where even after setting `--only`, tilt was starting scenarios. Examples: ``` tilt up -- --only agent controller tilt up -- --only agent controller istio ``` CC: @sudhanshu456  ##### Checklist - [x] Tested in playground or other setup<!-- This is an auto-generated comment: release notes by chatgpt --> ### Summary by ChatGPT **Release Notes:** - New Feature: Added support for `--only` flag to Tilt. - Bug fix: Fixed an issue where scenarios were still starting even after setting `--only`. <!-- end of auto-generated comment: release notes by chatgpt -->
PULL-REQUEST,  process, Fix brew release flow <!-- This is an auto-generated comment: release notes by chatgpt --> ### Summary by ChatGPT ## Release Notes ### Bug Fix - Fixes the brew release flow by changing the executor, adding a commit message, and creating a pull request using the GitHub CLI. <!-- end of auto-generated comment: release notes by chatgpt -->
PULL-REQUEST,  feature implementation/enhancement, Add svg rotation transform support Fixes #8166 - ChatGPT wrote the matrix transformations. I merely fixed type and import errors and verified the code works. <!-- Thank you for pull request. Below are items maintainers should consider when merging the PR. Feel free to suggest a `unit@` label or check-mark the others as appropriate. --> Maintainer merge checklist * [x] Title is descriptive/clear for inclusion in release notes. * [x] Applied a `Component: xxx` label. * [ ] Applied the `api-deprecation` or `api-break` label. * [ ] Applied the `release-highlight` label to be highlighted in release notes. * [x] Added to the milestone version it was merged into. * [x] **Unittests** are included in PR. * [ ] Properly documented, including `versionadded`, `versionchanged` as needed. I didn't include unittests, as I didn't see any other unittests for svg support and so I couldn't pattern logic and organization off of that. Would be happy to add tests if I had another example to work off of. I did include an example svg, which rendered properly once I figured out the build-run-verify workflow. I'm not certain about label adding and whatnot, as those feel like discretionary decisions that core developers should be making. 
PULL-REQUEST,  documentation, Weakly update 2023.03.17  - Version: to stable version 1.0.0  - Experiment: add backup, note, rerun methods, refine code  - Panel: add jupyter-compatible experiment management dashboard  - Project: update some config files  - Accelerator: make huggingface accelerate an optional choice, support accelerator switch  - othe compatiable changes  - example: add one-file training examples  - test: test coverage up to 77%  - Document: 100% docstring, powered by ChatGPT  - Document: update README, present the core concept of lumo, add examples and description
PULL-REQUEST,  software quality, ref(utils): reorder regexpa and add tests Reorder the regexp so that we prioritize the one more likely to match. Also adds tests and covered an edge case where input like `'at myFunction (/path/to/file.js)';` would have undefined paths. I have generated the tests with the help of chatgpt and manually checked all of them - it got a few of them wrong or was testing impossible cases, but it did find that one edge case apparently
PULL-REQUEST,  software quality, [FSSDK-8955] chore: Fix upload to S3 naming ## Summary - Trying to fix the DLL artifact uploads to AWS  chatGPT for the win (hopefully) ## Test plan Tests should pass ## Issues - FSSDK-8955
PULL-REQUEST,  software quality, Refactor Room Join Been going through our React code. Dig into RoomJoin to review it. Ask ChatGPT to improve & refactor it for fun. Could not make the improvements work but left the refactoring. And added some comments myself. 
PULL-REQUEST,  feature implementation/enhancement, feat(i18n): üåê Chinese translation This PR provides a Chinese translation for lazydocker. There are one translation items that I have comment out. I am not sure how this one can be aptly expressed in Chinese. <img width="313" alt="image" src="https://user-images.githubusercontent.com/29500388/226694938-e54802a5-577c-432d-95db-b1eab151afae.png"> It work fine in my computer. If there is any omissions, point out, please. (My english is very poor, I used ChatGPT to help me complete this translation.)  <img width="1512" alt="image" src="https://user-images.githubusercontent.com/29500388/226695192-26c5b447-6d6b-4a75-98e7-25242c37bb4b.png"> 
PULL-REQUEST,  software quality, Trying ChatGPT Code Review Closes https://github.com/nimblehq/android-templates/issues/425 ## What happened üëÄ - Use `launchIn` instead of `collect` for collecting Flows in ViewModel. - Replace the old `execute` protected fun in BaseViewModel with the new `launch` fun (renaming for consistency). - Add `injectLoading` fun to reduce the logic for `showLoading` and `hideLoading` at `onStart` and `onCompletion` in each Flow. ## Insight üìù - The new `launch` fun should accept `EmptyCoroutineContext` as default ([same as the original fun](https://kotlinlang.org/api/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/launch.html)). [With `EmptyCoroutineContext`, the `Dispatchers.Default` is used](https://kotlinlang.org/api/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/launch.html) üëâ **main thread**, to launch the Job.   <img width="1117" alt="image" src="https://user-images.githubusercontent.com/16315358/226787795-ad620e90-63ec-4d33-80be-87a4ed4bf081.png"> - [To launch a Flow in a specific CoroutineContext](https://developer.android.com/kotlin/flow), such as `IO` for storage or network execution. We must use [`flowOn`](https://kotlinlang.org/api/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/flow-on.html), e.g. `.flowOn(dispatchers.io)`. Hence, we don't need to pass `DispatchersProvider` into the BaseViewModel. - Note that `flowOn` changes the CoroutineContext of the upstream flow.   ![image](https://user-images.githubusercontent.com/16315358/226789924-ee6030ad-0d71-4dc9-aab9-1dc32c7b60c5.png) - This update ensures we will not launch all unnecessary Flows on Dispatcher.IO at all times.   Before ‚ùå  https://github.com/nimblehq/android-templates/blob/5a037e8ca67447e70610463bb77f2146a56a6e66/sample-compose/app/src/main/java/co/nimblehq/sample/compose/ui/base/BaseViewModel.kt#L49-L52      After ‚úÖ  https://github.com/nimblehq/android-templates/blob/869f82ea73f98e9a47d5934485ff0a3810a5cb7a/sample-compose/app/src/main/java/co/nimblehq/sample/compose/ui/screens/home/HomeViewModel.kt#L29-L37 - `.catch { }` should always be put at the end of a Flow collector, in front of `.launchIn` to catch all the prior errors. ## Proof Of Work üìπ - All the core app logic remains the same as before ‚úÖ  - All tests are passed ‚úÖ  
PULL-REQUEST,  development environment, Try ChatGPT Code Review on #228 ## Why Most of apps use API request and have `refresh token` mechanism to request new access token in case of expired. We don't have the setup for the initial `TokenAuthenticator` yet so we should add it into our templates. ## Who Benefits? `Developers` who use this template to initialize their project with network request. I think this is **essential** because most of apps have API request and have refresh token mechanism. Having a standard setup would help us a lot in development and reduce mistakes. Example of implementation: https://github.com/nimblehq/toyota-wallet-android/blob/main/data/src/main/java/co/omise/gcpf/service/authenticator/ApplicationRequestAuthenticator.kt 
PULL-REQUEST,  development environment, add chatgpt code review Use ChatGPT to review iotex-core code
PULL-REQUEST,  software quality, WIP chore: test chatgpt code review Test chatGPT code review, not need merge.
PULL-REQUEST,  development environment, chore: build - [x] chatgpt code review 
PULL-REQUEST,  documentation, Do an editing pass of the book using chatgpt  - [x] Changelog updated / no changelog update needed 
PULL-REQUEST,  documentation, feat: Add chinese README This README has been translated using ChatGPT. PS: Using ChatGPT to translate articles with markdown format is a great experience, but even with ChatGPT's help, it still took 1 hour and 30 minutes to translate this README. The main time was spent waiting for ChatGPT's response and manually copying the content from GitHub to chat.openai.com. PS: If ChatGPT could access external up-to-date information through a plugin, the experience of translating README.md and similar documents would be greatly improved.
PULL-REQUEST,  development environment, test chatgpt codereview for test,don't merge
PULL-REQUEST,  software quality, test(map_loader): add a ROS 2 test ## Description Mostly written by ChatGPT <!-- Write a brief description of this PR. --> ## Pre-review checklist for the PR author The PR author **must** check the checkboxes below when creating the PR. - [x] I've confirmed the [contribution guidelines]. - [x] The PR follows the [pull request guidelines]. ## In-review checklist for the PR reviewers The PR reviewers **must** check the checkboxes below before approval. - [ ] The PR follows the [pull request guidelines]. ## Post-review checklist for the PR author The PR author **must** check the checkboxes below before merging. - [ ] There are no open discussions or they are tracked via tickets. After all checkboxes are checked, anyone who has write access can merge the PR. [contribution guidelines]: https://autowarefoundation.github.io/autoware-documentation/main/contributing/ [pull request guidelines]: https://autowarefoundation.github.io/autoware-documentation/main/contributing/pull-request-guidelines/ 
PULL-REQUEST,  software quality, test chatgpt code review for test,don't merge
PULL-REQUEST,  software quality, test chatgpt code review test
PULL-REQUEST,  software quality, test chatgpt code review test
PULL-REQUEST,  software quality, test chatgpt code review mode: file test
PULL-REQUEST,  software quality, test chatgpt code review test
PULL-REQUEST,  feature implementation/enhancement, Add extra motivational messages generated by chatGPT This pull request adds extra motivational messages that were generated by chatGPT. (There are now six times as many, for a total of 36) I already removed and adjusted some, and generated some new dutch text using copilot. But the quality of the English texts is still a lot higher compared to dutch. Feel free to remove/adjust text that you don't like. I also changed the alert to a callout to better render all the emoji. Old: ![image](https://user-images.githubusercontent.com/21177904/227904444-dc9904ce-cf87-4fdf-81d2-08269f7f991e.png) New: ![image](https://user-images.githubusercontent.com/21177904/227904399-e7a11501-4845-49ac-8d1e-01c11f98c422.png) 
PULL-REQUEST,  feature implementation/enhancement, feature: wait("2s") filter that was done with the help of chatGPT I was playing around with chatGPT and tried to build a trivia skipper filter.
PULL-REQUEST,  software quality, [api] ServerV2.Stop() use correct context # Description From ChatGPT code review Fixes #(issue) ## Type of change Please delete options that are not relevant. - [x] Bug fix (non-breaking change which fixes an issue) - [] New feature (non-breaking change which adds functionality) - [] Code refactor or improvement - [] Breaking change (fix or feature that would cause a new or changed behavior of existing functionality) - [] This change requires a documentation update # How Has This Been Tested? Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration - [x] make test - [] fullsync - [] Other test (please specify) **Test Configuration**: - Firmware version: - Hardware: - Toolchain: - SDK: # Checklist: - [x] My code follows the style guidelines of this project - [x] I have performed a self-review of my code - [x] I have commented my code, particularly in hard-to-understand areas - [x] I have made corresponding changes to the documentation - [x] My changes generate no new warnings - [x] I have added tests that prove my fix is effective or that my feature works - [x] New and existing unit tests pass locally with my changes - [x] Any dependent changes have been merged and published in downstream modules 
PULL-REQUEST,  generating/manipulating data, Size tier list for Azure DBforMySQL flexible servers This is a size tier list for Azure DBforMySQL flexible servers. The data was taken from this website: https://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-service-tiers-storage I then used ChatGPT to paste the data from the site and have it rearranged in the proper format. Further research will be needed to figure out how to automate this since "use ChatGPT" isn't really repeatable without manual intervention. (Note: The already-existing sql_service_tier_types.json can be used for Azure DBforMySQL single servers)
PULL-REQUEST,  learning, docs(README): add pip command for hugging face library I didn't know how to do this so I asked ChatGPT xD
PULL-REQUEST,  software quality, Fixed failing test Issue https://github.com/kiali/kiali/issues/5972 Fixed using ChatGPT :)
PULL-REQUEST,  software quality, Fixed failing test by sorting Fixed by using ChatGPT for issue https://github.com/kiali/kiali/issues/5972
PULL-REQUEST,  documentation, rephrase README by ChatGPT https://github.com/webp-sh/webp_server_go/tree/rephrase_md powered by AI  üòºüòºüòº
PULL-REQUEST,  documentation, Update README.md¬†‚Äî edits "vision" section to remove some hype (written by Chat-GPT) I told ChatGPT to "rewrite the following paragraph but remove the oversold promises and hype language" and here's what I got
PULL-REQUEST,  process, Added environment variables **Summary** (This was written by ChatGPT üôÇ) This pull request moves user-configurable variables from within the code to a .env file, improving security and flexibility. **Description:** Previously, user-configurable variables were hard-coded within the codebase. This is not ideal, as it can expose sensitive information such as API keys and passwords, and makes it difficult to manage different configurations across different environments. This pull request moves these variables to a .env file, which can be loaded at runtime and easily customized by the user. Specifically, the following variables have been moved: ``` # API CONFIG OPENAI_API_KEY= PINECONE_API_KEY= PINECONE_ENVIRONMENT=us-east1-gcp # TABLE CONFIG TABLE_NAME=test-table # PROJECT CONFIG OBJECTIVE=Solve world hunger FIRST_TASK=Develop a task list ``` Users can now customize these variables by editing the .env file, rather than modifying the codebase directly. This makes it easier to manage different configurations across different environments (e.g. development, staging, production), and improves security by separating sensitive information from the codebase. **Testing:** This pull request has been tested locally by loading the .env file and verifying that the configurable variables are loaded correctly. It is expected to work seamlessly in production as well.
PULL-REQUEST,  development environment, build: set up workflow to automatically manage/mirror the v0.16.1 branch In this PR, we set up an automated Github Actions workflow to merge any PR that has a milestone of v0.16.1 into the v0.16.1 branch once it has been merged into the master branch. For this to work, we'll need to create a new scoped PAT that gives our normal code review bot the ability to merge PRs into the master branch. Note that with the way this is set up, we only want to merge things in when they have all the checks passed. We might want to also consider `peter-evans/enable-pull-request-automerge` for this task. **NOTE**: The contents of this PR were generated entirely by ChatGPT ü§ñ. 
PULL-REQUEST,  development environment, interesting test with chatgpt : from HL7/FHIR schema to NGSI-LD yaml model example for Patient resource with a few questions to the brave chatGPT I managed to get an NGSI-LD yaml model file of the Patient resource type from HL7/FHIR json schema page... It could be interesting to dig a bit the possibilities,  but at least it can be a simple way to produce entity model on demand... what do you think @albertoabellagarcia ?
PULL-REQUEST,  feature implementation/enhancement, Added 5 More Emoji's that Elon uses frequently to algorithm.ts Asked ChatGPT about 10 Emoji's Elon Uses Frequently , Added them to the Algorithm.ts  ü§ñ Robot Face: used in tweets about robotics or AI. üåû Sun: used in tweets about solar power or renewable energy. üåç Globe Showing Americas: used to indicate global impact or presence, particularly in North or South America. üß¢ Billed Cap: used to indicate support for a political or social movement, particularly related to the United States. ü•≥ Partying Face: used to indicate celebration or excitement.
PULL-REQUEST,  documentation, docs: add the latest Chinese translation  The previous two versions of the Chinese translation were too old and not updated anymore. This is the latest Chinese version based on ChatGPT and manually corrected. 
PULL-REQUEST,  feature implementation/enhancement, use ChatGPT to write hugging face connectors This is a fun gimmick, but you can actually just use ChatGPT to write connectors for the local Hugging Face Flask server. The prompt below should be fairly self-explanatory, but I've also updated the README with instructions on using `create_hugging_face_connector.py`. The prompt to ChatGPT: ``` Given the description below, please write a Python function that follows the following format: - The function should be named `import_and_return_<model_name>`. - The function should return a tuple of three items: a model, a tokenizer, and a function that takes the model, a string, and the tokenizer as arguments and returns the output of the model on the string. - If there is no need for, for instance, a tokenizer, then the function should return `None` for that item in the tuple. Same with the model. - The function should be able to be called as `model, tokenizer, model_fn = import_and_return_<model_name>()`. - If no examples are given in the description, then simply return "No examples given on the page.". Below is an example. def import_and_return_tk_instruct_3b_def():     # Example import function for Huggingface model. Returns model, tokenizer,     and code for running the model.     tokenizer = AutoTokenizer.from_pretrained(ALLENAI_INSTRUCT3B_MODEL)     model = AutoModelForSeq2SeqLM.from_pretrained(ALLENAI_INSTRUCT3B_MODEL)     def model_fn(model, string, tokenizer):         input_ids = tokenizer.encode(string, return_tensors="pt")         output = model.generate(input_ids, max_length=10)         output = tokenizer.decode(output[0], skip_special_tokens=True)         return output     return model, tokenizer, model_fn ================== DESCRIPTION ================== {description} ================================================= Your response: ```
PULL-REQUEST,  feature implementation/enhancement, Turkish language support added Automatically translated the website into Turkish using ChatGPT and then controlled and corrected manually.
PULL-REQUEST,  feature implementation/enhancement, [devbox] add hidden all command to list all sub-commands (including hidden) ## Summary This provides a "single view" listing of all available commands. Sometimes I find myself wanting a global view to quickly find something. credit to chatgpt for writing some of this. Its a hidden command, since the printed view is not polished. ## How was it tested? ``` ‚ùØ devbox all devbox              Instant, easy, predictable development environments 	add <pkg>...        Add a new package to your devbox 	all                 List all commands, including hidden ones 	cloud               [Preview] Remote development environments on the cloud 		forward <local-port>:<remote-port> | :<remote-port> | stop | list[Preview] Port forward a local port to a remote devbox cloud port 			list                List all port forwards managed by devbox 			stop                Stop all port forwards managed by devbox 		init                Create a Cloud VM without connecting to its shell 		shell               [Preview] Shell into a cloud environment that matches your local devbox environment 	completion          Generate the autocompletion script for the specified shell 		bash                Generate the autocompletion script for bash 		fish                Generate the autocompletion script for fish 		powershell          Generate the autocompletion script for powershell 		zsh                 Generate the autocompletion script for zsh 	gen-docs <path>     [Internal] Generate documentation for the CLI 	generate            Generate supporting files for your project 		debug 		devcontainer        Generate Dockerfile and devcontainer.json files under .devcontainer/ directory 		direnv              Generate a .envrc file that integrates direnv with this devbox project 		dockerfile          Generate a Dockerfile that replicates devbox shell 		ssh-config          Generate ssh config to connect to devbox cloud 	global              Manage global devbox packages 		add <pkg>...        Add a new global package 		list                List global packages 		pull <file> | <url> Pull a global config from a file or URL 		rm <pkg>...         Remove a global package 		shellenv            Print shell commands that add global Devbox packages to your PATH 	help [command]      Help about any command 	info <pkg>          Display package info 	init [<dir>]        Initialize a directory as a devbox project 	install             Install all packages mentioned in devbox.json 	log <event-name> [<event-specific-args>] 	plan                Preview the plan used to build your environment 	rm <pkg>...         Remove a package from your devbox 	run [<script> | <cmd>]Run a script or command in a shell with access to your packages 	services            Interact with devbox services 		ls                  List available services 		restart [service]...Restart service. If no service is specified, restarts all services 		start [service]...  Start service. If no service is specified, starts all services 		stop [service]...   Stop service. If no service is specified, stops all services 		up                  Starts process manager with all supported services 	setup               Setup devbox dependencies 		nix                 Install Nix 	shell               Start a new shell with access to your packages 	shellenv            Print shell commands that add Devbox packages to your PATH 	version             Print version information 		update              Update devbox launcher and binary ``` 
PULL-REQUEST,  feature implementation/enhancement, eslint RFC: Ban positional boolean flag parameters ## What does this PR do? - Adds an eslint rule to ban positional boolean flag parameters ## Remaining Work - [ ] Clean up lint errors ## Discussion - Written by ChatGPT - Should decide with warning vs. error ## Checklist - [x] Add tests - [x] Designate a primary reviewer: @BLoe  
PULL-REQUEST,  development environment, Feat/simplify dockerfile **Before submitting this PR, please make sure**: - [ ] Your code builds clean without any errors or warnings - [ ] You are using approved title ("feat/", "fix/", "docs/", "refactor/") **A description of the changes proposed in the pull request**: Improve the installation process by removing all environment.yml files that are platform specific. Also, the Docker workflow is simplified to one file for all architectures. The final structure has only 1 environment.yml file under the folder setup/ and 1 Dockerfile. install.sh was modified too, and now is using the same file. If you want to build the image: `docker build -t hummingbot/hummingbot:multi_arch_test .` In case that you want to build and push the results you can use buildx. To create a new instance `docker buildx create --platform linux/arm64,linux/amd64` `docker build use $name_of_buildx_instance` `docker buildx build --platform linux/arm64,linux/amd64 -t hummingbot/hummingbot:multi_arch_test --push .` That will upload a version for all architectures directly. Probably this is a good task for @david-hummingbot and @rapcmia to see how it works. Based on ChatGPT this are all the available platforms: - linux/amd64 - linux/arm64 - linux/arm/v7 - linux/ppc64le - linux/s390x - windows/amd64 - windows/arm64 Please check if that's true, to create the corresponding buildx that works for all platforms. Here you have a refactored docker-compose.yml that works with the new dockerfile: https://gist.github.com/cardosofede/8cd795b26d2989cad376ba231b506955 Check that now we need to specify the full path: /home/hummingbot/data.... **Tests performed by the developer**: - uninstall and install conda environment - build docker image and run it - push image, clone it again and run it **Tips for QA testing**: - Run ./uninstall and ./install in all OS (windows, ubuntu, Mac) in all arch(amd64, arm64,..) - Build docker image and run it from all combinations mentioned before - Build an image with buildx that works in all architectures and upload it to hummingbot registry - Run the compose file with the new image created by QA in all combinations of OS and Arch mentioned before 
PULL-REQUEST,  software quality, chore(node-utils): simplify some util functions and add some unit tests ## PR Checklist - [ ] Addresses an existing open issue: fixes #000 - [ ] That issue was marked as [accepting prs](https://github.com/typescript-eslint/typescript-eslint/issues?q=is%3Aopen+is%3Aissue+label%3A%22accepting+prs%22) - [x] Steps in [Contributing](https://typescript-eslint.io/contributing) were taken ## Overview It uses some more modern syntax to make some utility functions more concise. I also added a few unit tests. This PR doesn't fix a specific issue. I'm not sure what I would put in the issue description? These code suggestions were done by chatgpt 
PULL-REQUEST,  feature implementation/enhancement, Add Spanish translation I didn't translate them myself, I used ChatGPT 4. I double-checked the translation (I'm a native speaker), and it's good.
PULL-REQUEST,  feature implementation/enhancement, Internationalization - First internationalization release, using i18next - Translations created by ChatGPT for Spanish and German - Backend/auth server still needs to be internationalized
PULL-REQUEST,  feature implementation/enhancement, add install option to disable the default win11 prtscr tool Hi there, inspired from #482 and with the help of ChatGPT I hacked out the installer to disable the PrtScr default tool on Windows 11 installations. It is shown as an extra option if you're on Win11 >= Build 22000. This can be refined further if required. Built and tested on wdk2023, arm64 - the PR is platform independent, though.
PULL-REQUEST,  software quality, feature: Improved User Input Validation and Status Display - Validates the correct structure for Israeli IDs - Validates the structure for valid Israeli cell phone numbers (foreign numbers are not supported) - Limits the selected cities to a maximum of 4 - If a field is invalid, it will be marked in red - The initial state is "error", but it will be updated as information is entered using onChange events - Includes tests, which were coded with care by ChatGPT <3 ## Description Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change. Fixes # (issue) ## Type of change Please delete options that are not relevant. -   [x] New feature (non-breaking change which adds functionality) ## How Has This Been Tested? Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration ## Checklist: -   [x] My code follows the style guidelines of this project -   [x] I have performed a self-review of my own code -   [x] I have commented my code, particularly in hard-to-understand areas -   [x] I have made corresponding changes to the documentation -   [x] My changes generate no new warnings -   [x] I have added tests that prove my fix is effective or that my feature works -   [x] New and existing unit tests pass locally with my changes -   [x] Any dependent changes have been merged and published in downstream modules -   [x] I have checked my code and corrected any misspellings 
PULL-REQUEST,  development environment, WIP refactor ### Description This WIP pull request includes several changes to improve the functionality, security, and performance of the application. It includes a new GitHub Workflow using ChatGPT for automated code reviews, a new GithubClient struct with methods to interact with GitHub Pull Requests, updated dependencies for improved security and stability, and a refactored codebase with updates to the GitHub Go SDK version. ### Changes - Added a GitHub Workflow for automated code reviews using ChatGPT in .github/workflows/cr.yml. - Added a new GithubClient struct with methods to interact with Github Pull Requests in github.go. - Added several new dependencies to the 'require' block in go.mod. - Updated dependencies to latest versions for improved security and stability in go.sum. - Refactored code and updated GitHub Go SDK version in main.go. - Added a new function to the OpenAIClient struct to enable chat completion using GPT 3.5 Turbo model in openai.go.
PULL-REQUEST,  process, chore: update multiple-environments feature flag criteria  ## Description As part of ongoing efforts to improve our feature flagging process, this Pull Request updates the "multiple-environments" feature flag criteria to limit access to a specific list of email addresses, rather than the entire Appsmith domain. This will help to reduce the impact of the latest merges and ensure that only authorised users have access to the feature. Specifically, this Pull Request modifies the existing criteria for the "multiple-environments" feature flag and changes the access control from email domains to individual email addresses. This ensures that only users with approved email addresses can access the feature, and reduces the risk of unauthorised access or unintended consequences resulting from recent code merges. description generated by chatGPT The email addresses are as follows: 1. [me-eng1@appsmith.com](mailto:me-eng1@appsmith.com) 2. [me-eng2@appsmith.com](mailto:me-eng2@appsmith.com) 3. [me-qa1@appsmith.com](mailto:me-qa1@appsmith.com) 4. [me-qa2@appsmith.com](mailto:me-qa2@appsmith.com) 5. [me-demo@appsmith.com](mailto:me-demo@appsmith.com) > changed the feature flag strategy from emailDomain to email and assigned five emails Fixes #22626 ## How Has This Been Tested? - Manual ## Checklist: ### Dev activity - [x] My code follows the style guidelines of this project - [x] I have performed a self-review of my own code - [ ] I have commented my code, particularly in hard-to-understand areas - [x] I have made corresponding changes to the documentation - [x] My changes generate no new warnings - [ ] I have added tests that prove my fix is effective or that my feature works - [ ] New and existing unit tests pass locally with my changes - [ ] PR is being merged under a feature flag 
PULL-REQUEST,  documentation, (supecollider/supercollider - improving documentation) Update docs with the help of ChatGPT #### (supecollider/supercollider - improving documentation) Update README_WINDOWS.md with the help of ChatGPT # Greetings <sub>***hereby I would like to introduce a few tips on how to improve the supercollider documentation***</sub> <sub>**thank you so much**</sub> <sub>**T.**</sub>
PULL-REQUEST,  documentation, Fix grammar and wording in README I used ChatGPT to revise the README file and corrected grammar errors and inappropriate wording. 
PULL-REQUEST,  documentation, (supecollider/supercollider - improving documentation) Update docs with the help of ChatGPT (supecollider/supercollider - improving documentation) Update docs with the help of ChatGPT please feel free to merg this to my previous pull requests
PULL-REQUEST,  documentation, (supecollider/supercollider - improving documentation) Update docs with the help of ChatGPT (supecollider/supercollider - improving documentation) Update docs with the help of ChatGPT please feel free to merge this to my previous pull requests
PULL-REQUEST,  documentation, [docs] crisp chat for kapa.ai integration Preview: https://anyscale-ray--34782.com.readthedocs.build/en/34782/ tomorrow, kapa.ai will finish our custom plugin for this chat interface, then we can use chatgpt for tech support on docs, and optionally loop in human operators.
PULL-REQUEST,  process, Add Notice to Logo/Branding Guidelines Button Hovertext [unsuccessful skip ci] ### Requirements for Contributing Documentation * Fill out the template below. Any pull request that does not include enough information to be reviewed in a timely manner may be closed at the maintainers' discretion. * The pull request must only contribute documentation (for example, markdown files or API docs). To contribute other changes, you must use a different template. You can see all templates at <https://github.com/atom/.github/tree/master/.github/PULL_REQUEST_TEMPLATE>. ### Description of the Change This pull request adds a notice to the hovertext of the Logo/Branding Guidelines button/badge stating that the guidelines were created by GitHub, not the Atom Community. The notice reads: "Please note that the branding guidelines accessible through the following link are the official branding guidelines for GitHub, and not a branding guideline created by the Atom Community." This is my first pull request here in a while, so any feedback or guidance is appreciated. Thank you for considering this contribution. (The text above was generated by ChatGPT bcz it sorta makes lives easier. The notice too, only a bit was removed) ### Release Notes N/A (this is not even Atom anyway) <!-- Please describe the changes in a single line that explains this improvement in terms that a user can understand.  This text will be used in Atom's release notes. If this change is not user-facing or notable enough to be included in release notes you may use the strings "Not applicable" or "N/A" here. Examples: - The GitHub package now allows you to add co-authors to commits. - Fixed an issue where multiple cursors did not work in a file with a single line. - Increased the performance of searching and replacing across a whole project. -->
PULL-REQUEST,  software quality, Telemetry: Persist sessionId across runs Closes N/A ## What I did - [ ] Persist sessionId into cache for 2h - [ ] Tests by ChatGPT ## How to test - [ ] Attached unit tests - [ ] Manual tests   - [ ] Run a sandbox locally multiple times & verify same sessionId   - [ ] Delete `node_modules/.cache` folder & verify new sessionId 
PULL-REQUEST,  feature implementation/enhancement, Add touch events So, you're on your way home with your kid, and talk about the nice Tetris game you recently played with your team. So you just have a Mac and and iPhone, which is bad for co-op playing. You look at the code, and ask ChatGPT to add touch capabilities ... and it works with a few tweaks. You and your kid can play co-op Tetris until the train arrives ... and another PR arrived (pls test, just to be sure)
PULL-REQUEST,  documentation, Update README.md I found the readme.md a bit difficult to digest, so I utilized ChatGPT to help me simplify the content. This allowed me to better understand the library's core features and functionality. It might be worth considering a shorter, more concise version of the readme for easier comprehension by other users as well.
PULL-REQUEST,  software quality, Try suggestion from chatGPT to accept empty string as valid result Hmm, this attempt doesn't work
PULL-REQUEST,  software quality, Fix ramdom Unexpected ASGI message to appear I asked ChatGPT what might cause the issue and here is the answer: > The BaseLangchainStreamingResponse class uses the async and await keywords to execute asynchronous code. When you use asynchronous programming, multiple coroutines can execute concurrently, meaning that different parts of your application may execute simultaneously. This can lead to race conditions where different coroutines try to modify the same data at the same time. >  > In this case, it is possible that multiple coroutines are trying to send response messages concurrently, which can cause the 'Unexpected ASGI message' error message to appear. To fix this issue, you can try synchronizing access to the send function by using a lock. Fixes ajndkr/fastapi-async-langchain#17
PULL-REQUEST,  feature implementation/enhancement, Support for Firefox & Privacy Consent (1.2.0?) Hi @jessejanderson, thanks for building this.  I spent some time at lunch porting `1.0.1` of this extension to Firefox, hastily submitted it to Mozilla (I got excited!), and it was promptly rejected by them because it sends browsing data (hostnames) to a third party service (Google DNS). üò≠ I worked to add a privacy consent workflow per [their guidelines](https://extensionworkshop.com/documentation/develop/best-practices-for-collecting-user-data-consents/) and backported your changes from 1.1.0 into my build. Then I stepped back and thought it would be silly to have 2 code bases, 2 extension maintainers, etc, so I worked to add some logic to make the extension work in both browsers (tested in Firefox 112.0.2 & Chrome 113.0.5672.63) so I could merge it back into your repo. Some notes: 1. The Firefox version is still using manifest v2. I tried building with manifest v3, but it seems that the flow for granting optional permissions for `<all_urls>` (you can no longer declare it by default) is not complete or broken (tried in latest Firefox nightly as well). I'm going to keep tinkering on this to see if I can get a v3 build with proper permissions. 2. Regardless, you'd need to maintain a separate `manifest.json` for Firefox, since it has a special section (`browser_specific_settings`) that Chrome will complain about if you use a unified manifest. So much for the interoperability goals of web extensions. 3. There is a portable (for macOS and Linux) shell script (`build_extensions.sh`) that will build the flat zip files for both Firefox and Chrome using the correct manifest. 4. I originally hadn't forked your repo, and also leaned on ChatGPT for some areas that I wasn't familiar with, so the coding style might not match 100%, but aside from the privacy consent workflows required by Mozilla, it is functionally identical to 1.1.0 when tested in both browsers. All said, I think the best course of action is to delete my hasty submission to Mozilla (which is currently awaiting a new review) and let you resubmit this to Mozilla under your name, unless you would like someone else to maintain the Firefox releases. Let me know what you think, and if/how you'd like to proceed.
PULL-REQUEST,  feature implementation/enhancement, Adding markdown-it plugin to parse alt attrs also as title. I played a bit ping pong with ChatGPT and came up with a markdown-it plugin, to fix the title tag as discussed in dev call and on #1264  - parses images and uses the found alt text and adds it as title - you can still override the title according to mardown spec with double quotes like I did in the Guide.md as example `![BTCPay Server](./img/BTCPayServerScreenshot.png "BTCPay Server screenshot")` - not sure about when prettier would change double quotes to single quotes, it works for me locally so it seems to be no problem as long as it keeps double quotes? **Youtube** For youtube videos if we would want to do that there too we would need to adjust the replaceYoutubeLinks in the preprocess here I think: https://github.com/btcpayserver/btcpayserver-doc/blob/master/docs/.vuepress/preprocessMarkdown.js#LL70C20-L70C20 It adds a link as wrapper to an iframe, the iframe title (which does not show up). If the alt attribute is available there we would need to adjust the regex to get it and set the title in the link tag to make it show up on mouseover. 
PULL-REQUEST,  software quality, Remove node streams-based ingest code to prepare for post-ingest notifications **Related Issue(s):**  - #45  **Proposed Changes:** 1.  Remove logic from `ingestItem` and have it call `ingestItems` with a single-item list. 2. Remove node streams-based ingest by extraction functionality into `lib/ingest.js` and calling it directly from `ingestItems` 3. Remove batch processing functionality (at least temporarily) My initial approach to preparing the code to support post-ingest notifications was to make a minimally invasive change that expanded the existing node streams-based implementation. Theoretically converting the `Writable` subclass to a `Duplex` subclass so that items would be written to the DB and then the results would be made available to downstream consumer ( eventually an SNS notification) would be small in scope. I abandoned this approach for a number of reasons. - Unresolved hanging problem in the `Duplex` implementation. The first item would be processed but failed to move forward.	   - Google searching did not lead to an effective solution   - I also tried using ChatGPT to suggest solution that resulted in the same hanging issues - The current project maintainers do not have a regular working knowledge of the node streams API - The stream processing switched between single and bulk operations in a way that is was not directly controllable. Code within the node streams API decides when to call `_write` to process a single item or `_writev` to process multiple items. - Streams are most helpful when dealing with large inputs because they allow avoiding the need to hold the entire set of data to be processed in memory. In this case the data to be processed is already all in memory. - The `INGEST_BATCH_SIZE` variable was misleading. It was only used to set `highWaterMark` of the `SearchDatabaseWritableStream`   - This is a threshold used by the buffering mechanism of node streams https://nodejs.org/dist/latest-v16.x/docs/api/stream.html#buffering   - Based on my reading it does not directly control batch size https://stackoverflow.com/questions/67435206/nodejs-writable-streams-writev-alternating-between-1-and-highwatermark-chunks This PR removes the node-stream based processing by extracting functionality from the `lib/databaseStream` module and moving it into `lib/ingest` where it can be called directly. Unmerged PR #131 was a valuable reference but I did not adapt it directly because it was opened against a much older version of the codebase and it mixed removing the node streams-based implementation with SNS notifications. Follow up PRs to add this notification functionality will likely borrow more heavily from this previous attempt.  `ingestItem` and `ingestItems` differed only in their logging, so to reduce duplication I have removed all the logic from `ingestItem` and it now just calls `ingestItems` Batch processing is removed from this implementation. `ingestItems` saves items to the database sequentially, one at a time. Allowing items to be created in parallel without using the `bulk` API resulted in test failures. If this is a significant performance issue in a high-volume setting it can be restored in a future pull request. Though not a load test, I did `time` the running of `./bin/system-tests.sh` 3 time on both the main branch and this branch as a minimal test that this did not change performance in small-load situations. `main` real	1m4.807s real	0m58.459s real	0m59.376s `jcw/remove-stream-based-ingest` real	0m57.250s real	0m57.398s real	0m58.861s **PR Checklist:** - [ ] I have added my changes to the [CHANGELOG](https://github.com/stac-utils/stac-server/blob/main/CHANGELOG.md) **or** a CHANGELOG entry is not required. 
PULL-REQUEST,  documentation, Terms of Service & Acceptable Use Policy Adds additional pages for terms of service (`/tos`) and acceptable use policy (`/aup`). On new user connection, they will be given a prompt to accept this policy: <img width="477" alt="image" src="https://user-images.githubusercontent.com/16991201/236647356-7bc113a9-0f3c-4470-8d70-ec56093eb890.png"> Also it made the TUI a bit nicer for when a user doesn't have any files: <img width="734" alt="image" src="https://user-images.githubusercontent.com/16991201/236647377-ab391e75-3e90-4d16-9074-3cc92c5febb8.png"> Gotta cover my üçë just in case. Thanks ChatGPT for ghost writing the documents ü§ñ 
PULL-REQUEST,  feature implementation/enhancement, tons of functionality I've been fleshing out this library for a personal project for a few months and at this point I've added a fairly long list of functionality: - Pulling history - Making edits - Sending typing signals - Uploading images - Downloading images - Getting user profile (pfp and nick) - Getting room names from ID The following message types now parse properly: - EditEvent - ImageMessageEvent - RedactionEvent - ReactionEvent BaseRoomEvents now include EventId and Timestamp. I only today got this into a semi-acceptable state to make a pull request. Some of it was written by ChatGPT and I rebelled against the established code style several times while adding code to areas I didn't understand. I've attempted to clean it up to a reasonable degree to fit into codebase, but I'm sure there are plenty of weird things throughout and it should be reviewed appropriately. For what it's worth, this code is currently running in production and seemingly all works properly.
PULL-REQUEST,  software quality, fix: added an early exit which greatly improves mesh generation This improvement has been suggested by ChatGPT 4. I adapted it to work on the game. The next steps of optimization would be :  1. Parallel Processing 2. Octress instead of 3D Array
PULL-REQUEST,  feature implementation/enhancement, Code Freeze CLI: Replace PHP script with TS ### Submission Review Guidelines: - I have followed the [WooCommerce Contributing Guidelines](https://github.com/woocommerce/woocommerce/blob/trunk/.github/CONTRIBUTING.md) and the [WordPress Coding Standards](https://make.wordpress.org/core/handbook/best-practices/coding-standards/). - I have checked to ensure there aren't other open [Pull Requests](https://github.com/woocommerce/woocommerce/pulls) for the same update/change. - I have reviewed my code for [security best practices](https://developer.wordpress.org/apis/security/).  - Following the above guidelines will result in quick merges and clear and detailed feedback when appropriate. <!-- You can erase any parts of this template not applicable to your Pull Request. --> ### Changes proposed in this Pull Request: <!-- Describe the changes made to this Pull Request and the reason for such changes. --> This is a follow up to https://github.com/woocommerce/woocommerce/pull/38082 and replaces a PHP script that manipulates the output of Jetpack Changelogger. There should be no functional difference in the resulting PRs. Note: I used chatGPT to convert the code from PHP to JS. The result was pretty good as a just little cleanup was involved, but overall happy with the result. Translating the regex and replace functions was the difficult part and the outcome is good. Fixes https://github.com/woocommerce/woocommerce/issues/37905 <!-- Begin testing instructions --> ### How to test the changes in this Pull Request: <!-- Include detailed instructions on how these changes can be tested. Review and follow the guide for how to write high-quality testing instructions. --> Using the [WooCommerce Testing Instructions Guide](https://github.com/woocommerce/woocommerce/wiki/Writing-high-quality-testing-instructions), include your detailed testing instructions: #### Prerequisites 1. You must have a `GITHUB_TOKEN` environment variable available in your PATH. This needs to be set to a [personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token) with repo permissions enabled. 2. Fork woocommerce onto your own namespace. #### Prepare your fork 1. We'll pretend this is 7.8 code freeze, so update `trunk` to the latest. 2. Create a new branch, `release/7.8`, and push to your fork. 3. In case you're repeating this test, remove any branches that shouldn't be there ``` git push <remote> --delete update/7.8-changelog && git push <remote> --delete delete/7.8-changelog   ``` #### Test the command 1. Run the command pointing to your fork. The command uses a full clone with history, so it will take a couple minutes. ``` pnpm run utils code-freeze changelog -o <owner> -v 7.8 ``` 2. Once the command has completed, you will see two pull requests. For reference here are the same pull requests from the 7.7 release. * https://github.com/woocommerce/woocommerce/pull/37776 * https://github.com/woocommerce/woocommerce/pull/37775 3. Confirm the pull requests created are identical to the 7.7 ones: * base * format * content * changelog format * etc. <!-- End testing instructions -->
PULL-REQUEST,  documentation, feat: add ESO threat model This is ready for review. It adds a threat model for ESO. I fed most of it through ChatGPT to make it more readable :sweat_smile:  Goals: It's really hard to define the scope. The goal of this document is to provide a easy-to-read and easy-to-understand piece of documentation for infosec folks and cluster admins. The document should: 1. highlight the most severe threats for our users 2. highlight good security controls that our users should put in place 3. document what assets need to be protected Non-goals: * follow a security guildeline or self-assessment just for the sake of it. We will need to go through it at some point (CNCF incubating/graduation phases), but not now. I think there is a more value for our users in providing a digestible document  There are a lot of good resources out there that i took for inspiration.  * [CNCF TAG Security - Self Assessment](https://github.com/cncf/tag-security/blob/main/assessments/guide/self-assessment.md) * [CNCF TAG Security - Lightweight Threat Modelling Recommendations](https://docs.google.com/document/d/1tuGtKrjcreDFlHcXYCTjLvy3mjyamdQzwCZr6uqFcR4/edit#heading=h.4zcarjfnymxt) (This is in the works, see https://github.com/cncf/tag-security/issues/903) * Also see [Flux Multi-Tenancy example](https://docs.google.com/document/d/1Ruf3VNmdJyvp5bgiu3rjhNDEZx3zK4Ui6BIonkHwl_E/edit#heading=h.s2vhbyeeeqni) As a result of this threat model there are some TODOs to improve the security posture of ESO: 1. implement NetworkPolicies for our components 2. implement principle of least privilege RBAC for `cert-controller` ([see here](https://github.com/external-secrets/external-secrets/blob/7b8fef2c182702c018b8ed078ae310d52ecd8ad5/deploy/charts/external-secrets/templates/cert-controller-rbac.yaml#L47-L50) and [limit CRUD to specific resources here](https://github.com/external-secrets/external-secrets/blob/7b8fef2c182702c018b8ed078ae310d52ecd8ad5/deploy/charts/external-secrets/templates/cert-controller-rbac.yaml#L21-L22)) 3. TBC.. <summary>:framed_picture:  Screenshots for viewing pleasure :point_left:  <details> ![localhost_8000_guides_security-best-practices_](https://github.com/external-secrets/external-secrets/assets/1709030/91e8f610-403c-4343-8fe2-ba03ce83f9a9) ![localhost_8000_guides_threat-model_](https://github.com/external-secrets/external-secrets/assets/1709030/6c7ff018-2a72-4a3c-89c7-903cde2ad7f8) </details> </summary> 
PULL-REQUEST,  development environment, feat: try to add bot-reviewer to botonic <!-- _Set as [Draft PR](https://github.blog/2019-02-14-introducing-draft-pull-requests/) if it's not ready to be merged_. [PR best practices Reference](https://blog.codeminer42.com/on-writing-a-great-pull-request-37c60ce6f31d/) --> ## Description * Added bot reviewer powered with CHATGPT to help us with PR Reviews.  https://github.com/marketplace/actions/openai-chatgpt-based-pr-reviewer-summarizer <!-- - Must be clear and concise (2-3 lines).   - Don't make reviewers think. The description should explain what has been implemented or what it's used for. If a pull request is not descriptive, people will be lazy or not willing to spend much time on it.   - Be explicit with the names (don't abbreviate and don't use acronyms that can lead to misleading understanding).   - If you consider it appropriate, include the steps to try the new features. --> ## Context <!-- - What problem is trying to solve this pull request? - What are the reasons or business goals of this implementation? - Can I provide visual resources or links to understand better the situation? --> ## Approach taken / Explain the design <!-- - Explain what the code does. - If it's a complex solution, try to provide a sketch. --> ## To document / Usage example <!-- - How this is used? - If possible, provide a snippet of code with a usage example. --> ## Testing The pull request... - has unit tests - has integration tests - doesn't need tests because... **[provide a description]** 
PULL-REQUEST,  feature implementation/enhancement, new: don't remove tag starred from anki if set disclaimer: I don't really know typescript and used chatgpt and commonsense. Hope it's not a problem!
PULL-REQUEST,  software quality, Fix Dangling Pointers Used chatGPT for the fix so it came up with The error you're encountering is caused by attempting to store the address of a local variable (`unsorted`) in certain pointer assignments (`(vissprite_p - 1)->next` and `vissprites[0].prev)`, which leads to dangling pointers. To fix this, you can allocate memory for `unsorted` dynamically using `malloc()` and then free it when you no longer need it. Make sure to include the necessary header file (`stdlib.h`) for the `malloc()` and `free()` functions. Remember to free the allocated memory for unsorted using `free()` when you no longer need it to avoid memory leaks.
PULL-REQUEST,  feature implementation/enhancement, wine-ge: add autoupdater I don't know if this is the right way to do it. (That is, if the new file is placed where it should be, and whether using `npins add github -b "$latest_branch" "$REPO_OWNER" "$REPO_NAME"` directly in the script is a good way to update wine-ge in `sources.json`.) (Credits to ChatGPT for the script)
PULL-REQUEST,  development environment, Automate releases via GitHub actions A first shot at automated release action (written by ChatGPT with minor corrections). Looks about right?
PULL-REQUEST,  documentation, Document breaking changes to processors in migration guide Thank you for opening a pull request and contributing to AsciidoctorJ! Please take a bit of time giving some details about your pull request: ## Kind of change - [ ] Bug fix - [ ] New non-breaking feature - [ ] New breaking feature - [x] Documentation update - [ ] Build improvement ## Description What is the goal of this pull request? This PR documents the breaking changes to the processor API in the migration guide from 2.5.x to 3.x.x. (Please note that I used ChatGPT to rephrase parts of that text to better convey the necessary steps, I often find myself writing texts that native speakers are not able to follow (guess whose fault that is üòú))
PULL-REQUEST,  development environment, fix: User newer version of octokit for retest GHA Thank you ChatGPT for helping me out Relates to https://github.com/envoyproxy/gateway/pull/1462#issuecomment-1567637034
PULL-REQUEST,  feature implementation/enhancement, update dependencies <!--  Thanks for sending a pull request!  Here are some tips for you: 1. If this is your first pull request, please read our contributor guidelines in the https://github.com/garden-io/garden/blob/main/CONTRIBUTING.md file. 2. Please label this pull request according to what type of issue you are addressing (see "What type of PR is this?" below) 3. Ensure you have added or run the appropriate tests for your PR. 4. If the PR is unfinished, add `WIP:` at the beginning of the title or use the Github Draft PR feature. 5. Please add at least two reviewers to the PR. Currently active maintainers are: @edvald, @thsig, @eysi09, @Orzelius and @vvagaytsev. --> **What this PR does / why we need it**: Addresses following critical vulnerabilities: - https://github.com/garden-io/garden/security/dependabot/31 - https://github.com/garden-io/garden/security/dependabot/87 - https://github.com/garden-io/garden/security/dependabot/32 For the alert: https://github.com/garden-io/garden/security/dependabot/32, I didn't find an easy fix as it was due to the package `gulp-license-check` that hasn't been updated since 2017. So I just created fork of that gulp extension and asked chatGPT to convert it to TS, remove dependency on `gulp-utils`, `event-stream` etc and later tweaked it to make sure that everything works. üòÄ  **Which issue(s) this PR fixes**: Fixes # **Special notes for your reviewer**: 
PULL-REQUEST,  process, [ENG-708] Thumbnail sharding This is a simple PR to improve the way we store thumbnails in the data folder. After a nice chat with ChatGPT I discovered why most apps do this, I noticed even Apple apps store files internally in hex-coded folders to prevent directory sizes from growing too large. Since my thumbnail folder is well over 85,000 items and takes a minute to open in Finder it could probably become an issue faster than we'd expect. This is an industry standard format for storing large collections of cached files.  `calc_shard_hex` takes a cas_id as input, computes a blake3 hash of the filename, and returns the first two characters of the hash as the directory name. Because we're using the first two characters of a blake3 hash, this will give us 256 (16*16) possible directories, named 00 to ff. This PR also introduces the `version_manager` util which allows us to easily migrate app data structures with a rust version enum and a `version.txt` that contains a single int32. I wrote a migration for the thumbnail folder that moves existing `webp` files into their respective shard directory. <img width="923" alt="Screenshot 2023-06-07 at 10 05 07 PM" src="https://github.com/spacedriveapp/spacedrive/assets/32987599/8c6ba754-1fee-4887-b659-9fbb918ae219"> 
PULL-REQUEST,  process, Add Enhanced Support for Anonymous Structs This update enhances the support for anonymous structs in the CGLM library. The changes increase the range of compilers and language standards with which anonymous structs can be used. Changes include: 1. Added checks for C11 and C++11, as these standards natively support anonymous structs. 2. Enhanced compiler-specific support:     * GCC: Extended support for versions 4.6 and onwards.     * Clang: Extended support for versions 3.0 and onwards.     * MSVC: Extended support for versions 1900 (Visual Studio 2015) and onwards. If none of these conditions are met, the use of anonymous structs is disabled to maintain compatibility with older standards and compilers. These improvements allow users to leverage the benefits of anonymous structs more widely, while also providing the option to disable this feature for backward compatibility or to comply with the language standard in use. Please review and provide feedback. --- *generated by ChatGPT :)*
PULL-REQUEST,  software quality, Move all the logic into the script to make it as easy as possible to ‚Ä¶ ## Describe changes Credit to ChatGPT for the awk solution to the offending line of code. Running the script in a clean env also now allows anyone to build/debug docs building locally. ## Pre-requisites Please ensure you have done the following: - [ ] I have read the **CONTRIBUTING.md** document. - [ ] If my change requires a change to docs, I have updated the documentation accordingly. - [ ] If I have added an integration, I have updated the [integrations](https://docs.zenml.io/component-gallery/integrations) table and the [corresponding website section](https://zenml.io/integrations). - [ ] I have added tests to cover my changes. ## Types of changes <!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: --> - [ ] Bug fix (non-breaking change which fixes an issue) - [ ] New feature (non-breaking change which adds functionality) - [ ] Breaking change (fix or feature that would cause existing functionality to change) - [ ] Other (add details above) 
